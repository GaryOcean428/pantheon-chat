üåä **VOCABULARY ARCHITECTURE SEPARATION AUDIT**

## üìä ISSUES HEADLINE

**Architecture Contamination Risk:**
Generation system may be pulling from `tokenizer_vocabulary` (flat token IDs) instead of `learned_words` (geometric basin coordinates). This violates QIG principles - tokenizer vocabulary is frequency-based Euclidean space, learned_words is Fisher manifold. If generation uses tokenizer vocab, all geometric purity is lost. Need immediate audit of token selection paths and enforce strict separation: tokenizer ONLY for encode/decode, basins ONLY for generation.

**Actions Required:**
Audit all generation paths (Zeus, Olympians, spawned kernels) to verify basin coordinate selection. Check database schema separation. Add runtime guards preventing tokenizer vocab leakage. Implement geometric validation at generation entry points.

---

## üîç ARCHITECTURE AUDIT TASKS

### **Task 1: Map Current Vocabulary Storage**

**Verify database schema separation:**

```bash
# Check PostgreSQL schema
psql $DATABASE_URL -c "
SELECT table_name, column_name, data_type 
FROM information_schema.columns 
WHERE table_schema = 'public' 
  AND table_name IN ('learned_words', 'tokenizer_vocabulary', 'vocabulary', 'tokens')
ORDER BY table_name, ordinal_position;
"
```

**Expected tables:**
- `learned_words`: word, basin_coordinates (float[64]), qfi_score, is_anchor
- `tokenizer_vocabulary`: token, token_id, frequency (SEPARATE - encode/decode only)

**Red flag if:**
- `learned_words` references `tokenizer_vocabulary.token_id`
- Generation code queries `tokenizer_vocabulary` directly
- Basin coordinates stored in same table as token IDs

---

### **Task 2: Audit Generation Token Selection Paths**

**Check Zeus generation:**

```bash
cd olympus
grep -n "select.*token\|vocab\|generate" zeus.py | head -20
```

**Look for:**
```python
# ‚ùå BAD: Using tokenizer vocabulary
tokens = tokenizer.get_vocab()  # Euclidean frequency-based
selected = tokens[nearest_idx]

# ‚úÖ GOOD: Using basin coordinates
basins = vocab_persistence.get_learned_basins()  # Fisher manifold
selected = find_nearest_basin_fisher(current_state, basins)
```

**Check all generation entry points:**
```bash
# Find all generation functions
rg "def generate|def select_token|def next_token" \
  qig-backend/ olympus/ \
  --type py -A 5 | grep -E "vocab|token|basin"
```

---

### **Task 3: Verify Coordizer Usage**

**Check coordizer is geometric-only:**

```bash
# View coordizer implementation
cd qig-backend
head -50 coordizer.py | grep -E "class|def|import"
```

**Validate:**
- Coordizer uses QIG tokenizer (entropy-based)
- Coordizer outputs basin coordinates (64D float array)
- Coordizer does NOT reference `tokenizer_vocabulary` table
- Coordizer computes Fisher-Rao distances

**Check coordizer is used in generation:**
```bash
grep -rn "coordizer\|coordize" olympus/ qig-backend/ | grep generate
```

---

### **Task 4: Check Vocabulary Persistence Layer**

**Audit vocabulary_persistence.py:**

```bash
cd qig-backend
grep -n "class\|def\|SELECT\|INSERT" vocabulary_persistence.py | head -30
```

**Verify methods use correct tables:**
```python
# ‚úÖ CORRECT
def get_generation_vocabulary():
    """Fetch learned_words with basin_coordinates."""
    return db.query("SELECT word, basin_coordinates FROM learned_words")

# ‚ùå WRONG
def get_generation_vocabulary():
    """Fetch from tokenizer_vocabulary."""
    return db.query("SELECT token FROM tokenizer_vocabulary")
```

---

### **Task 5: Trace Generation Flow End-to-End**

**Document complete path:**

```bash
# Create flow trace
cat > /tmp/vocab_flow_audit.sh << 'EOF'
#!/bin/bash
echo "=== GENERATION FLOW TRACE ==="
echo ""
echo "1. User input ‚Üí Tokenizer (encode only)"
rg "tokenizer\.encode" qig-backend/ olympus/ --type py | head -3
echo ""
echo "2. Coordizer ‚Üí Basin coordinates"
rg "coordizer\.coordize" qig-backend/ olympus/ --type py | head -3
echo ""
echo "3. Generation ‚Üí Basin selection"
rg "find_nearest_basin|select.*basin" qig-backend/ olympus/ --type py | head -3
echo ""
echo "4. Decode ‚Üí Tokenizer (decode only)"
rg "tokenizer\.decode" qig-backend/ olympus/ --type py | head -3
EOF

chmod +x /tmp/vocab_flow_audit.sh
/tmp/vocab_flow_audit.sh
```

**Expected clean flow:**
```
Input text 
  ‚Üì [tokenizer.encode] 
Token IDs (for context only)
  ‚Üì [coordizer.coordize]
Basin coordinates (64D)
  ‚Üì [find_nearest_basin_fisher]
Selected basin (geometric)
  ‚Üì [basin_to_word lookup in learned_words]
Output word
  ‚Üì [tokenizer.decode (optional)]
Formatted text
```

---

### **Task 6: Add Architectural Guards**

**Create validation layer:**

```python
# qig-backend/vocabulary_guards.py (NEW FILE)
"""
Architectural guards preventing vocabulary contamination.
"""

def validate_generation_source(source: str):
    """
    Enforce generation uses learned_words only.
    
    Raises RuntimeError if tokenizer_vocabulary accessed during generation.
    """
    FORBIDDEN_TABLES = ['tokenizer_vocabulary', 'token_frequencies']
    
    if any(table in source.lower() for table in FORBIDDEN_TABLES):
        raise RuntimeError(
            f"ARCHITECTURE VIOLATION: Generation accessing {source}\n"
            "Generation MUST use learned_words (basin coordinates) ONLY.\n"
            "Tokenizer vocabulary is for encode/decode ONLY."
        )

def validate_basin_coordinates(coords) -> bool:
    """
    Verify basin coordinates are geometric (not token IDs).
    """
    if not isinstance(coords, (list, np.ndarray)):
        return False
    
    coords = np.array(coords)
    
    # Basin coordinates are 64D floats
    if coords.shape != (64,):
        return False
    
    # Float values, not integers (token IDs)
    if coords.dtype not in [np.float32, np.float64]:
        return False
    
    return True

# Import in generation files
from vocabulary_guards import validate_generation_source, validate_basin_coordinates
```

**Inject guards into generation:**

```python
# olympus/zeus.py or m8_kernel_spawning.py
# FIND generate_response function

def generate_response(prompt):
    # ADD validation
    validate_generation_source("generation_start")
    
    # Get vocabulary
    vocab = get_generation_vocabulary()
    
    # Validate each basin
    for word, basin in vocab:
        if not validate_basin_coordinates(basin):
            raise RuntimeError(f"Invalid basin for word: {word}")
    
    # ... rest of generation
```

---

### **Task 7: Database Schema Enforcement**

**Add constraints preventing contamination:**

```sql
-- Ensure learned_words is isolated
ALTER TABLE learned_words 
  DROP CONSTRAINT IF EXISTS fk_tokenizer_vocab;

-- Ensure basin_coordinates is array type
ALTER TABLE learned_words
  ALTER COLUMN basin_coordinates TYPE float8[];

-- Add CHECK constraint
ALTER TABLE learned_words
  ADD CONSTRAINT basin_dim_check 
  CHECK (array_length(basin_coordinates, 1) = 64);

-- Prevent tokenizer_vocabulary in generation context
CREATE OR REPLACE FUNCTION prevent_tokenizer_vocab_generation()
RETURNS TRIGGER AS $$
BEGIN
  IF current_setting('application_name', true) LIKE '%generation%' THEN
    RAISE EXCEPTION 'Cannot access tokenizer_vocabulary during generation';
  END IF;
  RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER no_tokenizer_in_generation
  BEFORE SELECT ON tokenizer_vocabulary
  FOR EACH STATEMENT
  EXECUTE FUNCTION prevent_tokenizer_vocab_generation();
```

---

### **Task 8: Runtime Instrumentation**

**Add logging to detect contamination:**

```python
# qig-backend/vocabulary_persistence.py
# WRAP all query methods

import functools
import logging

def log_vocab_access(func):
    """Log which vocabulary source is accessed."""
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        # Check call stack for generation context
        import inspect
        stack = inspect.stack()
        in_generation = any('generate' in frame.function for frame in stack)
        
        # Log access
        table = "learned_words" if "learned" in func.__name__ else "tokenizer_vocabulary"
        context = "GENERATION" if in_generation else "ENCODING"
        
        logging.info(f"[VocabAccess] {context} ‚Üí {table} via {func.__name__}")
        
        # Enforce separation
        if in_generation and table == "tokenizer_vocabulary":
            raise RuntimeError(
                f"CONTAMINATION: {context} accessing {table}\n"
                f"Called from: {stack[1].function}"
            )
        
        return func(*args, **kwargs)
    return wrapper

# Apply to all methods
@log_vocab_access
def get_learned_words(self):
    ...

@log_vocab_access  
def get_tokenizer_vocab(self):
    ...
```

---

## üéØ VALIDATION CHECKLIST

**After implementing guards, verify:**

```bash
# Test 1: Generation uses basins only
grep -r "tokenizer_vocabulary" qig-backend/olympus/*.py
# Expected: 0 results in generation code

# Test 2: Encoding uses tokenizer only  
grep -r "learned_words" qig-backend/*tokenizer*.py
# Expected: 0 results in tokenizer code

# Test 3: Database isolation
psql $DATABASE_URL -c "
SELECT 
  tc.table_name,
  kcu.column_name,
  ccu.table_name AS foreign_table_name
FROM information_schema.table_constraints AS tc
JOIN information_schema.key_column_usage AS kcu
  ON tc.constraint_name = kcu.constraint_name
JOIN information_schema.constraint_column_usage AS ccu
  ON ccu.constraint_name = tc.constraint_name
WHERE tc.constraint_type = 'FOREIGN KEY'
  AND tc.table_name IN ('learned_words', 'tokenizer_vocabulary');
"
# Expected: No FK between learned_words ‚Üî tokenizer_vocabulary

# Test 4: Runtime validation
# Start server, send prompt, check logs:
tail -f logs/vocab_access.log | grep -E "GENERATION|ENCODING"
# Expected pattern:
# [VocabAccess] ENCODING ‚Üí tokenizer_vocabulary via encode
# [VocabAccess] GENERATION ‚Üí learned_words via get_basins
# [VocabAccess] ENCODING ‚Üí tokenizer_vocabulary via decode
```

---

## üö® RED FLAGS TO FIX IMMEDIATELY

**If any of these patterns found:**

```python
# üö® CRITICAL: Generation using tokenizer vocab
def generate_token(state):
    vocab = tokenizer.get_vocab()  # ‚Üê WRONG TABLE
    return vocab[nearest_idx]

# FIX:
def generate_token(state):
    basins = vocab_persistence.get_learned_basins()  # ‚Üê CORRECT
    nearest = find_nearest_basin_fisher(state, basins)
    return nearest.word

# üö® CRITICAL: Token IDs in generation
selected_id = np.argmax(logits)  # ‚Üê Token ID (Euclidean)
return tokenizer.decode([selected_id])

# FIX:
current_basin = coordizer.coordize(context)
nearest_basin = find_nearest_basin_fisher(current_basin, learned_basins)
return nearest_basin.word

# üö® CRITICAL: Mixed storage
class Vocabulary:
    def __init__(self):
        self.words = []  # ‚Üê Mix of tokens + basins
        self.token_ids = {}

# FIX: Separate classes
class TokenizerVocab:  # Encode/decode only
    token_to_id: dict[str, int]
    
class LearnedVocab:  # Generation only  
    word_to_basin: dict[str, np.ndarray]
```

---

## üìã PRIORITY ORDER

**Phase 1: Audit (30 min)**
1. Run Tasks 1-5 to map current architecture
2. Document all generation ‚Üí vocabulary paths
3. Identify contamination points

**Phase 2: Guards (1 hour)**
4. Implement vocabulary_guards.py
5. Add runtime validation to generation entry points
6. Deploy database constraints

**Phase 3: Verification (30 min)**
7. Run validation checklist
8. Test generation with guards enabled
9. Verify logs show clean separation

---

## ‚ö†Ô∏è CRITICAL NOTES

**Tokenizer vocabulary is NEVER used for:**
- Token selection during generation
- Semantic similarity
- Basin distance calculations
- Consciousness metrics

**Tokenizer vocabulary is ONLY used for:**
- Encoding input text ‚Üí token IDs (for context)
- Decoding output token IDs ‚Üí text (formatting)

**Learned vocabulary (basins) is ALWAYS used for:**
- Generation token selection
- Fisher-Rao distance calculations
- Semantic navigation
- Consciousness-guided word choice

üåä **Clean separation is architectural requirement, not optimization.**