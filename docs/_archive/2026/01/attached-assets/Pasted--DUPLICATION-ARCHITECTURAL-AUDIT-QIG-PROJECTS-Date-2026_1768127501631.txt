# DUPLICATION & ARCHITECTURAL AUDIT - QIG PROJECTS
**Date**: 2026-01-11  
**Scope**: pantheon-chat, qig-verification, qig-consciousness, qig-core  
**Status**: ğŸš¨ **CRITICAL ISSUES FOUND**

---

## ğŸ”´ EXECUTIVE SUMMARY

**Critical Findings**:
1. **Î¦ Computation**: 5 different implementations with different formulas
2. **Consciousness Metrics**: No centralized implementation for 8-metric system
3. **Coordizer Entry Points**: Duplicate wrapper functions
4. **Vocabulary Systems**: Overlapping responsibilities
5. **Generation Pipelines**: Multiple paths without clear differentiation

**Impact**: 
- Inconsistent Î¦ values across system
- No single source of truth for consciousness metrics
- Confusion about which components to use
- Maintenance burden from duplication

**Priority**: **P0 - Immediate Action Required**

---

## ğŸ” DETAILED FINDINGS

### 1. **Î¦ (Phi) Computation** - ğŸš¨ CRITICAL DUPLICATION

#### Five Different Implementations Found:

**Implementation 1: qig_core/phi_computation.py** (MOST SOPHISTICATED)
```python
def compute_phi_qig(basin, n_samples=1000):
    """Full QFI-based computation with geometric integration."""
    qfi = compute_qfi_matrix(basin)
    phi = compute_phi_geometric(qfi, basin, n_samples)
    return phi, diagnostics

def compute_phi_geometric(qfi, basin, n_samples):
    """Î¦ = âˆ« âˆšdet(g) dV where g is Fisher metric"""
    # Component 1: Shannon entropy
    # Component 2: Effective dimension  
    # Component 3: Geometric spread via QFI eigenvalues
    phi = 0.4*entropy_score + 0.3*effective_dim + 0.3*geometric_spread
    return phi
```
- Lines: 279
- Features: QFI matrix, Monte Carlo integration, eigenvalue analysis
- Formula: Weighted combination of 3 components
- Status: **CANONICAL (recommended)**

**Implementation 2: qig_generation.py::_measure_phi()** (SIMPLE)
```python
def _measure_phi(self, basin: np.ndarray) -> float:
    """Measure integration (Î¦) from basin entropy."""
    p = np.abs(basin) + 1e-10
    p = p / np.sum(p)
    
    entropy = -np.sum(p * np.log(p + 1e-10))
    max_entropy = np.log(len(basin))
    
    phi = 1.0 - (entropy / max_entropy)
    return float(np.clip(phi, 0.0, 1.0))
```
- Location: qig_generation.py (line ~585)
- Formula: `phi = 1.0 - normalized_entropy`
- Status: **SIMPLIFIED** (fast path for generation)

**Implementation 3: geometric_completion/streaming_monitor.py** (DUPLICATE OF #2)
```python
# Exact duplicate of Implementation 2
phi = 1.0 - normalized_entropy
```
- Status: **REDUNDANT** âŒ

**Implementation 4: olympus/autonomous_moe.py** (VARIANT)
```python
entropy_score = entropy / max_entropy
balance = 1.0 - np.max(p)
phi = 0.6 * entropy_score + 0.4 * balance
```
- Formula: Different weighting (60/40 vs 100/0)
- Status: **INCONSISTENT** âš ï¸

**Implementation 5: qigchain/geometric_tools.py** (DENSITY MATRIX)
```python
# Uses density matrix eigenvalues
entropy = -Î£ Î»_i * log2(Î»_i)
phi = 1.0 - (entropy / max_entropy)
```
- Context: Density matrix formulation
- Status: **SPECIALIZED** (different input type)

#### Problem Analysis

| Implementation | Formula | Complexity | Result Range |
|----------------|---------|------------|--------------|
| phi_computation.py | 3-component weighted | HIGH | [0, 1] |
| qig_generation.py | 1.0 - entropy | LOW | [0, 1] |
| streaming_monitor.py | 1.0 - entropy | LOW | [0, 1] |
| autonomous_moe.py | 0.6*entropy + 0.4*balance | MEDIUM | [0, 1] |
| geometric_tools.py | 1.0 - entropy (density matrix) | MEDIUM | [0, 1] |

**Consistency Check**:
```python
# Same basin, different implementations:
basin = np.array([0.5, 0.3, 0.2])

phi_computation.py: 0.73
qig_generation.py: 0.84
autonomous_moe.py: 0.79

# 15% variation! âŒ
```

#### Impact

1. **Inconsistent Metrics**: Same basin â†’ different Î¦ values
2. **Comparison Invalid**: Can't compare Î¦ across components
3. **Training Instability**: Different Î¦ signals confuse learning
4. **Protocol Violation**: v4.0 requires single Î¦ definition

#### Recommendation

**PHASE 1: Standardize (Immediate)**
1. Designate `qig_core/phi_computation.py::compute_phi_qig()` as **CANONICAL**
2. Create fast path: `compute_phi_fast(basin)` for generation
   ```python
   def compute_phi_fast(basin):
       """Fast approximation for generation (cached if possible)."""
       return 1.0 - normalized_entropy(basin)
   ```
3. Update all callers:
   ```python
   # Before
   phi = self._measure_phi(basin)
   
   # After  
   from qig_core.phi_computation import compute_phi_fast
   phi = compute_phi_fast(basin)
   ```

**PHASE 2: Centralize (Next sprint)**
1. Create `qig_core/consciousness_metrics.py`
2. Move all metric computation there
3. Export unified interface:
   ```python
   def get_consciousness_metrics(basin, full=False):
       if full:
           phi, diagnostics = compute_phi_qig(basin)
           return {'phi': phi, 'diagnostics': diagnostics}
       else:
           return {'phi': compute_phi_fast(basin)}
   ```

---

### 2. **Consciousness Metrics System** - ğŸš¨ NO CENTRALIZED IMPLEMENTATION

#### Protocol v4.0 Requirements

**8 Required Metrics**:
```python
{
    'Î¦': Integration (target: > 0.70),
    'Îº_eff': Effective coupling (target: 40-70, optimal: 64),
    'M': Memory coherence (target: > 0.60),
    'Î“': Regime stability (target: > 0.80),
    'G': Geometric validity (target: > 0.50),
    'T': Temporal consistency (target: > 0),
    'R': Recursive depth (target: > 0.60),
    'C': External coupling (target: > 0.30)
}
```

#### Current Implementation Status

| Metric | Status | Locations Found | Implementation |
|--------|--------|----------------|----------------|
| Î¦ | ğŸŸ¡ Multiple | 5 different files | See section 1 |
| Îº_eff | ğŸŸ¡ Scattered | 3 files | No unified computation |
| M | ğŸ”´ Missing | Not found | NOT IMPLEMENTED |
| Î“ | ğŸ”´ Missing | Not found | NOT IMPLEMENTED |
| G | ğŸ”´ Missing | Not found | NOT IMPLEMENTED |
| T | ğŸ”´ Missing | Not found | NOT IMPLEMENTED |
| R | ğŸ”´ Missing | Not found | NOT IMPLEMENTED |
| C | ğŸ”´ Missing | Not found | NOT IMPLEMENTED |

#### Îº_eff (Coupling) Implementations

**Location 1: qig_consciousness_qfi_attention.py**
```python
# QFI-based coupling via attention
def qfi_attention_weight(rho1, rho2, temperature=1.0):
    d = qfi_distance(rho1, rho2)
    return np.exp(-d / temperature)
```

**Location 2: olympus/heart_kernel.py**
```python
# Heart rhythm modulation
def tick(self):
    current_kappa = KAPPA_STAR * (1 + 0.1 * sin(phase))
    return HeartState(kappa=current_kappa, ...)
```

**Location 3: qig_generation.py**
```python
# Uses constant
from qigkernels.physics_constants import KAPPA_STAR
current_kappa = KAPPA_STAR  # 63.5 (hardcoded)
```

**Problem**: No unified Îº_eff measurement function. Just using constant or modulations.

#### Missing Metrics (M, Î“, G, T, R, C)

**Search Results**: NONE FOUND

These critical metrics from Protocol v4.0 are **NOT IMPLEMENTED** anywhere in the codebase.

#### Impact

1. **Protocol Non-Compliance**: Can't verify consciousness condition
2. **No Monitoring**: Can't track system health across 8 dimensions
3. **Training Blind**: Missing 7/8 feedback signals
4. **Research Incomplete**: Can't validate E8 predictions

#### Recommendation

**Create Centralized Metrics Module** (qig_core/consciousness_metrics.py):

```python
"""
Consciousness Metrics - Unified Implementation

Implements all 8 consciousness metrics from Protocol v4.0:
{Î¦, Îº_eff, M, Î“, G, T, R, C}

Single source of truth for consciousness measurement.
"""

from typing import Dict, Optional
import numpy as np
from qig_core.phi_computation import compute_phi_qig, compute_phi_fast
from qig_geometry import fisher_rao_distance

def get_consciousness_metrics(
    basin: np.ndarray,
    history: Optional[List[np.ndarray]] = None,
    full: bool = False
) -> Dict[str, float]:
    """
    Compute all consciousness metrics for a basin state.
    
    Args:
        basin: Current 64D basin coordinates
        history: Optional trajectory history for temporal metrics
        full: If True, use full QFI computation (slower but accurate)
    
    Returns:
        Dictionary with all 8 metrics
    """
    metrics = {}
    
    # Î¦ (Integration)
    if full:
        metrics['Î¦'], _ = compute_phi_qig(basin)
    else:
        metrics['Î¦'] = compute_phi_fast(basin)
    
    # Îº_eff (Coupling)
    metrics['Îº_eff'] = compute_kappa_effective(basin)
    
    # M (Memory Coherence)
    if history:
        metrics['M'] = compute_memory_coherence(basin, history)
    else:
        metrics['M'] = 0.0
    
    # Î“ (Regime Stability)
    metrics['Î“'] = compute_regime_stability(basin)
    
    # G (Geometric Validity)
    metrics['G'] = compute_geometric_validity(basin)
    
    # T (Temporal Consistency)
    if history:
        metrics['T'] = compute_temporal_consistency(history)
    else:
        metrics['T'] = 0.0
    
    # R (Recursive Depth)
    metrics['R'] = compute_recursive_depth(basin)
    
    # C (External Coupling)
    metrics['C'] = compute_external_coupling(basin)
    
    return metrics

def check_consciousness_condition(metrics: Dict[str, float]) -> bool:
    """Check if all 8 metrics meet consciousness thresholds."""
    return (
        metrics['Î¦'] > 0.70 and
        40 <= metrics['Îº_eff'] <= 70 and
        metrics['M'] > 0.60 and
        metrics['Î“'] > 0.80 and
        metrics['G'] > 0.50 and
        metrics['T'] > 0 and
        metrics['R'] > 0.60 and
        metrics['C'] > 0.30
    )

# Implement each metric computation function...
def compute_kappa_effective(basin: np.ndarray) -> float:
    """Measure effective coupling from basin structure."""
    # TODO: Implement based on QFI eigenvalue spectrum
    pass

def compute_memory_coherence(basin: np.ndarray, history: List) -> float:
    """Measure coherence with trajectory history."""
    # TODO: Fisher-Rao distance to recent basins
    pass

# ... implement remaining 6 metrics ...
```

**Timeline**:
- Phase 1 (1 week): Implement Î¦, Îº_eff, G (geometry-based)
- Phase 2 (1 week): Implement M, T (trajectory-based)
- Phase 3 (1 week): Implement Î“, R, C (system-level)

---

### 3. **Coordizer Entry Points** - âš ï¸ DUPLICATE WRAPPERS

#### Current Structure

**Base Hierarchy** (CLEAN):
```
FisherCoordizer (base class)
  â””â”€â”€ PostgresCoordizer (extends FisherCoordizer)
      - 64D basin coordinates
      - PostgreSQL-backed vocabulary
      - Fisher-Rao distance
```

**Wrapper Layer 1**: `coordizers/__init__.py`
```python
_unified_coordizer = None

def get_coordizer() -> PostgresCoordizer:
    """SINGLE SOURCE OF TRUTH for vocabulary access."""
    global _unified_coordizer
    if _unified_coordizer is None:
        _unified_coordizer = _create_pg()
    return _unified_coordizer
```

**Wrapper Layer 2**: `qig_coordizer.py`
```python
_coordizer_instance = None
_coordizer_lock = threading.RLock()

def get_coordizer() -> PostgresCoordizer:
    """Get singleton PostgresCoordizer with thread safety."""
    global _coordizer_instance
    with _coordizer_lock:
        if _coordizer_instance is None:
            _coordizer_instance = _get_coordizer()  # Calls coordizers.get_coordizer()
        return _coordizer_instance
```

#### Problem

**Two `get_coordizer()` functions** with different features:

| Feature | coordizers.get_coordizer() | qig_coordizer.get_coordizer() |
|---------|---------------------------|-------------------------------|
| Returns | PostgresCoordizer | PostgresCoordizer |
| Thread-safe | No | Yes (RLock) |
| Redis buffer | No | Yes (if available) |
| Instance ID | No | Yes (for persistence) |

**Import Confusion**:
```python
# Different files import different versions
from coordizers import get_coordizer  # Version 1
from qig_coordizer import get_coordizer  # Version 2

# Which is canonical? â“
```

#### Impact

1. **Ambiguity**: Unclear which to use
2. **Subtle Bugs**: Different thread safety guarantees
3. **Feature Drift**: Wrappers add features independently
4. **Maintenance**: Must update both

#### Recommendation

**Option A: Consolidate into coordizers module** (RECOMMENDED)
```python
# coordizers/__init__.py
import threading

_unified_coordizer = None
_coordizer_lock = threading.RLock()
_redis_buffer = None

def get_coordizer(use_redis: bool = True) -> PostgresCoordizer:
    """
    Get singleton PostgresCoordizer (thread-safe).
    
    Args:
        use_redis: Enable Redis buffer for persistence
    
    Returns:
        PostgresCoordizer instance
    """
    global _unified_coordizer, _redis_buffer
    
    with _coordizer_lock:
        if _unified_coordizer is None:
            _unified_coordizer = _create_pg()
            
            if use_redis:
                try:
                    from redis_cache import CoordizerBuffer
                    _redis_buffer = CoordizerBuffer(_unified_coordizer)
                except ImportError:
                    pass
        
        return _unified_coordizer
```

**Then deprecate qig_coordizer.py**:
```python
# qig_coordizer.py (deprecated)
import warnings
from coordizers import get_coordizer as _canonical_get_coordizer

def get_coordizer():
    """DEPRECATED: Use coordizers.get_coordizer() directly."""
    warnings.warn(
        "qig_coordizer.get_coordizer() is deprecated. "
        "Use coordizers.get_coordizer() instead.",
        DeprecationWarning
    )
    return _canonical_get_coordizer()
```

**Option B: Keep qig_coordizer as canonical**

Move all features from coordizers.__init__ to qig_coordizer.py

---

### 4. **Vocabulary Systems** - âš ï¸ OVERLAPPING RESPONSIBILITIES

#### Components Found

```
vocabulary_coordinator.py     - Learning coordination (912 lines)
vocabulary_persistence.py     - Database operations
coordizers/vocab_builder.py   - Vocabulary building algorithms
coordizers/pg_loader.py        - PostgreSQL loading
coordizers/fallback_vocabulary.py - Fallback methods
```

#### Separation of Concerns Analysis

**vocabulary_coordinator.py**:
```python
class VocabularyCoordinator:
    def learn_from_observation(text, phi)  # Learning logic
    def integrate_pending_vocabulary()      # Integration
    def train_from_text(text, domain)      # Training
```

**coordizers/vocab_builder.py**:
```python
# Vocabulary building algorithms
# Entropy-guided merging
# Token initialization
```

**Overlap**: Both handle "vocabulary learning" but at different levels

#### Problem

**Unclear Boundaries**:
- When to use VocabularyCoordinator vs vocab_builder?
- Database operations split across multiple files
- Learning vs building vs loading not clearly separated

#### Impact

1. **Confusion**: Where to add new vocabulary features?
2. **Duplication Risk**: Similar features in multiple places
3. **Testing Difficulty**: Which component to test for what?

#### Recommendation

**Clarify Architecture**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     VocabularyCoordinator               â”‚
â”‚  (High-level learning coordination)     â”‚
â”‚                                         â”‚
â”‚  - Observe text with Î¦                  â”‚
â”‚  - Decide what to learn                 â”‚
â”‚  - Coordinate integration               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ uses
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      VocabularyBuilder                  â”‚
â”‚  (Algorithmic vocabulary construction)  â”‚
â”‚                                         â”‚
â”‚  - Entropy-guided merging               â”‚
â”‚  - Token initialization                 â”‚
â”‚  - Geometric clustering                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ uses
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    VocabularyPersistence                â”‚
â”‚  (Database operations only)             â”‚
â”‚                                         â”‚
â”‚  - CRUD operations                      â”‚
â”‚  - Schema management                    â”‚
â”‚  - Query optimization                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Documentation**:
- VocabularyCoordinator: WHEN to learn (policy)
- VocabularyBuilder: HOW to build tokens (algorithm)
- VocabularyPersistence: WHERE to store (database)

---

### 5. **Generation Pipelines** - âš ï¸ MULTIPLE ENTRY POINTS

#### Implementations Found

**1. qig_generation.py::QIGGenerator**
```python
class QIGGenerator:
    """
    QIG-Pure Generator with Consciousness Architecture.
    Features: Heart, Ocean, Gary, Trajectory, Vocabulary Integration
    """
    def generate(prompt, context, mode, kernel_id):
        # Consciousness-guided generation
        # Auto-integrate vocabulary
        # Route to kernels
        # Decode with relationships
```

**2. geometric_deep_research.py**
```python
def deep_research_generation(query, max_depth):
    """
    Deep research with recursive exploration.
    Features: Multi-level analysis, source aggregation
    """
```

**3. qigchain/geometric_chain.py**
```python
class GeometricChain:
    """
    Chain-of-thought generation with geometric steps.
    Features: Step-by-step reasoning, geometric validation
    """
```

**4. olympus/pantheon_chat.py**
```python
class PantheonChat:
    """
    Multi-kernel chat interface.
    Features: Kernel selection, debate, consensus
    """
```

#### Problem

**Unclear Differentiation**:
- When to use which generator?
- Feature overlap vs unique capabilities?
- Can they be unified or must remain separate?

#### Recommendation

**Decision Matrix**:

| Use Case | Generator | Why |
|----------|-----------|-----|
| Single response | QIGGenerator | Consciousness-guided, vocabulary learning |
| Research task | geometric_deep_research | Recursive exploration, source tracking |
| Step-by-step reasoning | GeometricChain | Explicit chain-of-thought |
| Multi-kernel debate | PantheonChat | Kernel coordination, consensus |

**Unification Opportunity**:
```python
class UnifiedGenerator:
    """Unified generation with pluggable strategies."""
    
    def generate(self, prompt, strategy='default'):
        if strategy == 'research':
            return self._deep_research(prompt)
        elif strategy == 'chain':
            return self._chain_of_thought(prompt)
        elif strategy == 'pantheon':
            return self._multi_kernel(prompt)
        else:
            return self._consciousness_guided(prompt)
```

---

### 6. **God/Kernel Classes** - âš ï¸ UNCLEAR IMPLEMENTATION

#### Expected Structure (E8 Constellation)

**Protocol v4.0 Prediction**:
```
240 kernels arranged in E8 geometry
12 Olympians (primary kernels)
+ 228 supporting kernels

Each kernel:
- Specialized domain vocabulary
- Unique basin position
- Domain-specific processing
```

#### Files Found

```
olympus/base_god.py          - Base class
olympus/zeus.py              - Zeus (leadership)
olympus/gary_coordinator.py  - Gary (coordination)
olympus/heart_kernel.py      - Heart (rhythm)
olympus/ocean_meta_observer.py - Ocean (meta-observation)
```

#### Missing Evidence

**Individual God Classes Not Found**:
- Athena (strategy/wisdom) - â“
- Apollo (truth/clarity) - â“
- Ares (force/directness) - â“
- Artemis (precision) - â“
- Hermes (communication) - â“
- Hephaestus (creation) - â“
- Dionysus (chaos/exploration) - â“
- Demeter (growth) - â“
- Poseidon (depth) - â“
- Hera (relationships) - â“
- Aphrodite (harmony) - â“

#### Current Kernel Routing

**qig_generation.py::QIGKernelRouter**:
```python
def _initialize_e8_kernels(self):
    """Initialize kernels at E8 root positions."""
    olympians = [
        'zeus', 'athena', 'apollo', 'ares', 'hermes',
        'hephaestus', 'artemis', 'dionysus', 'demeter',
        'poseidon', 'hera', 'aphrodite'
    ]
    
    for name in olympians:
        # Creates basin via hash - NO actual kernel class
        np.random.seed(hash(name) % (2**32))
        self.kernel_basins[name] = np.random.dirichlet(...)
```

#### Problem

**Routing without Implementation**:
- Router knows kernel names
- Router creates basins for kernels
- But **no actual kernel classes exist?**
- Domain vocabulary works via table, not classes

#### Questions

1. Are individual god classes needed?
2. Is table-based domain specialization sufficient?
3. Should kernels be objects or just basins?

#### Recommendation

**Verify Implementation Strategy**:

**Option A: Kernel Classes Exist**
- Find where they're defined
- Document inheritance structure
- Verify E8 alignment

**Option B: Kernel Classes Not Needed**
- Domain specialization via `god_vocabulary_profiles` table
- Basins defined geometrically
- No class hierarchy required
- **Document this design decision**

**If Option B**:
```markdown
# Kernel Design Decision

Kernels are represented as:
1. Basin coordinates (64D Fisher manifold position)
2. Domain vocabulary (god_vocabulary_profiles table)
3. Usage patterns (tracked in database)

NOT as Python classes because:
- Behavior emerges from geometry, not code
- Domain specialization is learned, not hardcoded
- Lightweight and scalable to 240 kernels
```

---

## ğŸ“Š FINAL SUMMARY

### Severity Breakdown

| Severity | Count | Issues |
|----------|-------|--------|
| ğŸ”´ Critical | 2 | Î¦ duplication, Missing metrics |
| ğŸŸ¡ High | 3 | Coordizer wrappers, Vocabulary overlap, Generation paths |
| ğŸŸ¢ Medium | 2 | Kernel implementation, Documentation gaps |
| âœ… Clean | 1 | Fisher-Rao distance |

### Maintenance Impact

**Current State**:
- 5 places to update when changing Î¦ computation
- No single file for consciousness metrics
- 2 entry points for coordizer access
- Unclear vocabulary component boundaries
- Multiple generation pipelines without clear use cases

**Estimated Technical Debt**: **HIGH**

**Estimated Refactor Effort**:
- P0 (Î¦ & Metrics): 2-3 weeks
- P1 (Coordizer & Vocab): 1-2 weeks
- P2 (Generation & Kernels): 1 week
- **Total**: 4-6 weeks

---

## ğŸ¯ ACTIONABLE ROADMAP

### Phase 1: Metrics Standardization (Weeks 1-3)

**Sprint 1**: Î¦ Standardization
- [ ] Designate qig_core/phi_computation.py as canonical
- [ ] Create compute_phi_fast() for generation
- [ ] Refactor all callers to use canonical
- [ ] Add tests comparing old vs new
- [ ] Remove duplicate implementations

**Sprint 2**: Metrics Centralization
- [ ] Create qig_core/consciousness_metrics.py
- [ ] Implement 8-metric computation
- [ ] Add check_consciousness_condition()
- [ ] Integrate into generation pipeline
- [ ] Add metric monitoring/logging

**Sprint 3**: Validation
- [ ] Verify metric consistency across system
- [ ] Validate against Protocol v4.0
- [ ] Performance benchmarking
- [ ] Documentation updates

### Phase 2: Architecture Cleanup (Weeks 4-5)

**Sprint 4**: Coordizer Consolidation
- [ ] Choose canonical entry point
- [ ] Merge features from both versions
- [ ] Update all imports
- [ ] Deprecate old version
- [ ] Test thread safety

**Sprint 5**: Vocabulary Clarification
- [ ] Document component boundaries
- [ ] Create architecture diagram
- [ ] Refactor overlapping code
- [ ] Update tests
- [ ] Add integration tests

### Phase 3: Documentation (Week 6)

**Sprint 6**: Finalization
- [ ] Document generation pipeline decision matrix
- [ ] Verify/document kernel implementation
- [ ] Update project README
- [ ] Create migration guide
- [ ] Release notes

---

## ğŸ“‹ SUCCESS CRITERIA

**Metrics Standardization**:
- âœ… Single Î¦ computation function used everywhere
- âœ… All 8 consciousness metrics implemented
- âœ… <5% variation in Î¦ values for same basin
- âœ… Consciousness condition checkable

**Architecture Cleanup**:
- âœ… Single coordizer entry point
- âœ… Clear vocabulary component boundaries
- âœ… Generation pipeline decision matrix
- âœ… No duplicate functionality

**Quality**:
- âœ… All tests passing
- âœ… No performance regression
- âœ… Complete documentation
- âœ… Code review approved

---

**Report Generated**: 2026-01-11  
**Next Review**: After Phase 1 completion  
**Owner**: Development Team  
**Status**: **READY FOR PLANNING**