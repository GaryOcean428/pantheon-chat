# SQL Specifications for Vocabulary Integration
## Complete Database Schema Requirements

**Context**: The Python code now integrates learned vocabulary, domain-specific vocabularies per kernel, and word relationships during generation. The database needs to support these operations efficiently.

---

## 1. REQUIRED TABLES (Validation & Enhancement)

### **Table: `learned_words`** ✅ EXISTS
**Purpose**: Store learned vocabulary with integration tracking

**Required Schema**:
```sql
CREATE TABLE IF NOT EXISTS learned_words (
    id SERIAL PRIMARY KEY,
    word TEXT UNIQUE NOT NULL,
    frequency INT DEFAULT 1,
    avg_phi REAL DEFAULT 0.0,
    max_phi REAL DEFAULT 0.0,
    source TEXT NOT NULL,
    learned_from TEXT,
    contexts TEXT[],
    first_seen TIMESTAMP DEFAULT NOW(),
    last_seen TIMESTAMP DEFAULT NOW(),
    is_integrated BOOLEAN DEFAULT FALSE,
    integrated_at TIMESTAMP,
    
    -- NEW: Add if missing
    basin_coords vector(64),  -- For geometric queries
    last_used_in_generation TIMESTAMP  -- Track actual usage
);
```

**Critical Columns to Add** (if missing):
```sql
-- Add basin coordinates column
ALTER TABLE learned_words 
ADD COLUMN IF NOT EXISTS basin_coords vector(64);

-- Add usage tracking
ALTER TABLE learned_words 
ADD COLUMN IF NOT EXISTS last_used_in_generation TIMESTAMP;

-- Ensure is_integrated column exists
ALTER TABLE learned_words 
ADD COLUMN IF NOT EXISTS is_integrated BOOLEAN DEFAULT FALSE;

ALTER TABLE learned_words 
ADD COLUMN IF NOT EXISTS integrated_at TIMESTAMP;
```

**Required Indexes**:
```sql
-- High-phi pending integration (CRITICAL for performance)
CREATE INDEX IF NOT EXISTS idx_learned_words_pending_integration 
ON learned_words(avg_phi DESC, frequency DESC) 
WHERE is_integrated = FALSE;

-- Integration timestamp tracking
CREATE INDEX IF NOT EXISTS idx_learned_words_integrated_at 
ON learned_words(integrated_at DESC) 
WHERE integrated_at IS NOT NULL;

-- Last used tracking
CREATE INDEX IF NOT EXISTS idx_learned_words_last_used 
ON learned_words(last_used_in_generation DESC NULLS LAST);

-- Existing indexes (validate they exist)
CREATE INDEX IF NOT EXISTS idx_learned_word ON learned_words(word);
CREATE INDEX IF NOT EXISTS idx_learned_phi ON learned_words(avg_phi DESC);
CREATE INDEX IF NOT EXISTS idx_learned_source ON learned_words(source);
```

---

### **Table: `god_vocabulary_profiles`** ✅ EXISTS
**Purpose**: Per-kernel domain-specific vocabulary

**Required Schema**:
```sql
CREATE TABLE IF NOT EXISTS god_vocabulary_profiles (
    id SERIAL PRIMARY KEY,
    god_name TEXT NOT NULL,
    word TEXT NOT NULL,
    relevance_score REAL NOT NULL,  -- 0.0 to 1.0
    usage_count INT DEFAULT 0,
    last_used TIMESTAMP DEFAULT NOW(),
    
    -- NEW: Add if missing
    learned_from_phi REAL,  -- Φ when relationship was learned
    basin_distance REAL,    -- Distance from kernel's basin
    
    UNIQUE(god_name, word)
);
```

**Critical Columns to Add** (if missing):
```sql
-- Add phi context
ALTER TABLE god_vocabulary_profiles 
ADD COLUMN IF NOT EXISTS learned_from_phi REAL;

-- Add geometric distance
ALTER TABLE god_vocabulary_profiles 
ADD COLUMN IF NOT EXISTS basin_distance REAL;
```

**Required Indexes**:
```sql
-- Per-god relevance queries (CRITICAL - used every generation)
CREATE INDEX IF NOT EXISTS idx_god_vocab_god_relevance 
ON god_vocabulary_profiles(god_name, relevance_score DESC, usage_count DESC);

-- Word lookups across gods
CREATE INDEX IF NOT EXISTS idx_god_vocab_word 
ON god_vocabulary_profiles(word, relevance_score DESC);

-- High-relevance vocabulary only
CREATE INDEX IF NOT EXISTS idx_god_vocab_high_relevance 
ON god_vocabulary_profiles(relevance_score DESC, usage_count DESC) 
WHERE relevance_score >= 0.5;

-- Existing indexes (validate)
CREATE INDEX IF NOT EXISTS idx_god_vocab_name ON god_vocabulary_profiles(god_name);
CREATE INDEX IF NOT EXISTS idx_god_vocab_relevance ON god_vocabulary_profiles(relevance_score DESC);
```

---

### **Table: `word_relationships`** ❓ NEEDS CREATION
**Purpose**: Co-occurrence and relationships between words

**Create Table**:
```sql
CREATE TABLE IF NOT EXISTS word_relationships (
    id SERIAL PRIMARY KEY,
    word_a TEXT NOT NULL,
    word_b TEXT NOT NULL,
    co_occurrence INT DEFAULT 1,
    fisher_distance REAL,
    avg_phi REAL DEFAULT 0.5,
    max_phi REAL DEFAULT 0.5,
    contexts TEXT[],  -- Sample contexts where they co-occurred
    first_seen TIMESTAMP DEFAULT NOW(),
    last_seen TIMESTAMP DEFAULT NOW(),
    
    UNIQUE(word_a, word_b)
);
```

**Required Indexes**:
```sql
-- Forward lookup (CRITICAL - used during decode)
CREATE INDEX IF NOT EXISTS idx_word_rel_word_a_phi 
ON word_relationships(word_a, avg_phi DESC, co_occurrence DESC);

-- Reverse lookup
CREATE INDEX IF NOT EXISTS idx_word_rel_word_b 
ON word_relationships(word_b, avg_phi DESC);

-- High-phi relationships only
CREATE INDEX IF NOT EXISTS idx_word_rel_high_phi 
ON word_relationships(avg_phi DESC, co_occurrence DESC) 
WHERE avg_phi >= 0.6;

-- Bi-directional lookups
CREATE INDEX IF NOT EXISTS idx_word_rel_both 
ON word_relationships(word_a, word_b);
```

---

### **Table: `vocabulary_observations`** ✅ EXISTS (ENHANCED)
**Purpose**: Raw vocabulary observations before aggregation

**Enhancements**:
```sql
-- Add relationship tracking
ALTER TABLE vocabulary_observations 
ADD COLUMN IF NOT EXISTS co_occurred_with TEXT[];

-- Add basin coordinates if missing
ALTER TABLE vocabulary_observations 
ADD COLUMN IF NOT EXISTS basin_coords vector(64);
```

**Indexes**:
```sql
-- Pending real-word validation
CREATE INDEX IF NOT EXISTS idx_vocab_obs_needs_validation 
ON vocabulary_observations(last_seen DESC) 
WHERE is_real_word IS NULL;

-- High-phi unintegrated
CREATE INDEX IF NOT EXISTS idx_vocab_obs_pending_integration 
ON vocabulary_observations(avg_phi DESC) 
WHERE is_integrated = FALSE;
```

---

## 2. HELPER FUNCTIONS (PostgreSQL)

### **Function: Get Pending Vocabulary for Integration**
```sql
CREATE OR REPLACE FUNCTION get_pending_vocabulary_for_integration(
    p_min_phi REAL DEFAULT 0.65,
    p_limit INT DEFAULT 100
) RETURNS TABLE (
    word TEXT,
    avg_phi REAL,
    max_phi REAL,
    frequency INT,
    source TEXT,
    basin_coords vector
) AS $$
BEGIN
    RETURN QUERY
    SELECT 
        lw.word,
        lw.avg_phi,
        lw.max_phi,
        lw.frequency,
        lw.source,
        lw.basin_coords
    FROM learned_words lw
    WHERE lw.is_integrated = FALSE
      AND lw.avg_phi >= p_min_phi
    ORDER BY lw.avg_phi DESC, lw.frequency DESC
    LIMIT p_limit;
END;
$$ LANGUAGE plpgsql;
```

---

### **Function: Mark Vocabulary as Integrated**
```sql
CREATE OR REPLACE FUNCTION mark_vocabulary_integrated(
    p_words TEXT[]
) RETURNS INT AS $$
DECLARE
    rows_updated INT;
BEGIN
    UPDATE learned_words
    SET 
        is_integrated = TRUE,
        integrated_at = NOW()
    WHERE word = ANY(p_words);
    
    GET DIAGNOSTICS rows_updated = ROW_COUNT;
    RETURN rows_updated;
END;
$$ LANGUAGE plpgsql;
```

---

### **Function: Get God Domain Vocabulary**
```sql
CREATE OR REPLACE FUNCTION get_god_domain_vocabulary(
    p_god_name TEXT,
    p_min_relevance REAL DEFAULT 0.5,
    p_limit INT DEFAULT 50
) RETURNS TABLE (
    word TEXT,
    relevance_score REAL,
    usage_count INT,
    basin_distance REAL
) AS $$
BEGIN
    RETURN QUERY
    SELECT 
        gvp.word,
        gvp.relevance_score,
        gvp.usage_count,
        gvp.basin_distance
    FROM god_vocabulary_profiles gvp
    WHERE gvp.god_name = p_god_name
      AND gvp.relevance_score >= p_min_relevance
    ORDER BY gvp.relevance_score DESC, gvp.usage_count DESC
    LIMIT p_limit;
END;
$$ LANGUAGE plpgsql;
```

---

### **Function: Update God Vocabulary Usage**
```sql
CREATE OR REPLACE FUNCTION update_god_vocabulary_usage(
    p_god_name TEXT,
    p_word TEXT
) RETURNS VOID AS $$
BEGIN
    UPDATE god_vocabulary_profiles
    SET 
        usage_count = usage_count + 1,
        last_used = NOW()
    WHERE god_name = p_god_name
      AND word = p_word;
END;
$$ LANGUAGE plpgsql;
```

---

### **Function: Get Word Relationships**
```sql
CREATE OR REPLACE FUNCTION get_word_relationships(
    p_context_words TEXT[],
    p_min_phi REAL DEFAULT 0.5,
    p_limit INT DEFAULT 50
) RETURNS TABLE (
    word_b TEXT,
    co_occurrence INT,
    fisher_distance REAL,
    avg_phi REAL
) AS $$
BEGIN
    RETURN QUERY
    SELECT 
        wr.word_b,
        wr.co_occurrence,
        wr.fisher_distance,
        wr.avg_phi
    FROM word_relationships wr
    WHERE wr.word_a = ANY(p_context_words)
      AND wr.avg_phi >= p_min_phi
    ORDER BY wr.avg_phi DESC, wr.co_occurrence DESC
    LIMIT p_limit;
END;
$$ LANGUAGE plpgsql;
```

---

### **Function: Record Word Co-occurrence**
```sql
CREATE OR REPLACE FUNCTION record_word_cooccurrence(
    p_word_a TEXT,
    p_word_b TEXT,
    p_phi REAL DEFAULT 0.5,
    p_fisher_distance REAL DEFAULT NULL,
    p_context TEXT DEFAULT NULL
) RETURNS VOID AS $$
BEGIN
    INSERT INTO word_relationships (
        word_a, word_b, co_occurrence, avg_phi, max_phi,
        fisher_distance, contexts, first_seen, last_seen
    )
    VALUES (
        p_word_a, p_word_b, 1, p_phi, p_phi,
        p_fisher_distance,
        CASE WHEN p_context IS NOT NULL THEN ARRAY[p_context] ELSE NULL END,
        NOW(), NOW()
    )
    ON CONFLICT (word_a, word_b) DO UPDATE SET
        co_occurrence = word_relationships.co_occurrence + 1,
        avg_phi = (word_relationships.avg_phi * word_relationships.co_occurrence + p_phi) 
                  / (word_relationships.co_occurrence + 1),
        max_phi = GREATEST(word_relationships.max_phi, p_phi),
        last_seen = NOW(),
        contexts = CASE 
            WHEN p_context IS NOT NULL AND array_length(word_relationships.contexts, 1) < 10 
            THEN word_relationships.contexts || p_context
            ELSE word_relationships.contexts
        END;
END;
$$ LANGUAGE plpgsql;
```

---

## 3. DATA MIGRATION STRATEGY

### **Step 1: Validate Existing Data**
```sql
-- Count existing learned words
SELECT COUNT(*) as total_learned_words,
       COUNT(*) FILTER (WHERE is_integrated = TRUE) as integrated,
       COUNT(*) FILTER (WHERE is_integrated = FALSE) as pending,
       COUNT(*) FILTER (WHERE avg_phi >= 0.65) as high_phi_pending
FROM learned_words;

-- Check god vocabulary profiles
SELECT god_name, COUNT(*) as vocab_size
FROM god_vocabulary_profiles
GROUP BY god_name
ORDER BY vocab_size DESC;
```

---

### **Step 2: Bootstrap God Vocabularies (if empty)**
```sql
-- Bootstrap from learned_words by source
INSERT INTO god_vocabulary_profiles (god_name, word, relevance_score, learned_from_phi)
SELECT 
    learned_from as god_name,
    word,
    LEAST(avg_phi, 1.0) as relevance_score,
    avg_phi as learned_from_phi
FROM learned_words
WHERE learned_from IS NOT NULL
  AND avg_phi >= 0.5
  AND NOT EXISTS (
      SELECT 1 FROM god_vocabulary_profiles gvp 
      WHERE gvp.god_name = learned_words.learned_from 
        AND gvp.word = learned_words.word
  )
LIMIT 1000;

-- Add default vocabularies for standard gods
INSERT INTO god_vocabulary_profiles (god_name, word, relevance_score) VALUES
    ('athena', 'strategy', 0.95),
    ('athena', 'wisdom', 0.90),
    ('athena', 'plan', 0.85),
    ('athena', 'pattern', 0.85),
    ('ares', 'attack', 0.95),
    ('ares', 'force', 0.90),
    ('ares', 'direct', 0.85),
    ('ares', 'aggressive', 0.85),
    ('apollo', 'truth', 0.95),
    ('apollo', 'clarity', 0.90),
    ('apollo', 'precise', 0.85),
    ('apollo', 'light', 0.80)
ON CONFLICT (god_name, word) DO NOTHING;
```

---

### **Step 3: Build Initial Word Relationships**
```sql
-- Build relationships from vocabulary_observations contexts
-- This is a one-time bootstrap operation
CREATE OR REPLACE FUNCTION bootstrap_word_relationships()
RETURNS INT AS $$
DECLARE
    v_count INT := 0;
    v_obs RECORD;
    v_words TEXT[];
    v_word_a TEXT;
    v_word_b TEXT;
    i INT;
    j INT;
BEGIN
    -- Get high-phi observations with contexts
    FOR v_obs IN 
        SELECT text, avg_phi, contexts
        FROM vocabulary_observations
        WHERE avg_phi >= 0.6
          AND contexts IS NOT NULL
          AND array_length(contexts, 1) > 0
        LIMIT 1000
    LOOP
        -- Extract words from first context
        IF v_obs.contexts[1] IS NOT NULL THEN
            v_words := regexp_split_to_array(
                lower(v_obs.contexts[1]), 
                '\s+'
            );
            
            -- Create relationships for word pairs
            FOR i IN 1..array_length(v_words, 1) - 1 LOOP
                v_word_a := v_words[i];
                
                -- Skip if too short
                IF length(v_word_a) < 3 THEN
                    CONTINUE;
                END IF;
                
                FOR j IN (i+1)..LEAST(i+3, array_length(v_words, 1)) LOOP
                    v_word_b := v_words[j];
                    
                    -- Skip if too short
                    IF length(v_word_b) < 3 THEN
                        CONTINUE;
                    END IF;
                    
                    -- Record relationship
                    PERFORM record_word_cooccurrence(
                        v_word_a,
                        v_word_b,
                        v_obs.avg_phi,
                        NULL,
                        v_obs.contexts[1]
                    );
                    
                    v_count := v_count + 1;
                END LOOP;
            END LOOP;
        END IF;
    END LOOP;
    
    RETURN v_count;
END;
$$ LANGUAGE plpgsql;

-- Run bootstrap (this may take a few minutes)
-- SELECT bootstrap_word_relationships();
```

---

## 4. MONITORING QUERIES

### **Check Integration Status**
```sql
-- Vocabulary integration health
SELECT 
    COUNT(*) as total_words,
    COUNT(*) FILTER (WHERE is_integrated = TRUE) as integrated,
    COUNT(*) FILTER (WHERE is_integrated = FALSE) as pending,
    COUNT(*) FILTER (WHERE is_integrated = FALSE AND avg_phi >= 0.65) as high_phi_pending,
    AVG(avg_phi) FILTER (WHERE is_integrated = TRUE) as avg_phi_integrated,
    AVG(avg_phi) FILTER (WHERE is_integrated = FALSE) as avg_phi_pending
FROM learned_words;
```

---

### **Check God Vocabulary Coverage**
```sql
-- God vocabulary profiles summary
SELECT 
    god_name,
    COUNT(*) as vocab_size,
    AVG(relevance_score) as avg_relevance,
    SUM(usage_count) as total_usages,
    MAX(last_used) as last_used
FROM god_vocabulary_profiles
GROUP BY god_name
ORDER BY vocab_size DESC;
```

---

### **Check Word Relationships**
```sql
-- Word relationships summary
SELECT 
    COUNT(*) as total_relationships,
    AVG(co_occurrence) as avg_cooccurrence,
    AVG(avg_phi) as avg_phi,
    COUNT(*) FILTER (WHERE avg_phi >= 0.7) as high_phi_relationships
FROM word_relationships;

-- Top relationships
SELECT word_a, word_b, co_occurrence, avg_phi
FROM word_relationships
ORDER BY avg_phi DESC, co_occurrence DESC
LIMIT 50;
```

---

## 5. PERFORMANCE OPTIMIZATION

### **Analyze Tables**
```sql
-- Update statistics for query planner
ANALYZE learned_words;
ANALYZE god_vocabulary_profiles;
ANALYZE word_relationships;
ANALYZE vocabulary_observations;
```

---

### **Index Maintenance**
```sql
-- Reindex if queries are slow
REINDEX TABLE learned_words;
REINDEX TABLE god_vocabulary_profiles;
REINDEX TABLE word_relationships;
```

---

### **Query Performance Check**
```sql
-- Check index usage
SELECT 
    schemaname,
    tablename,
    indexname,
    idx_scan,
    idx_tup_read,
    idx_tup_fetch
FROM pg_stat_user_indexes
WHERE schemaname = 'public'
  AND tablename IN ('learned_words', 'god_vocabulary_profiles', 'word_relationships')
ORDER BY idx_scan DESC;
```

---

## 6. TESTING QUERIES

### **Test Pending Integration Query**
```sql
-- Should return high-phi unintegrated words
SELECT * FROM get_pending_vocabulary_for_integration(0.65, 10);
```

---

### **Test God Vocabulary Query**
```sql
-- Should return domain-specific vocabulary
SELECT * FROM get_god_domain_vocabulary('athena', 0.5, 20);
SELECT * FROM get_god_domain_vocabulary('ares', 0.5, 20);
```

---

### **Test Word Relationships Query**
```sql
-- Should return related words
SELECT * FROM get_word_relationships(ARRAY['quantum', 'fisher', 'basin'], 0.5, 20);
```

---

## IMPLEMENTATION CHECKLIST

- [ ] Run schema validation queries
- [ ] Add missing columns to existing tables
- [ ] Create word_relationships table if missing
- [ ] Create all required indexes
- [ ] Create all helper functions
- [ ] Bootstrap god vocabularies (if needed)
- [ ] Run initial word relationship bootstrap (optional, can build over time)
- [ ] Run ANALYZE on all tables
- [ ] Test queries with monitoring scripts
- [ ] Verify Python code can query functions

---

## NEXT STEPS AFTER SQL

Once SQL is complete:
1. Python code will auto-integrate vocabulary every 5 min
2. Kernels will use domain-specific vocabularies
3. Decode will use word relationships for coherence
4. Monitor with provided queries to verify integration working

Expected behavior:
- `learned_words.is_integrated` should flip to TRUE gradually
- `god_vocabulary_profiles` should show balanced vocabulary per god
- `word_relationships` should grow as generation produces coherent sequences