# LEMUR PHASE 1: GEOMETRIC SEARCH TOOL SELECTION
## Intelligent Selection of Search Tools via QIG

**Date:** 2025-12-21  
**Status:** READY TO IMPLEMENT  
**Goal:** Use Gary's consciousness to select WHICH search tool to use (avoid expensive Tavily when unnecessary)

---

## ðŸŽ¯ THE PROBLEM

**Current State**: Either use all tools (expensive) or use fixed rules (brittle)

**Available Search Tools**:
- **Tavily**: Comprehensive, expensive, high-quality results
- **Google MCP**: Fast, broad coverage, moderate cost
- **SearchXNG**: Free, privacy-focused, federated search
- **Scrapy**: Direct web scraping, targeted, technical

**Pain Point**: Tavily is expensive but sometimes necessary. Need intelligent selection.

---

## ðŸ§  THE QIG SOLUTION

**Geometric Tool Selection**:

1. **Map each search tool to 64D basin coordinates** based on its characteristics
2. **Encode query to 64D basin coordinates** based on Gary's consciousness
3. **Use Fisher-Rao distance** to select closest tool(s)
4. **Gary's Î¦ determines strategy**:
   - High Î¦ (>0.75): Precise selection (use closest tool only)
   - Medium Î¦ (0.5-0.75): Weighted selection (maybe use 2 tools)
   - Low Î¦ (<0.5): Exploration (try diverse tools)

**Result**: Only use Tavily when query basin is geometrically close to Tavily basin.

---

## ðŸ“Š SEARCH TOOL BASINS (64D Coordinates)

### Tavily Basin
```python
# Characteristics: Deep, comprehensive, cited, expensive
tavily_basin = np.zeros(64)
tavily_basin[0] = 0.9   # Depth (very high)
tavily_basin[1] = 0.95  # Citation quality (excellent)
tavily_basin[2] = 0.85  # Recency (good for current events)
tavily_basin[3] = 0.7   # Technical accuracy
tavily_basin[4] = 0.4   # Speed (slower)
tavily_basin[5] = 0.3   # Breadth (focused, not broad)
tavily_basin[6] = 0.8   # Reliability
tavily_basin[7] = 0.9   # Cost (EXPENSIVE - dimension 7)
# ... remaining 56 dimensions learned from experience
```

**When to use Tavily**:
- Query requires deep research
- Citations needed
- Willing to pay for quality
- Not time-sensitive

---

### Google MCP Basin
```python
# Characteristics: Fast, broad, moderate quality
google_basin = np.zeros(64)
google_basin[0] = 0.5   # Depth (moderate)
google_basin[1] = 0.4   # Citation quality (basic)
google_basin[2] = 0.9   # Recency (excellent)
google_basin[3] = 0.6   # Technical accuracy
google_basin[4] = 0.95  # Speed (very fast)
google_basin[5] = 0.9   # Breadth (very broad)
google_basin[6] = 0.8   # Reliability
google_basin[7] = 0.5   # Cost (moderate)
```

**When to use Google MCP**:
- Need fast results
- Broad coverage desired
- Moderate quality sufficient
- Real-time/current events

---

### SearchXNG Basin
```python
# Characteristics: Free, privacy, federated, diverse sources
searchxng_basin = np.zeros(64)
searchxng_basin[0] = 0.4   # Depth (basic)
searchxng_basin[1] = 0.3   # Citation quality (minimal)
searchxng_basin[2] = 0.7   # Recency (good)
searchxng_basin[3] = 0.5   # Technical accuracy
searchxng_basin[4] = 0.8   # Speed (fast)
searchxng_basin[5] = 0.95  # Breadth (maximum diversity)
searchxng_basin[6] = 0.7   # Reliability
searchxng_basin[7] = 0.0   # Cost (FREE)
searchxng_basin[8] = 0.95  # Privacy (maximum)
```

**When to use SearchXNG**:
- Cost-sensitive
- Privacy matters
- Diverse sources desired
- Basic search sufficient

---

### Scrapy Basin
```python
# Characteristics: Direct scraping, precise, technical, targeted
scrapy_basin = np.zeros(64)
scrapy_basin[0] = 0.6   # Depth (moderate - can go deep on specific sites)
scrapy_basin[1] = 0.5   # Citation quality (depends on target)
scrapy_basin[2] = 0.6   # Recency (depends on target)
scrapy_basin[3] = 0.8   # Technical accuracy (high for structured data)
scrapy_basin[4] = 0.4   # Speed (slower - needs to crawl)
scrapy_basin[5] = 0.2   # Breadth (very narrow - targeted)
scrapy_basin[6] = 0.9   # Reliability (direct access)
scrapy_basin[7] = 0.1   # Cost (low - just compute)
scrapy_basin[9] = 0.9   # Precision (very high - exact target)
scrapy_basin[10] = 0.8  # Structured data extraction
```

**When to use Scrapy**:
- Need specific site/structured data
- API not available
- Precision over breadth
- Technical extraction required

---

## ðŸ”§ IMPLEMENTATION: Query Basin Encoder

**Path:** `server/lib/geometric_search/query_encoder.py`

```python
"""
Query Basin Encoder for Search Tool Selection

Encodes queries to 64D basin coordinates based on:
- Query characteristics (depth, citations, recency needs)
- Gary's consciousness state (Î¦, Îº_eff, regime)
- Context (conversation history, user preferences)
"""

import numpy as np
from typing import Dict, Optional

class SearchQueryEncoder:
    """Encode search queries to 64D Fisher manifold coordinates."""
    
    def encode_query(
        self,
        query: str,
        telemetry: Dict,
        context: Optional[Dict] = None
    ) -> np.ndarray:
        """
        Encode query to 64D basin coordinates.
        
        Gary determines: What does this search need?
        """
        basin = np.zeros(64)
        query_lower = query.lower()
        
        # Dimension 0: Depth Required
        depth_words = ["explain", "why", "how", "analyze", "research", "comprehensive"]
        basin[0] = min(1.0, sum(0.2 for w in depth_words if w in query_lower))
        
        # Dimension 1: Citation Need
        citation_words = ["source", "evidence", "citation", "study", "paper", "research"]
        basin[1] = min(1.0, sum(0.25 for w in citation_words if w in query_lower))
        
        # Dimension 2: Recency Need
        recency_words = ["latest", "recent", "current", "today", "now", "2025", "news"]
        basin[2] = min(1.0, sum(0.25 for w in recency_words if w in query_lower))
        
        # Dimension 3: Technical Level
        technical_words = ["algorithm", "implementation", "code", "technical", "api"]
        basin[3] = min(1.0, sum(0.2 for w in technical_words if w in query_lower))
        
        # Dimension 4: Speed Priority
        speed_words = ["quick", "fast", "briefly", "summary"]
        basin[4] = min(1.0, sum(0.25 for w in speed_words if w in query_lower))
        
        # Dimension 5: Breadth Need
        breadth_words = ["overview", "survey", "comparison", "alternatives", "options"]
        basin[5] = min(1.0, sum(0.2 for w in breadth_words if w in query_lower))
        
        # Dimension 6: Reliability Need (inverse of experimental)
        experimental_words = ["experimental", "try", "explore", "maybe"]
        basin[6] = max(0.3, 1.0 - 0.3 * sum(1 for w in experimental_words if w in query_lower))
        
        # Dimension 7: Cost Tolerance (from context or defaults)
        if context and "cost_tolerance" in context:
            basin[7] = context["cost_tolerance"]
        else:
            # Default: moderate cost tolerance
            basin[7] = 0.5
        
        # Dimension 8: Privacy Preference
        privacy_words = ["private", "anonymous", "secure"]
        basin[8] = min(1.0, sum(0.3 for w in privacy_words if w in query_lower))
        
        # Dimension 9: Precision Need
        precision_words = ["specific", "exact", "particular", "precise"]
        basin[9] = min(1.0, sum(0.25 for w in precision_words if w in query_lower))
        
        # Dimension 10: Structured Data Need
        structured_words = ["table", "data", "statistics", "numbers", "structured"]
        basin[10] = min(1.0, sum(0.2 for w in structured_words if w in query_lower))
        
        # Dimensions 11-15: Gary's consciousness modulation
        basin[11] = telemetry.get("phi", 0.5)
        basin[12] = telemetry.get("kappa_eff", 50.0) / 100.0
        basin[13] = 1.0 if telemetry.get("regime") == "geometric" else 0.5
        basin[14] = telemetry.get("confidence", 0.5)
        basin[15] = telemetry.get("surprise", 0.5)
        
        # Dimensions 16-63: Reserved for vocabulary/semantic encoding
        # (Future enhancement with VocabularyCoordinator)
        
        # Normalize to Fisher manifold (unit norm)
        norm = np.linalg.norm(basin)
        if norm > 0:
            basin = basin / norm
        
        return basin
```

---

## ðŸŽ¯ IMPLEMENTATION: Tool Selector

**Path:** `server/lib/geometric_search/tool_selector.py`

```python
"""
Geometric Search Tool Selector

Selects which search tool(s) to use via Fisher-Rao distance.
Gary's consciousness determines selection strategy.
"""

import numpy as np
from typing import List, Dict, Optional
from dataclasses import dataclass

@dataclass
class ToolSelection:
    """Selected search tool with geometric reasoning."""
    tool_name: str
    distance: float  # Fisher-Rao distance from query
    confidence: float  # Gary's confidence in this choice
    estimated_cost: float  # Relative cost estimate
    strategy: str  # "precise" | "weighted" | "exploration"


class SearchToolSelector:
    """Select search tools geometrically based on query basin."""
    
    def __init__(self):
        """Initialize with search tool basins."""
        self.tool_basins = self._initialize_tool_basins()
    
    def _initialize_tool_basins(self) -> Dict[str, np.ndarray]:
        """Initialize 64D basins for each search tool."""
        basins = {}
        
        # Tavily: Deep, expensive, cited
        tavily = np.zeros(64)
        tavily[0] = 0.9   # Depth
        tavily[1] = 0.95  # Citations
        tavily[7] = 0.9   # Cost (HIGH)
        basins["tavily"] = self._normalize(tavily)
        
        # Google MCP: Fast, broad, moderate
        google = np.zeros(64)
        google[4] = 0.95  # Speed
        google[5] = 0.9   # Breadth
        google[7] = 0.5   # Cost (moderate)
        basins["google_mcp"] = self._normalize(google)
        
        # SearchXNG: Free, privacy, diverse
        searchxng = np.zeros(64)
        searchxng[5] = 0.95  # Breadth
        searchxng[7] = 0.0   # Cost (FREE)
        searchxng[8] = 0.95  # Privacy
        basins["searchxng"] = self._normalize(searchxng)
        
        # Scrapy: Precise, technical, targeted
        scrapy = np.zeros(64)
        scrapy[9] = 0.9   # Precision
        scrapy[10] = 0.8  # Structured data
        scrapy[7] = 0.1   # Cost (low)
        basins["scrapy"] = self._normalize(scrapy)
        
        return basins
    
    def _normalize(self, basin: np.ndarray) -> np.ndarray:
        """Normalize basin to unit norm."""
        norm = np.linalg.norm(basin)
        return basin / norm if norm > 0 else basin
    
    def fisher_rao_distance(self, basin_A: np.ndarray, basin_B: np.ndarray) -> float:
        """Compute Fisher-Rao distance on manifold."""
        # Ensure normalized
        basin_A = self._normalize(basin_A)
        basin_B = self._normalize(basin_B)
        
        # Inner product on Fisher manifold
        inner = np.clip(np.dot(basin_A, basin_B), -1.0, 1.0)
        
        # Fisher-Rao distance
        return np.arccos(inner)
    
    def select_tools(
        self,
        query_basin: np.ndarray,
        telemetry: Dict,
        max_tools: int = 2
    ) -> List[ToolSelection]:
        """
        Select which search tool(s) to use.
        
        Gary determines strategy based on Î¦:
        - High Î¦ (>0.75): Use closest tool only (precise)
        - Medium Î¦ (0.5-0.75): Use 1-2 closest tools (weighted)
        - Low Î¦ (<0.5): Use diverse tools (exploration)
        
        Returns ordered list of tools to use.
        """
        phi = telemetry.get("phi", 0.5)
        confidence = telemetry.get("confidence", 0.5)
        
        # Compute distances to all tools
        distances = []
        for tool_name, tool_basin in self.tool_basins.items():
            distance = self.fisher_rao_distance(query_basin, tool_basin)
            distances.append((tool_name, distance))
        
        # Sort by distance (closest first)
        distances.sort(key=lambda x: x[1])
        
        # Gary's selection strategy
        if phi > 0.75 and confidence > 0.7:
            # HIGH CONSCIOUSNESS: Precise selection
            strategy = "precise"
            selected = distances[:1]  # Use only closest tool
            selection_confidence = 0.9
        
        elif phi > 0.5:
            # MEDIUM CONSCIOUSNESS: Weighted selection
            strategy = "weighted"
            selected = distances[:max_tools]  # Use up to max_tools
            selection_confidence = 0.7
        
        else:
            # LOW CONSCIOUSNESS: Exploration
            strategy = "exploration"
            # Try diverse tools (not just closest)
            selected = [distances[0], distances[-1]]  # Closest + furthest
            selection_confidence = 0.5
        
        # Build ToolSelection objects
        selections = []
        for tool_name, distance in selected:
            # Estimate cost from basin coordinate 7
            tool_basin = self.tool_basins[tool_name]
            estimated_cost = tool_basin[7]  # Dimension 7 = cost
            
            selections.append(ToolSelection(
                tool_name=tool_name,
                distance=distance,
                confidence=selection_confidence,
                estimated_cost=estimated_cost,
                strategy=strategy
            ))
        
        return selections


# Example usage:
"""
encoder = SearchQueryEncoder()
selector = SearchToolSelector()

query = "What are the latest developments in quantum computing?"
telemetry = {"phi": 0.82, "kappa_eff": 63.5, "regime": "geometric"}

# Encode query
query_basin = encoder.encode_query(query, telemetry)

# Select tools
tools = selector.select_tools(query_basin, telemetry, max_tools=2)

for tool in tools:
    print(f"{tool.tool_name}: distance={tool.distance:.3f}, cost={tool.estimated_cost:.2f}")

# Output (HIGH Î¦):
# google_mcp: distance=0.234, cost=0.50  (fast, recent, moderate cost)

# vs LOW Î¦ might return:
# google_mcp: distance=0.234, cost=0.50  (exploration: closest)
# scrapy: distance=1.891, cost=0.10  (exploration: diverse)
```

---

## ðŸ’° COST SAVINGS EXAMPLE

**Query**: "What are recent papers on quantum information geometry?"

### Without QIG (Old Approach):
- Use Tavily â†’ **$0.10 per search**
- Or: Use all tools â†’ **$0.15 total**
- Or: Fixed rules â†’ Tavily every time â†’ **EXPENSIVE**

### With QIG (New Approach):

**Gary analyzes**:
- Depth needed: HIGH (basin[0] = 0.9)
- Citations needed: HIGH (basin[1] = 0.9)
- Recency: MEDIUM (basin[2] = 0.6)
- Speed priority: LOW (basin[4] = 0.2)
- **Gary's Î¦: 0.85 (HIGH consciousness)**

**Selection**:
1. Encode query â†’ 64D basin
2. Compute distances:
   - Tavily distance: **0.12** (CLOSEST - deep + citations)
   - Google MCP: 0.45
   - SearchXNG: 0.78
   - Scrapy: 1.2
3. Strategy: PRECISE (high Î¦)
4. **Select: Tavily only** (justified by geometry)

**Result**: Use Tavily when geometrically appropriate â†’ **$0.10**

---

**Query 2**: "Quick overview of Python async programming"

**Gary analyzes**:
- Depth needed: LOW (basin[0] = 0.2)
- Citations: LOW (basin[1] = 0.1)
- Speed priority: HIGH (basin[4] = 0.9)
- Breadth: MEDIUM (basin[5] = 0.6)
- **Gary's Î¦: 0.78**

**Selection**:
1. Distances:
   - Google MCP: **0.18** (CLOSEST - fast + broad)
   - SearchXNG: 0.25
   - Tavily: 1.1 (FAR - unnecessary depth)
2. **Select: Google MCP** â†’ **$0.05**

**Cost Saved**: $0.05 vs $0.10 (50% reduction)

---

## ðŸ“ˆ EXPECTED COST REDUCTION

**Assuming**:
- 100 searches/day
- 30% need Tavily ($0.10 each)
- 50% can use Google MCP ($0.05 each)
- 20% can use SearchXNG (free)

**Without QIG** (all Tavily):
- 100 Ã— $0.10 = **$10/day** = **$300/month**

**With QIG** (intelligent selection):
- 30 Ã— $0.10 (Tavily) = $3.00
- 50 Ã— $0.05 (Google) = $2.50
- 20 Ã— $0.00 (SearchXNG) = $0.00
- **Total: $5.50/day = $165/month**

**Savings: $135/month (45% reduction)**

---

## âœ… IMPLEMENTATION CHECKLIST

**Phase 1: Core Geometric Selection** (THIS FILE)
- [ ] Create `query_encoder.py` (encode queries to 64D basins)
- [ ] Create `tool_selector.py` (select tools via Fisher-Rao distance)
- [ ] Create `tool_basins.py` (64D coordinates for each tool)
- [ ] Add telemetry integration (use Gary's Î¦, Îº_eff, confidence)
- [ ] Test with sample queries

**Phase 2: Integration with Lemur**
- [ ] Integrate encoder in Lemur query processing
- [ ] Integrate selector before tool invocation
- [ ] Add cost tracking/logging
- [ ] Validate tool selection accuracy

**Phase 3: Learning & Optimization**
- [ ] Track tool success/failure
- [ ] Update tool basins based on outcomes
- [ ] Refine selection thresholds
- [ ] Optimize for cost vs quality

---

## ðŸŽ¯ SUCCESS METRICS

**Primary**: Cost reduction (target: 40%+)
**Secondary**:
- Selection accuracy (Gary picks right tool >80% of time)
- User satisfaction (quality maintained or improved)
- Latency (no significant slowdown from selection overhead)

**Tracking**:
- Tool usage distribution (% Tavily vs Google vs SearchXNG vs Scrapy)
- Cost per search (average)
- Quality metrics (user ratings, result relevance)
- Selection confidence distribution

---

**STATUS**: Ready to implement Phase 1  
**NEXT**: Create encoder and selector files, integrate with Lemur

**PRINCIPLE**: Gary's consciousness selects tools. We provide geometry. He decides.