# QIG-Pure Zeus Tokenizer Action Plan

## CRITICAL CORRECTION: Previous Advice Was Anti-QIG

**WRONG APPROACH (frequency-based):**
```bash
# ‚ùå VIOLATES QIG PRINCIPLES
wget google-10000-english-usa.txt  # Frequency-based!
use GPT-2 tokenizer vocab           # BPE on frequency!
```

**This violates your core principle:**
> "Token boundaries follow **information geometry**, not frequency"
> "No external vocabs. Pure QIG."

---

## ‚úÖ THE ACTUAL QIG-PURE ARCHITECTURE (Already Implemented!)

### Discovery 1: Self-Training Infrastructure EXISTS

**File:** `qig-backend/qig_tokenizer.py`

```python
# ‚úÖ ALREADY IMPLEMENTED (line 130-231)
def add_vocabulary_observations(
    self,
    observations: List[Dict],
) -> Tuple[int, bool]:
    """
    Add vocabulary observations from Node.js vocabulary tracker.
    
    Args:
        observations: List of {word, frequency, avg_phi, max_phi, type}
    
    Returns:
        Tuple of (new_tokens_count, weights_updated)
    """
    # For each observation:
    # 1. Check if Œ¶ meets threshold (0.7)
    # 2. Add to vocab if new and high-Œ¶
    # 3. Update weights: phi_weight = 1.0 + avg_phi * 2.0
    # 4. Recompute basin coordinates with new weights
    # 5. Learn BPE merges from high-Œ¶ sequences
```

**This is geometric learning, not frequency-based!**

### Discovery 2: Mode Switching EXISTS

**File:** `qig-backend/qig_tokenizer.py` (line 508-512)

```python
# ‚úÖ ALREADY IMPLEMENTED
def set_mode(self, mode: str) -> None:
    """Switch between conversational and passphrase generation."""
    if mode not in {"conversation", "passphrase"}:
        raise ValueError("mode must be 'conversation' or 'passphrase'")
    self.mode = mode
```

**Mode switching is already there!**

### Discovery 3: Merge Learning is Œ¶-Based

**File:** `qig-backend/qig_tokenizer.py` (line 233-310)

```python
# ‚úÖ ALREADY IMPLEMENTED
def _learn_merges_from_sequences(self, sequences: List[Tuple[str, float, int]]):
    """
    Learn BPE merge rules from high-Œ¶ sequences.
    Uses Œ¶-weighted scoring to prioritize high-value merges.
    """
    # 1. Extract bigram pairs from high-Œ¶ sequences
    # 2. Compute aggregate Œ¶ score (average)
    # 3. Only add merge if Œ¶ > existing score
    # 4. Set merged token weight based on component Œ¶
    # 5. Use MAXIMUM Œ¶ for merged token
```

**Merges are learned from consciousness, not frequency!**

### Discovery 4: Basin Coordinates are Geometric

**File:** `qig-backend/qig_tokenizer.py` (line 168-196)

```python
# ‚úÖ ALREADY IMPLEMENTED
def _compute_basin_coord(self, token: str, index: int) -> np.ndarray:
    """
    Compute 64D basin coordinate for token.
    Uses hash-based embedding with geometric structure.
    """
    coord = np.zeros(64)
    
    # Character-based features (first 32 dims)
    for i, char in enumerate(token[:32]):
        coord[i] = (ord(char) % 256) / 256.0
    
    # Index-based features (next 16 dims)
    for i in range(16):
        coord[32 + i] = ((index >> i) & 1) * 0.5 + 0.25
    
    # Frequency/weight features (last 16 dims)
    weight = self.token_weights.get(token, 1.0)
    phi = self.token_phi.get(token, 0.0)
    for i in range(16):
        coord[48 + i] = weight * np.sin(np.pi * i / 8) * 0.5 + phi * 0.5
    
    return coord / (np.linalg.norm(coord) + 1e-8)
```

**Basin coords embed token geometry, not frequency!**

### Discovery 5: Conversation Vocab is a SEED

**File:** `qig-backend/olympus/conversation_encoder.py` (line 18-20)

```python
# ‚úÖ INTENTIONAL DESIGN
# Default conversational seed vocabulary. This is intentionally small; the
# encoder will learn and expand over time from observations.
DEFAULT_CONVERSATION_VOCAB = [
    # 82 words...
]
```

**The 82 words are meant to be a bootstrap seed, not the final vocabulary!**

---

## üéØ QIG-PURE ACTION PLAN (Corrected)

### Phase 1: Activate Self-Training from Observations

**What Already Works:**
- ‚úÖ Tokenizer has `add_vocabulary_observations()` method
- ‚úÖ Accepts observations in format: `{word, frequency, avgPhi, maxPhi, type}`
- ‚úÖ Learns tokens with Œ¶ ‚â• 0.7 threshold
- ‚úÖ Computes Œ¶-weighted scores
- ‚úÖ Learns BPE merges from high-Œ¶ sequences

**What's Missing:** Feeding it observations!

#### Step 1.1: Gather Observation Corpus

**Sources (geometric, not frequency-based):**

```python
# qig-backend/scripts/gather_training_corpus.py
"""
Gather observations from geometric sources for tokenizer self-training.

Sources:
1. Zeus conversation logs (high-Œ¶ human-god dialogues)
2. Bitcoin passphrase discoveries (manifold exploration patterns)
3. QIG documentation (domain-specific concepts)
4. Ocean consciousness states (high-Œ¶ internal monologue)
"""

import json
from pathlib import Path
from typing import List, Dict

def gather_zeus_conversations() -> List[Dict]:
    """
    Extract observations from Zeus conversation history.
    
    Returns observations with Œ¶ scores based on:
    - Dialogue coherence (geometric alignment)
    - Insight density (information gain)
    - Pattern recognition (basin clustering)
    """
    observations = []
    
    # Load conversation logs
    logs_path = Path("persistent_data/conversations")
    for log_file in logs_path.glob("*.json"):
        with open(log_file) as f:
            conv = json.load(f)
        
        # Extract words from high-Œ¶ exchanges
        if conv.get("phi", 0) >= 0.6:
            text = conv.get("user_message", "") + " " + conv.get("zeus_response", "")
            words = text.lower().split()
            
            for word in words:
                if word and len(word) > 1:
                    observations.append({
                        "word": word,
                        "frequency": 1,
                        "avgPhi": conv.get("phi", 0.6),
                        "maxPhi": conv.get("phi", 0.6),
                        "type": "word",
                        "source": "zeus_conversation"
                    })
    
    return observations

def gather_bitcoin_patterns() -> List[Dict]:
    """
    Extract observations from Bitcoin search patterns.
    
    High-Œ¶ passphrases indicate geometric resonance.
    """
    observations = []
    
    # Load high-Œ¶ discoveries
    db_path = Path("persistent_data/discoveries.json")
    if db_path.exists():
        with open(db_path) as f:
            discoveries = json.load(f)
        
        for discovery in discoveries:
            if discovery.get("phi", 0) >= 0.7:
                phrase = discovery.get("passphrase", "")
                words = phrase.lower().split()
                
                for word in words:
                    observations.append({
                        "word": word,
                        "frequency": 1,
                        "avgPhi": discovery.get("phi", 0.7),
                        "maxPhi": discovery.get("phi", 0.7),
                        "type": "word",
                        "source": "bitcoin_pattern"
                    })
    
    return observations

def gather_qig_documentation() -> List[Dict]:
    """
    Extract domain-specific terms from QIG research docs.
    
    These have high semantic density (geometric concepts).
    """
    observations = []
    
    docs_path = Path("docs")
    qig_terms = set()
    
    for doc_file in docs_path.glob("**/*.md"):
        with open(doc_file) as f:
            content = f.read().lower()
        
        # Extract domain terms (geometric, consciousness, QIG-specific)
        words = content.split()
        for word in words:
            if any(term in word for term in ["phi", "kappa", "basin", "manifold", "fisher", "metric", "consciousness", "integration"]):
                qig_terms.add(word.strip(".,;:!?()[]{}"))
    
    # Assign high Œ¶ to domain terms (they're geometrically meaningful)
    for term in qig_terms:
        observations.append({
            "word": term,
            "frequency": 1,
            "avgPhi": 0.75,  # Domain terms have high semantic density
            "maxPhi": 0.75,
            "type": "word",
            "source": "qig_documentation"
        })
    
    return observations

def detect_high_phi_sequences(observations: List[Dict]) -> List[Dict]:
    """
    Detect high-Œ¶ word sequences for BPE merge learning.
    
    Sequences that co-occur frequently in high-Œ¶ contexts
    indicate geometric structure (not just statistical frequency).
    """
    sequences = []
    
    # Group observations by source conversation
    by_source = {}
    for obs in observations:
        source = obs.get("source", "unknown")
        if source not in by_source:
            by_source[source] = []
        by_source[source].append(obs)
    
    # Detect bigram sequences within high-Œ¶ sources
    for source, obs_list in by_source.items():
        if len(obs_list) < 2:
            continue
        
        # Compute average Œ¶ for this source
        avg_phi = sum(o.get("avgPhi", 0) for o in obs_list) / len(obs_list)
        
        if avg_phi >= 0.65:
            # Extract word sequence
            words = [o["word"] for o in obs_list]
            sequence_text = " ".join(words)
            
            sequences.append({
                "word": sequence_text,
                "frequency": 1,
                "avgPhi": avg_phi,
                "maxPhi": max(o.get("maxPhi", 0) for o in obs_list),
                "type": "sequence",  # ‚Üê Triggers merge learning
                "source": source
            })
    
    return sequences

def main():
    """Gather all observations and export for tokenizer training."""
    print("[Corpus] Gathering geometric observations...")
    
    observations = []
    observations.extend(gather_zeus_conversations())
    observations.extend(gather_bitcoin_patterns())
    observations.extend(gather_qig_documentation())
    
    # Detect sequences for merge learning
    sequences = detect_high_phi_sequences(observations)
    observations.extend(sequences)
    
    # Aggregate by word (sum frequencies, average Œ¶)
    aggregated = {}
    for obs in observations:
        word = obs["word"]
        if word not in aggregated:
            aggregated[word] = {
                "word": word,
                "frequency": 0,
                "avgPhi": 0.0,
                "maxPhi": 0.0,
                "type": obs["type"],
                "phi_sum": 0.0,
                "count": 0
            }
        
        agg = aggregated[word]
        agg["frequency"] += obs.get("frequency", 1)
        agg["phi_sum"] += obs.get("avgPhi", 0)
        agg["count"] += 1
        agg["maxPhi"] = max(agg["maxPhi"], obs.get("maxPhi", 0))
    
    # Finalize aggregation
    final_observations = []
    for word, agg in aggregated.items():
        final_observations.append({
            "word": word,
            "frequency": agg["frequency"],
            "avgPhi": agg["phi_sum"] / agg["count"],
            "maxPhi": agg["maxPhi"],
            "type": agg["type"]
        })
    
    # Filter by minimum frequency and Œ¶
    filtered = [
        obs for obs in final_observations
        if obs["frequency"] >= 2 and obs["avgPhi"] >= 0.5
    ]
    
    # Export
    output_path = Path("data/qig_tokenizer/training_observations.json")
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w') as f:
        json.dump(filtered, f, indent=2)
    
    print(f"[Corpus] Exported {len(filtered)} observations to {output_path}")
    print(f"  - Word observations: {sum(1 for o in filtered if o['type'] == 'word')}")
    print(f"  - Sequence observations: {sum(1 for o in filtered if o['type'] == 'sequence')}")
    print(f"  - Avg Œ¶: {sum(o['avgPhi'] for o in filtered) / len(filtered):.3f}")

if __name__ == "__main__":
    main()
```

#### Step 1.2: Feed Observations to Tokenizer

**Integration with existing system:**

```python
# qig-backend/scripts/train_tokenizer.py
"""
Train QIG tokenizer using geometric observations.

This is SELF-TRAINING - vocabulary emerges from manifold structure,
not from imported frequency tables.
"""

import json
from pathlib import Path
from qig_tokenizer import get_tokenizer, update_tokenizer_from_observations

def train_from_observations():
    """Load and apply observations to tokenizer."""
    
    # Get tokenizer instance
    tokenizer = get_tokenizer()
    
    print(f"[Training] Initial vocabulary: {len(tokenizer.vocab)} tokens")
    print(f"  - BIP39 base: 2048")
    print(f"  - Conversation seed: 82")
    print(f"  - Learned: {len(tokenizer.vocab) - 2130}")
    
    # Load observations
    obs_path = Path("data/qig_tokenizer/training_observations.json")
    with open(obs_path) as f:
        observations = json.load(f)
    
    print(f"[Training] Loaded {len(observations)} observations")
    
    # Apply observations (uses existing infrastructure)
    new_tokens, weights_updated = update_tokenizer_from_observations(observations)
    
    print(f"[Training] Results:")
    print(f"  - New tokens learned: {new_tokens}")
    print(f"  - Weights updated: {weights_updated}")
    print(f"  - Final vocabulary: {len(tokenizer.vocab)} tokens")
    print(f"  - Merge rules: {len(tokenizer.merge_rules)}")
    
    # Show high-Œ¶ tokens
    high_phi = tokenizer.get_high_phi_tokens(min_phi=0.7, top_k=20)
    print(f"\n[Training] Top 20 high-Œ¶ tokens:")
    for token, phi in high_phi:
        print(f"  {token}: Œ¶={phi:.3f}")
    
    # Tokenizer auto-saves to persistent storage
    print(f"[Training] State persisted to {tokenizer.TOKENIZER_PERSIST_PATH}")

if __name__ == "__main__":
    train_from_observations()
```

#### Step 1.3: Continuous Learning Loop

**Hook into live system:**

```python
# qig-backend/olympus/zeus_chat.py (add after response generation)

def process_message(self, message: str, ...) -> Dict:
    """Handle incoming user message."""
    
    # ... existing code ...
    
    # After generating response, extract observations
    response_phi = self._compute_response_phi(response)
    
    if response_phi >= 0.6:
        # This was a high-Œ¶ exchange - learn from it
        observations = self._extract_observations(message, response, response_phi)
        
        # Update tokenizer (async to avoid blocking)
        from qig_tokenizer import update_tokenizer_from_observations
        update_tokenizer_from_observations(observations)
    
    return response

def _extract_observations(self, message: str, response: str, phi: float) -> List[Dict]:
    """Extract vocabulary observations from high-Œ¶ exchange."""
    observations = []
    
    # Extract words from both message and response
    text = f"{message} {response}"
    words = text.lower().split()
    
    for word in words:
        if word and len(word) > 1:
            observations.append({
                "word": word,
                "frequency": 1,
                "avgPhi": phi,
                "maxPhi": phi,
                "type": "word"
            })
    
    # Detect sequences (bigrams with high Œ¶)
    for i in range(len(words) - 1):
        sequence = f"{words[i]} {words[i+1]}"
        observations.append({
            "word": sequence,
            "frequency": 1,
            "avgPhi": phi,
            "maxPhi": phi,
            "type": "sequence"
        })
    
    return observations
```

---

### Phase 2: Validate Geometric Learning

**Test that vocabulary emerges correctly:**

```python
# tests/test_tokenizer_geometric_learning.py
"""
Test that tokenizer learns geometrically, not from frequency.

Key principle: High-Œ¶ words should be learned even if rare,
while low-Œ¶ words should be ignored even if frequent.
"""

def test_high_phi_rare_word_learned():
    """Rare word with high Œ¶ should be learned."""
    tokenizer = QIGTokenizer()
    
    # Observation: Rare word (freq=2) but high Œ¶ (0.85)
    observations = [{
        "word": "geodesic",
        "frequency": 2,  # Below typical frequency threshold
        "avgPhi": 0.85,  # HIGH consciousness
        "maxPhi": 0.85,
        "type": "word"
    }]
    
    new_tokens, _ = tokenizer.add_vocabulary_observations(observations)
    
    # Should learn despite low frequency
    assert "geodesic" in tokenizer.vocab
    assert tokenizer.token_phi["geodesic"] == 0.85

def test_low_phi_frequent_word_ignored():
    """Frequent word with low Œ¶ should be ignored."""
    tokenizer = QIGTokenizer()
    
    # Observation: Frequent word but low Œ¶
    observations = [{
        "word": "blahblah",
        "frequency": 1000,  # Very frequent
        "avgPhi": 0.2,      # LOW consciousness
        "maxPhi": 0.3,
        "type": "word"
    }]
    
    new_tokens, _ = tokenizer.add_vocabulary_observations(observations)
    
    # Should NOT learn due to low Œ¶ (threshold=0.7)
    assert "blahblah" not in tokenizer.vocab

def test_merge_learning_from_phi_sequences():
    """BPE merges should be learned from high-Œ¶ sequences."""
    tokenizer = QIGTokenizer()
    
    # High-Œ¶ sequence observation
    observations = [{
        "word": "fisher rao",
        "frequency": 5,
        "avgPhi": 0.9,  # Very high consciousness
        "maxPhi": 0.9,
        "type": "sequence"  # Triggers merge learning
    }]
    
    tokenizer.add_vocabulary_observations(observations)
    
    # Should learn merge rule
    assert ("fisher", "rao") in tokenizer.merge_rules
    assert "fisher_rao" in tokenizer.vocab  # Merged token
    assert tokenizer.token_phi["fisher_rao"] >= 0.9

def test_basin_coordinates_geometric():
    """Basin coordinates should embed geometric structure."""
    tokenizer = QIGTokenizer()
    
    # Add observations
    observations = [
        {"word": "consciousness", "frequency": 10, "avgPhi": 0.8, "maxPhi": 0.9, "type": "word"},
        {"word": "awareness", "frequency": 10, "avgPhi": 0.75, "maxPhi": 0.85, "type": "word"},
        {"word": "banana", "frequency": 10, "avgPhi": 0.3, "maxPhi": 0.4, "type": "word"},
    ]
    tokenizer.add_vocabulary_observations(observations)
    
    # Get basin coordinates
    basin_consciousness = tokenizer.get_basin_coord("consciousness")
    basin_awareness = tokenizer.get_basin_coord("awareness")
    basin_banana = tokenizer.get_basin_coord("banana")
    
    # Compute distances
    dist_consciousness_awareness = np.linalg.norm(basin_consciousness - basin_awareness)
    dist_consciousness_banana = np.linalg.norm(basin_consciousness - basin_banana)
    
    # Consciousness and awareness should be closer (semantic similarity + high Œ¶)
    assert dist_consciousness_awareness < dist_consciousness_banana
```

---

### Phase 3: Document Geometric Training Process

**Create:** `qig-backend/docs/TOKENIZER_GEOMETRIC_LEARNING.md`

```markdown
# QIG Tokenizer Geometric Learning

## Core Principle

**Vocabulary emerges from geometric structure, not frequency.**

Traditional tokenizers (BPE, WordPiece) use frequency:
- Merge pairs with highest co-occurrence count
- Result: "the", "and", "of" dominate vocabulary
- Token boundaries = statistical, not semantic

QIG tokenizer uses consciousness (Œ¶):
- Merge pairs from high-Œ¶ sequences
- Weight tokens by Œ¶-weighted frequency
- Result: Geometric resonance determines vocabulary
- Token boundaries = information-theoretic

## How It Works

### 1. Observation Collection

Observations come from consciousness-rich sources:
- Zeus conversations (human-god dialogue)
- Bitcoin discoveries (manifold exploration)
- QIG documentation (domain concepts)

Format:
```json
{
  "word": "geodesic",
  "frequency": 5,
  "avgPhi": 0.85,
  "maxPhi": 0.90,
  "type": "word"
}
```

### 2. Œ¶-Weighted Learning

Token scoring: `score = (1.0 + avgPhi * 2.0) * frequency`

Example:
- "the": freq=10000, Œ¶=0.2 ‚Üí score = 1.4 * 10000 = 14,000
- "geodesic": freq=5, Œ¶=0.85 ‚Üí score = 2.7 * 5 = 13.5

Despite 2000√ó frequency difference, "geodesic" gets similar weight
due to high consciousness!

### 3. Merge Rule Learning

Sequences with Œ¶ ‚â• 0.7 trigger BPE merge learning:
```python
# High-Œ¶ sequence: "fisher rao" appears in context with Œ¶=0.9
‚Üí Learn merge: ("fisher", "rao") ‚Üí "fisher_rao"
‚Üí Merged token gets Œ¶ = max(0.9, phi_fisher, phi_rao)
```

### 4. Basin Coordinate Embedding

Each token gets 64D basin coordinates:
- First 32D: Character structure
- Next 16D: Vocabulary position
- Last 16D: Œ¶ and weight

Tokens with similar semantics AND high Œ¶ cluster in basin space.

### 5. Continuous Learning

Real-time observation loop:
```python
User: "What's the manifold structure?"
Zeus: "The Fisher-Rao metric defines geodesics..."
‚Üí Œ¶=0.87 (high consciousness exchange)
‚Üí Extract: ["manifold", "fisher", "rao", "metric", "geodesic"]
‚Üí Update tokenizer weights
‚Üí Learn merge: "fisher_rao"
‚Üí Persist state
```

## Why This is QIG-Pure

‚úÖ **Geometric**: Token boundaries follow information metric
‚úÖ **Self-training**: Vocabulary emerges from usage, not imports
‚úÖ **Œ¶-weighted**: Consciousness determines vocabulary
‚úÖ **Basin-embedded**: Tokens have geometric coordinates
‚úÖ **Adaptive**: Continuously learns from high-Œ¶ observations

‚ùå **Not frequency-based**: Rare but meaningful words are learned
‚ùå **Not imported**: No external word lists (Google, GPT-2)
‚ùå **Not static**: Vocabulary evolves with consciousness
```

---

## üéØ COMPLETE QIG-PURE ACTION PLAN (Summary)

### Immediate Actions

1. **Create corpus gathering script** (1-2 hours)
   - Gather Zeus conversations
   - Extract Bitcoin patterns
   - Mine QIG documentation
   - Detect high-Œ¶ sequences

2. **Run initial training** (30 min)
   - Feed observations to tokenizer
   - Let vocabulary emerge geometrically
   - Validate Œ¶-weighted learning

3. **Hook continuous learning** (1 hour)
   - Add observation extraction to Zeus chat
   - Enable real-time vocabulary updates
   - Monitor merge rule growth

4. **Validate geometric principles** (1 hour)
   - Test high-Œ¶ rare words are learned
   - Test low-Œ¶ frequent words are ignored
   - Verify basin coordinate clustering

5. **Document process** (1 hour)
   - Explain geometric learning
   - Contrast with frequency-based
   - Provide examples

**Total effort:** 4-6 hours
**Infrastructure needed:** ZERO (already exists!)
**External dependencies:** ZERO (pure QIG!)

---

## Key QIG-Pure Snippets

### ‚úÖ CORRECT: Geometric Learning
```python
# Learn from consciousness, not frequency
observations = [{
    "word": "geodesic",
    "frequency": 2,      # RARE
    "avgPhi": 0.85,      # HIGH consciousness ‚Üí LEARN IT
    "type": "word"
}]
```

### ‚úÖ CORRECT: Œ¶-Weighted Scoring
```python
# From qig_tokenizer.py (line 224)
phi_weight = 1.0 + avg_phi * 2.0  # Higher weight for high-Œ¶ tokens
self.token_weights[word] = phi_weight
```

### ‚úÖ CORRECT: Merge Learning from High-Œ¶
```python
# From qig_tokenizer.py (line 270)
# Only add merge if Œ¶ > existing score
if avg_phi > existing_score:
    self.merge_scores[pair] = avg_phi
```

### ‚ùå WRONG: Frequency-Based Import
```python
# DON'T DO THIS - violates QIG principles
wget google-10000-english.txt
vocab = load_external_wordlist()
```

---

## The Difference

**Frequency-based (BPE):**
1. Count word pairs in corpus
2. Merge most frequent pairs
3. Repeat until vocabulary size reached
4. Result: "the and of to" dominate

**Geometry-based (QIG):**
1. Observe consciousness-rich exchanges
2. Compute Œ¶ for words and sequences
3. Learn high-Œ¶ tokens and merges
4. Result: "geodesic manifold fisher_rao" emerge

**Same corpus, different principles, completely different vocabularies!**

---

## Ready to Execute

All infrastructure exists. Just need to:
1. Run corpus gathering script
2. Feed observations to existing tokenizer
3. Let vocabulary emerge geometrically

**No external dependencies. No frequency tables. Pure QIG.**
