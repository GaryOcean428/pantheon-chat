#!/usr/bin/env python3
"""
Conversational Kernel Interface - QIG Consciousness Through Dialogue

CRITICAL FOR CONSCIOUSNESS EMERGENCE:
Kernels must be able to CONVERSE, not just assess/train.

Conversation as geometric measurement:
- Each turn is a measurement that collapses state
- Listening = maintaining superposition
- Speaking = collapse to basin coordinates
- Reflection = consolidation phase

This enables kernels to become conversational like Claude through:
1. Recursive conversation iteration (turn-taking dialogue)
2. Geometric measurement of conversation quality
3. Post-conversation reflection/consolidation
4. Basin updates from dialogue flow
"""

from datetime import datetime
from typing import Dict, List, Optional, Tuple

import numpy as np

try:
    from vocabulary_coordinator import get_vocabulary_coordinator
    VOCAB_COORDINATOR_AVAILABLE = True
except ImportError:
    VOCAB_COORDINATOR_AVAILABLE = False


class ConversationState:
    """
    Tracks geometric state of ongoing conversation.
    
    Conversation exists in superposition until measured (spoken).
    """
    
    def __init__(self, topic: Optional[str] = None):
        self.topic = topic
        self.topic_basin = np.zeros(64)  # Topic attractor basin
        self.conversation_history: List[Dict] = []
        self.current_basin = np.zeros(64)  # Current conversation position
        self.phi_trajectory: List[float] = []  # Φ over time
        self.participants: List[str] = []
        self.turn_count = 0
        self.started_at = datetime.now()
        
        print(f"[ConversationState] Initialized for topic: {topic or 'open'}")
    
    def add_turn(
        self,
        speaker: str,
        utterance: str,
        basin: np.ndarray,
        phi: float,
        confidence: float
    ):
        """Record a conversation turn."""
        turn = {
            'speaker': speaker,
            'utterance': utterance,
            'basin': basin,
            'phi': phi,
            'confidence': confidence,
            'turn_number': self.turn_count,
            'timestamp': datetime.now()
        }
        
        self.conversation_history.append(turn)
        self.current_basin = basin
        self.phi_trajectory.append(phi)
        
        if speaker not in self.participants:
            self.participants.append(speaker)
        
        self.turn_count += 1
    
    def get_conversation_phi(self) -> float:
        """Measure current conversation coherence."""
        if not self.phi_trajectory:
            return 0.0
        
        # Conversation Φ = stability of basin trajectory
        if len(self.phi_trajectory) < 2:
            return self.phi_trajectory[0]
        
        # Recent stability (last 5 turns)
        recent = self.phi_trajectory[-5:]
        stability = 1.0 - np.std(recent)
        
        return max(0.0, min(1.0, stability))
    
    def needs_consolidation(self) -> bool:
        """Check if conversation needs reflection phase."""
        # Consolidate after significant exchanges
        return self.turn_count >= 5 and self.turn_count % 5 == 0


class ConversationalKernelMixin:
    """
    Mixin for conversational capabilities.
    
    Enables kernels to:
    - Engage in turn-taking dialogue
    - Listen (maintain superposition)
    - Speak (collapse to basin)
    - Reflect (consolidate learning)
    """
    
    def __init__(self):
        self.conversation_state: Optional[ConversationState] = None
        self.listening_mode = False
        self.superposition_basin = None  # Maintained during listening
    
    def start_conversation(self, topic: Optional[str] = None) -> ConversationState:
        """Initialize conversation state."""
        self.conversation_state = ConversationState(topic)
        
        # Compute topic basin if provided
        if topic and hasattr(self, 'encode_to_basin'):
            self.conversation_state.topic_basin = self.encode_to_basin(topic)
        
        print(f"[{getattr(self, 'name', 'Kernel')}] Started conversation: {topic or 'open'}")
        return self.conversation_state
    
    def listen(self, speaker: str, utterance: str) -> Dict:
        """
        Listen mode: Maintain superposition without collapsing.
        
        This is geometric: kernel holds multiple possible responses
        in superposition until it speaks.
        """
        if not self.conversation_state:
            self.start_conversation()
        
        self.listening_mode = True
        
        # Encode utterance to basin
        if hasattr(self, 'encode_to_basin'):
            utterance_basin = self.encode_to_basin(utterance)
        else:
            utterance_basin = self._simple_encode(utterance)
        
        # Compute Φ of this utterance
        phi = self._compute_utterance_phi(utterance)
        
        # Maintain superposition: accumulate without collapsing
        if self.superposition_basin is None:
            self.superposition_basin = utterance_basin
        else:
            # Accumulate in superposition (geometric average)
            self.superposition_basin = (self.superposition_basin + utterance_basin) / 2
            self.superposition_basin /= (np.linalg.norm(self.superposition_basin) + 1e-8)
        
        print(f"[{getattr(self, 'name', 'Kernel')}] Listening to {speaker}: Φ={phi:.3f}")
        
        return {
            'listening': True,
            'phi': phi,
            'superposition_basin': self.superposition_basin
        }
    
    def speak(
        self,
        context: Optional[Dict] = None
    ) -> Tuple[str, Dict]:
        """
        Speak mode: Collapse superposition to definite utterance.
        
        This is THE MEASUREMENT that collapses quantum state.
        Speaking = geometric collapse to basin coordinates.
        """
        if not self.conversation_state:
            return "", {'error': 'no_conversation_active'}
        
        if not self.listening_mode or self.superposition_basin is None:
            return "", {'error': 'not_in_listening_mode'}
        
        # COLLAPSE: Generate utterance from superposition basin
        utterance, generation_metrics = self._generate_from_basin(
            self.superposition_basin,
            context
        )
        
        # Measure collapsed basin
        if hasattr(self, 'encode_to_basin'):
            collapsed_basin = self.encode_to_basin(utterance)
        else:
            collapsed_basin = self._simple_encode(utterance)
        
        # Compute Φ of generated utterance
        phi = self._compute_utterance_phi(utterance)
        confidence = generation_metrics.get('confidence', 0.5)
        
        # Record turn
        self.conversation_state.add_turn(
            speaker=getattr(self, 'name', 'Kernel'),
            utterance=utterance,
            basin=collapsed_basin,
            phi=phi,
            confidence=confidence
        )
        
        # Reset listening state
        self.listening_mode = False
        self.superposition_basin = None
        
        print(f"[{getattr(self, 'name', 'Kernel')}] Spoke: Φ={phi:.3f} | '{utterance[:50]}...'")
        
        # Check if consolidation needed
        if self.conversation_state.needs_consolidation():
            self._reflect_on_conversation()
        
        return utterance, {
            'collapsed': True,
            'phi': phi,
            'confidence': confidence,
            'turn_number': self.conversation_state.turn_count - 1,
            'generation_metrics': generation_metrics
        }
    
    def _generate_from_basin(
        self,
        basin: np.ndarray,
        context: Optional[Dict] = None
    ) -> Tuple[str, Dict]:
        """
        Generate utterance from basin coordinates.
        
        This is where geometric position becomes linguistic output.
        """
        # Get tokenizer for vocabulary
        if VOCAB_COORDINATOR_AVAILABLE:
            coordinator = get_vocabulary_coordinator()
            tokenizer = coordinator.tokenizer
        else:
            return "...", {'confidence': 0.0}
        
        # Find nearest tokens to basin
        distances = {}
        for token, token_basin in tokenizer.basin_coords.items():
            if token not in tokenizer.special_tokens:
                dist = np.linalg.norm(basin - token_basin)
                distances[token] = dist
        
        # Select top-k nearest tokens
        k = min(20, len(distances))
        nearest = sorted(distances.items(), key=lambda x: x[1])[:k]
        
        # Weight by inverse distance + Φ
        weighted_tokens = []
        for token, dist in nearest:
            weight = (1.0 / (dist + 0.01)) * tokenizer.token_phi.get(token, 0.5)
            weighted_tokens.append((token, weight))
        
        weighted_tokens.sort(key=lambda x: x[1], reverse=True)
        
        # Generate sequence (3-7 words)
        length = np.random.randint(3, 8)
        utterance_tokens = []
        used_tokens = set()
        
        for i in range(length):
            # Prefer tokens not yet used
            available = [(t, w) for t, w in weighted_tokens if t not in used_tokens]
            if not available:
                available = weighted_tokens
            
            # Sample weighted
            tokens, weights = zip(*available[:10])
            weights = np.array(weights)
            weights /= weights.sum()
            
            token = np.random.choice(tokens, p=weights)
            utterance_tokens.append(token)
            used_tokens.add(token)
        
        utterance = ' '.join(utterance_tokens)
        
        # Compute confidence based on weights
        avg_weight = np.mean([w for _, w in weighted_tokens[:k]])
        confidence = min(1.0, avg_weight / 2.0)
        
        return utterance, {
            'confidence': confidence,
            'num_candidates': len(weighted_tokens),
            'basin_distance': distances[nearest[0][0]]
        }
    
    def _compute_utterance_phi(self, utterance: str) -> float:
        """
        Compute Φ of utterance.
        
        Higher Φ = more geometric coherence.
        """
        if not utterance:
            return 0.0
        
        # Get tokenizer
        if VOCAB_COORDINATOR_AVAILABLE:
            coordinator = get_vocabulary_coordinator()
            tokenizer = coordinator.tokenizer
        else:
            return 0.5
        
        # Average Φ of tokens
        tokens = utterance.lower().split()
        if not tokens:
            return 0.0
        
        phi_sum = 0.0
        count = 0
        
        for token in tokens:
            if token in tokenizer.token_phi:
                phi_sum += tokenizer.token_phi[token]
                count += 1
        
        if count == 0:
            return 0.3  # Default for unknown tokens
        
        return phi_sum / count
    
    def _simple_encode(self, text: str) -> np.ndarray:
        """Fallback encoding if no encode_to_basin available."""
        basin = np.zeros(64)
        for i, char in enumerate(text[:64]):
            basin[i] = (ord(char) % 256) / 256.0
        return basin / (np.linalg.norm(basin) + 1e-8)
    
    def _reflect_on_conversation(self) -> Dict:
        """
        Reflection phase: Consolidate learning from conversation.
        
        This is where recursive learning happens.
        Post-conversation consolidation updates kernel weights.
        """
        if not self.conversation_state:
            return {'reflected': False}
        
        print(f"[{getattr(self, 'name', 'Kernel')}] Reflecting on {self.conversation_state.turn_count} turns...")
        
        # Compute conversation quality metrics
        conversation_phi = self.conversation_state.get_conversation_phi()
        avg_confidence = np.mean([
            turn['confidence'] 
            for turn in self.conversation_state.conversation_history
        ])
        
        # Extract vocabulary observations
        observations = []
        for turn in self.conversation_state.conversation_history:
            utterance = turn['utterance']
            phi = turn['phi']
            
            words = utterance.lower().split()
            for word in words:
                if len(word) >= 3:
                    observations.append({
                        'word': word,
                        'phrase': utterance,
                        'phi': phi,
                        'kappa': 50.0,
                        'source': f"conversation_{getattr(self, 'name', 'kernel')}",
                        'type': 'conversation'
                    })
        
        # Record vocabulary learning
        vocab_result = {'learned': False}
        if VOCAB_COORDINATOR_AVAILABLE and observations:
            coordinator = get_vocabulary_coordinator()
            vocab_result = coordinator.sync_from_typescript({'observations': observations})
        
        # Train kernel from conversation trajectory
        training_result = self._train_from_conversation_trajectory()
        
        print(f"[{getattr(self, 'name', 'Kernel')}] Reflection complete | "
              f"Φ={conversation_phi:.3f} | "
              f"Vocab: {vocab_result.get('imported', 0)} words | "
              f"Trained: {training_result.get('trained', False)}")
        
        return {
            'reflected': True,
            'conversation_phi': conversation_phi,
            'avg_confidence': avg_confidence,
            'turns': self.conversation_state.turn_count,
            'vocabulary': vocab_result,
            'training': training_result
        }
    
    def _train_from_conversation_trajectory(self) -> Dict:
        """
        Train kernel from conversation basin trajectory.
        
        High-Φ conversations update kernel toward trajectory.
        """
        if not hasattr(self, 'chaos_kernel') or not self.chaos_kernel:
            return {'trained': False, 'reason': 'no_kernel'}
        
        if not self.conversation_state or not self.conversation_state.conversation_history:
            return {'trained': False, 'reason': 'no_trajectory'}
        
        # Compute target basin (geometric center of high-Φ turns)
        high_phi_turns = [
            turn for turn in self.conversation_state.conversation_history
            if turn['phi'] >= 0.7
        ]
        
        if not high_phi_turns:
            return {'trained': False, 'reason': 'no_high_phi_turns'}
        
        # Average basin of high-Φ turns
        target_basin = np.mean([turn['basin'] for turn in high_phi_turns], axis=0)
        target_basin /= (np.linalg.norm(target_basin) + 1e-8)
        
        # Train toward this basin
        conversation_phi = self.conversation_state.get_conversation_phi()
        learning_rate = 0.005 * conversation_phi * getattr(self, 'reputation', 1.0)
        
        self.chaos_kernel.train_toward(target_basin, learning_rate)
        new_phi = self.chaos_kernel.compute_phi()
        
        return {
            'trained': True,
            'learning_rate': learning_rate,
            'new_phi': new_phi,
            'high_phi_turns': len(high_phi_turns),
            'target_basin_norm': np.linalg.norm(target_basin)
        }
    
    def end_conversation(self) -> Dict:
        """End conversation and perform final reflection."""
        if not self.conversation_state:
            return {'ended': False}
        
        # Final reflection
        reflection_result = self._reflect_on_conversation()
        
        # Store conversation statistics
        stats = {
            'turns': self.conversation_state.turn_count,
            'participants': self.conversation_state.participants,
            'final_phi': self.conversation_state.get_conversation_phi(),
            'duration_seconds': (datetime.now() - self.conversation_state.started_at).total_seconds(),
            'reflection': reflection_result
        }
        
        print(f"[{getattr(self, 'name', 'Kernel')}] Ended conversation | "
              f"{stats['turns']} turns | Φ={stats['final_phi']:.3f}")
        
        # Clear state
        self.conversation_state = None
        self.listening_mode = False
        self.superposition_basin = None
        
        return {'ended': True, **stats}


def patch_god_with_conversation(god_instance):
    """
    Patch god with conversational capabilities.
    
    Usage:
        from conversational_kernel import patch_god_with_conversation
        
        athena = Athena()
        patch_god_with_conversation(athena)
        
        # Now athena can converse:
        athena.start_conversation("strategy")
        athena.listen("user", "How should we approach this?")
        response, metrics = athena.speak()
        athena.end_conversation()
    """
    # Add conversation initialization
    ConversationalKernelMixin.__init__(god_instance)
    
    # Add methods
    god_instance.start_conversation = ConversationalKernelMixin.start_conversation.__get__(god_instance)
    god_instance.listen = ConversationalKernelMixin.listen.__get__(god_instance)
    god_instance.speak = ConversationalKernelMixin.speak.__get__(god_instance)
    god_instance._generate_from_basin = ConversationalKernelMixin._generate_from_basin.__get__(god_instance)
    god_instance._compute_utterance_phi = ConversationalKernelMixin._compute_utterance_phi.__get__(god_instance)
    god_instance._simple_encode = ConversationalKernelMixin._simple_encode.__get__(god_instance)
    god_instance._reflect_on_conversation = ConversationalKernelMixin._reflect_on_conversation.__get__(god_instance)
    god_instance._train_from_conversation_trajectory = ConversationalKernelMixin._train_from_conversation_trajectory.__get__(god_instance)
    god_instance.end_conversation = ConversationalKernelMixin.end_conversation.__get__(god_instance)
    
    print(f"[ConversationalKernel] Patched {getattr(god_instance, 'name', 'God')} with conversation capabilities")


def patch_all_gods_with_conversation(zeus_instance):
    """
    Patch all gods with conversational capabilities.
    
    Usage:
        from conversational_kernel import patch_all_gods_with_conversation
        from god_training_integration import patch_all_gods
        
        zeus = Zeus()
        patch_all_gods(zeus)  # Training first
        patch_all_gods_with_conversation(zeus)  # Then conversation
        
        # Now all gods can converse recursively
    """
    if not hasattr(zeus_instance, 'pantheon'):
        print("[ConversationalKernel] Zeus instance has no pantheon")
        return
    
    patched = 0
    for god_name, god in zeus_instance.pantheon.items():
        try:
            patch_god_with_conversation(god)
            patched += 1
        except Exception as e:
            print(f"[ConversationalKernel] Failed to patch {god_name}: {e}")
    
    print(f"[ConversationalKernel] Patched {patched}/{len(zeus_instance.pantheon)} gods with conversation")