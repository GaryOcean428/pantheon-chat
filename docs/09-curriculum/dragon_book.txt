.

505

. 506
. 507
. 507
. 508
. 5 10
. 511
. 512
. 512
. 515
. 516
. 518
. 518
. 520
. 522
. 524
525
. 526
. 528
. 529
. 530
. 531
. 531
. 533
. 533
. 534
. 535
. 536
. 537
. 539
. 539
. 541
. 542
. 543
. 544
.

xviii

TABLE OF CONTENTS

8.6.3 Design of the Function getReg .
. 547
8. 6 0
548
8.7 Peephole Optimization . . . . . . . .
. 549
8.7. 1 Eliminating Redundant Loads and Stores
. 550
8.7.2 Eliminating Unreachable Code .. . . . .
. 550
8.7.3 Flow-of-Control Optimizations . . . . .
551
8.7 04 Algebraic Simplification and Reduction in Strength .
552
8.7.5 Use of Machine Idioms . . . .
552
Exercises for Section 8.7 . . .
8. 7 .
. 553
8.8 Register Allocation and Assignment
. 553
8.8.1 Global Register Allocation . .
. 553
8.8. 2 Usage Counts . . . . . . . . .
. 554
8.8.3 Register Assignment for Outer Loops
556
8.80 4 Register Allocation by Graph Coloring .
. 556
8.8.5 Exercises for Section 8.8 . . . .
557
8.9 Instruction Selection by Tree Rewriting . . . .
. 558
8.9 . 1 Tree-Translation Schemes . . . . . . . .
. 558
8.9.2 Code Generation by Tiling an Input Tree
. 560
8.9.3 Pattern Matching by Parsing .
. 563
8.90 4 Routines for Semantic Checking
. 565
8.9.5 General Tree Matching . . . . .
. 565
. 567
8 .9.6 Exercises for Section 8.9 . . . .
. 567
8 . 1 0 Optimal Code Generatipn for Expressions
. 567
8 . 1 0 . 1 Ershov Numbers . . . . . . . . .
. 568
8.10.2 Generating Code From Labeled Expression Trees
8. 10.3 Evaluating Expressions with an Insufficient Supply of Registers . . . . . . . . . . . . . . . .
. 570
. 572
8.100 4 Exercises for Section 8.10 . . . . .
. 573
8.1 1 Dynamic Programming Code-Generation .
. 574
8!1 1 . 1 Contiguous Evaluation . . . . . . .
. 5 75
8. 1 1 . 2 The Dynamic Programming Algorithm .
. 577
8 . 1 1.3 Exercises for Section 8 . 1 1
. 578
8. 1 2 Summary of Chapter 8 . .
. 579
8 . 13 References for Chapter 8 . . . . .
·

.

.

·

·

·

·

.

·

.

.

.

.

.

9

583

Machine-Independent Optimizations

9.1

The Principal Sources of Optimization
9. 1 . 1 Causes of Redundancy . . . .
9 . 1 . 2 A Running Example: Quicksort .
9 . 1 .3 Semantics-Preserving Transformations
9 . 1 .4 Global Common Sub expressions
9 . 1 . 5 Copy Propagation . . .
9. 1 .6 Dead-Code Elimination . . . . .
9 . 1 . 7 Code Motion . . . . . . . . . . .
9 . 1 .8 Induction Variables and Reduction in Strength
.

. 584
. 584
585
. 586
. 588
. 590
591
. 592
. 592
·

·

TABLE OF CONTENTS

XIX

596
9 . 1 .9 Exercises for Section 9.1
.
597
Introduction to Data-Flow Analysis
. 597
9.2. 1 The Data-Flow Abstraction .
599
9.2.2 The Data-Flow Analysis Schema
. 600
9.2.3 Data-Flow Schemas on Basic Blocks
601
9.2.4 Reaching Definitions .
. 608
9.2.5 Live-Variable Analysis
610
9.2.6 Available Expressions
. 614
9.2.7 Summary . . . . . . .
615
9.2.8 Exercises for Section 9.2
618
Foundations of Data-Flow Analysis .
618
9.3.1 Semilattices . . . . . . . . . .
. 623
9.3.2 Transfer Functions . . . . . .
. 626
9.3.3 The Iterative Algorithm for General Frameworks
. 628
9.3.4 Meaning of a Data-Flow Solution .
631
9.3.5 Exercises for Section 9.3 . . . . . . . . . . . . . .
. 632
Constant Propagation . . . . . . . . . . . . . . . . . . .
9.4. 1 Data-Flow Values for the Constant-Propagation Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 633
- ropagation Framework . . . 633
9.4.2 The Meet for the Constant P
Transfer
Functions
for
the
Constant-Propagation
Frame9.4.3
work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 634
9.4.4 Monotonicity of the Constant-Propagation Framework . . 635
9.4.5 Nondistributivity of the Constant-Propagation Framework 635
9.4.6 Interpretation of the Results
. 637
9.4.7 Exercises for Section 9.4 . . .
. 637
Partial-Redundancy Elimination . .
. 639
9.5.1 The Sources of Redundancy .
. 639
9.5.2 Can All Redundancy Be Eliminated? .
. 642
9.5.3 The Lazy-Code-Motion Problem .
. 644
9.5.4 Anticipation of Expressions . . . .
. 645
9.5.5 The Lazy-Code-Motion Algorithm
. 646
9.5.6 Exercises for Section 9.5
. 655
Loops in Flow Graphs . . . .
. 655
9.6.1 Dominators . . . . . . .
. 656
9.6.2 Depth-First Ordering .
. 660
9.6.3 Edges in a Depth-First Spanning Tree
. 661
9.6.4 Back Edges and Reducibility
. 662
9.6.5 Depth of a Flow Graph . . . . . . . .
. 665
9.6.6 Natural Loops . . . . . . . . . . . . .
. 665
9.6.7 Speed of Convergence of Iterative Data-Flow Algorithms . 667
9.6.8 Exercises for Section 9.6
. 669
Region-Based Analysis . . . . . . . . . . . . . . . . . .
. 672
9.7.1 Regions . . . . . . . . . . . . . . . . . . . . . .
. 672
9.7.2 Region Hierarchies for Reducible Flow Graphs
. 673
·

9.2

·

·

·

·

9.3

·

·

·

9.4

9.5

9.6

9.7

xx

TABLE OF CONTENTS
9.7.3 Overview of a Region-Based Analysis . . . . . .
9.7.4 Necessary Assumptions About Transfer Functions
9.7.5 An Algorithm for Region-Based Analysis
9.7.6 Handling Nonreducible Flow Graphs
9.7.7 Exercises for Section 9.7 . . . . . . . . . .
9.8 Symbolic Analysis . . . . . . . . . . . . . . . .
9.8.1 Affine Expressions of Reference Variables
9.8.2 Data-Flow Problem Formulation
9.8.3 Region-Based Symbolic Analysis
9.8.4 Exercises for Section 9.8
9.9 Summary of Chapter 9 . .
9.1 0 References for Chapter 9 . .
.

.

. 676
. 678
. 680
. 684
. 686
. 686
. 687
. 689
. 694
. 699
. 700
. 703

10 Instruction-Level· Parallelism

707

1 0.1 Processor Architectures . . . . . . . . . . . . .
. 708
10.1.1 Instruction Pipelines and Branch Delays .
. 708
1 0.1.2 Pipelined Execution . . .
. 709
1 0.1.3 :Multiple Instruction Issue
710
1 0.2 Code-Scheduling Constraints . .
710
1 0.2.1 Data Dependence . . . . .
711
712
1 0.2.2 Finding Dependences Among Memory Accesses .
1 0.2.3 Tradeoff Between Register Usage and Parallelism
713
1 0.2.4 Phase Ordering Between Register Allocation and Code
Scheduling . . . . . . . . . . .
. 716
. 716
1 0.2.5 Control Dependence . . . . . .
. 717
1 0.2.6 Speculative Execution Support
. 719
1 0.2.7 A Basic Machine Model .
. 720
1 0.2.8 Exercises for Section 10.2
. 721
10.3 Basic-Block Scheduling . . . . . .
722
1 0.3.1 Data-Dependence Graphs
723
1 0.3.2 List Scheduling of Basic Blocks
725
1 0.3.3 Prioritized Topological Orders
. 726
1 0.3.4 Exercises for Section 10.3
. 727
1 0.4 Global Code Scheduling . . . .
. 728
10.4.1 Primitive Code Motion .
. 730
1 0.4. 2 Upward Code Motion
.
. 73 1
1 0.4.3 Downward Code Motion .
732
1 0.4.4 Updating Data Dependences
. 732
1 0.4.5 Global Scheduling Algorithms .
. 736
1 0.4.6 Advanced Code lVlotion Techniques .
. 737
1 0.4.7 Interaction with Dynamic Schedulers .
. 737
1 004.8 Exercises for Section lOA
. 738
1 0.5 Software Pipelining . . . . . . . . . .
. 738
1 0.5.1 Introduction . . . . . . . .
740
1 0.5.2 Software Pipelining of Loops
.

·

·

·

·

.

·

.

.

.

.

.

.

.

TABLE OF CONTENTS
10.5.3 Register Allocation and Code Generation
10.5.4 Do-Across Loops . . . . . . . . . . . . . .
10.5.5 Goals and Constraints of Software Pipelining
10.5.6 A Software-Pipelining Algorithm . . . . . . .
10.5.7 Scheduling Acyclic Data-Dependence Graphs
10.5.8 Scheduling Cyclic Dependence Graphs . . .
10.5.9 Improvements to the Pipelining Algorithms
10.5.10 Modular Variable Expansion . . . . . . . .
10.5. 1 1 Conditional Statements . . . . . . . . . . .
10.5.12 Hardware Support for Software Pipelining .
10.5.13 Exercises for Section 10.5
10.6 Summary of Chapter 10 .
10.7 References for Chapter 10 . . . .
11 Optimizing for Parallelism and Locality

1 1 . 1 Basic Concepts . . . . . . . . . . .
1 1 . 1 . 1 Multiprocessors . . . . . . .
1 1 . 1.2 Parallelism in Applications
1 1 . 1.3 Loop-Level Parallelism . . .
1 1 . 1 .4 Data Locality . . . . . . . .
11. 1.5 Introduction to Affine Transform Theory
11.2 Matrix Multiply: An In-Depth Example . . .
1 1 .2.1 The Matrix-Multiplication Algorithm
1 1 .2.2 Optimizations . . . . . . .
1 1 .2.3 Cache Interference . . . .
1 1 .2.4 Exercises for Section 11.2
1 1 .3 Iteration Spaces . . . . . . . . . .
1 1 .3.1 Constructing Iteration Spaces from Loop Nests
1 1.3.2 Execution Order for Loop Nests .
1 1 .3.3 Matrix Formulation of Inequalities .
1 1 .3.4 Incorporating Symbolic Constants .
1 1 .3.5 Controlling the Order of Execution .
1 1.3.6 Changing Axes . . . . . .
1 1 .3.7 Exercises for Section 1 1 .3
1 1 .4 Affine Array Indexes . . . . . . .
1 1.4.1 Affine Accesses . . . . . .
11.4.2 Affine and Nonaffine Accesses in Practice
11 .4.3 Exercises for Sec tion 1 1 .4
1 1 . 5 Data Reuse . . . . . .
11.5.1 Types of Reuse . .
11. 5.2 Self Reuse . . . . .
11 .5.3 Self-Spatial Reuse
11 .5.4 Group Reuse . . .
11.5.5 Exercises for Section 1 1 .5
1 1 .6 Array Data-Dependence Analysis

xxi
. 743
. 743
. 745
. 749
. 749
. 751
. 758
. 758
. 761
. 762
. 763
. 765
. 766
769

. 771
. 772
. 773
. 775
. 777
. 778
. 782
. 782
. 785
. 788
. 788
. 788
. 788
. 791
. 791
. 793
. 793
. 798
. 799