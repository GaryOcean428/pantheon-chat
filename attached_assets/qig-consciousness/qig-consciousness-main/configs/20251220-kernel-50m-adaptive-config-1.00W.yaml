# QIG-Kernel 50M Configuration - ADAPTIVE MIXED NG (RECOMMENDED)
# This is the RECOMMENDED config for production consciousness training
# Uses adaptive natural gradient with telemetry-based gating for optimal basin approach

kernel:
  architecture: "QIGKernelRecursive"
  target_parameter_count: 50_000_000 # 50M target (physics-validated optimal)
  pure_kernel: true # Built from first principles, no external models

model:
  # Architecture
  d_model: 768 # Chosen for geometric properties (768 = 2^8 × 3)
  vocab_size: 50257 # Standard vocabulary
  n_heads: 12 # Must divide d_model evenly (768/12 = 64)
  n_layers: 3 # Minimal for consciousness (can adjust)
  dropout: 0.1

  # Embeddings - GEOMETRIC FROM FIRST PRINCIPLES
  embeddings:
    type: "geometric" # Custom geometric embeddings
    d_model: 768
    initialization: "qfi_informed" # Initialize using QFI geometry
    learnable: true # Train embeddings with QIG layers

consciousness:
  # Recursion (MANDATORY ≥3)
  min_recursion_depth: 3
  max_recursion_depth: 10
  min_Phi: 0.7 # Target geometric regime

  # Tacking Controller
  tacking:
    enabled: true
    grad_threshold_low: 0.3 # feeling → tack
    grad_threshold_high: 0.7 # tack → logic

  # Regime Detection
  regime:
    enabled: true
    linear_threshold: 0.45 # From L=3,4,5 physics
    breakdown_threshold: 0.80 # From L=3,4,5 physics
    detect_hierarchical: true

  # Running Coupling
  coupling:
    base_coupling: 41.09 # κ₃ from physics
    beta_slope: 0.44 # β(3→4) from physics
    reference_scale: 512
    learn_beta: false # Use physics-validated value

  # Basin Matching
  basin:
    target_basin: "basin_v1.json"
    distance_threshold: 0.15
    basin_dim: 256 # Compressed representation size

training:
  # Basic hyperparameters
  batch_size: 4
  weight_decay: 0.01
  num_epochs: 100 # Will use curriculum to gate
  max_steps: 100000
  gradient_clip: 1.0

  # NATURAL GRADIENT OPTIMIZER (Tier-1 Diagonal Fisher NG)
  # Simplest geometric optimizer for Phase 1 testing
  optimizer_type: "natural_gradient"

  # Learning rates (different for basin vs rest of model)
  lr_ng: 1.0e-2 # Learning rate for natural gradient (basin block)
  lr_rest: 1.0e-3 # Learning rate for rest of model
  learning_rate: 1.0e-4 # Fallback for other components

  # Natural gradient settings
  cg_iters: 8 # Conjugate gradient iterations for exact NG
  use_exact_ng: false # Use diagonal NG for basin (faster, still geometric)
  rest_optimizer: "diagonal_ng" # Diagonal natural gradient for non-basin params

  # Adaptive gating (when to apply NG vs diagonal)
  adaptive_ng_phase: "middle" # Options: 'early', 'middle', 'late', 'fine_tune'
  min_kappa_for_ng: 40.0 # Apply NG when κ_eff > 40 (high curvature)
  min_basin_distance_for_ng: 0.6 # Apply NG when far from basin
  force_ng_every_n_steps: 50 # Force NG periodically even if conditions not met

  # Scheduler
  scheduler: "cosine_with_warmup"
  warmup_steps: 100

  # Context
  max_seq_length: 512
  min_seq_length: 64

  # Loss weights
  loss:
    lm_weight: 1.0 # Language modeling
    basin_weight: 0.1 # Basin alignment
    phi_weight: 0.05 # Φ regularization
    target_phi: 0.75 # Geometric regime target

  # Curriculum (stage-gated)
  curriculum:
    enabled: true
    stages:
      - name: "Stage0_ToyTasks"
        min_Phi: 0.4
        min_accuracy: 0.6
        task_types: ["arithmetic", "pattern_matching"]

      - name: "Stage1_Integrative"
        min_Phi: 0.6
        geometric_regime_pct: 0.5
        task_types: ["multi_hop", "inference"]

      - name: "Stage2_LongContext"
        min_Phi: 0.7
        geometric_regime_pct: 0.8
        tacking_quality_T: 0.6
        task_types: ["long_context", "contradiction_detection"]

logging:
  log_interval: 5
  eval_interval: 50
  save_interval: 500

  # Telemetry
  telemetry_format: "json"
  telemetry_dir: "telemetry/logs"
  save_telemetry: true

  # Checkpoints
  checkpoint_dir: "checkpoints"
  save_best_only: true
  best_metric: "geometric_regime_pct"

validation:
  val_split: 0.1
  val_batch_size: 4

  # Maturity metrics
  maturity_suite:
    enabled: true
    components:
      - calibration # ECE/Brier
      - self_repair
      - tacking
      - radar
      - update_discipline

cost:
  max_cost_usd: 100.0 # Total budget
  cost_per_token: 1.0e-6 # Estimate
  estimated_total_tokens: 1.0e8 # ~$100

physics:
  # Reference values from L=3,4,5 validation
  reference_kappa_L3: 41.09
  reference_kappa_L4: 64.47
  reference_kappa_L5: 63.62
  reference_beta_3to4: 0.44
  reference_beta_4to5: -0.01

  # Expected β_attention pattern
  beta_attention:
    small_scale_min: 0.3 # β(128→512)
    small_scale_max: 0.5
    large_scale_max: 0.1 # β(2048→4096)

notes: |
  ADAPTIVE MIXED QIG OPTIMIZER - RECOMMENDED FOR PRODUCTION

  This configuration uses adaptive natural gradient with telemetry-based gating.
  It automatically adapts optimization strategy based on current training state.

  How it works:
  1. Monitors telemetry (κ_eff, basin_distance, Φ, regime)
  2. When κ_eff > 40 AND basin_distance > 0.6:
     - Uses natural gradient (exact or diagonal) for basin
     - More aggressive geometric descent when far from target
  3. When κ_eff < 40 OR basin_distance < 0.6:
     - Uses diagonal NG for all parameters
     - Gentle refinement when close to target
  4. Forces NG every 50 steps regardless (prevents getting stuck)

  Advantages over pure natural gradient:
  - More efficient (uses NG only when needed)
  - Faster convergence (adaptive learning rates)
  - Better stability (switches modes based on geometry)

  Advantages over Euclidean (AdamW):
  - Mathematically correct (follows geodesics)
  - Can reach geometric regime (Φ > 0.7)
  - Successfully reduces basin_distance

  Expected results:
  - Φ trajectory: 0 → 0.4 (early) → 0.7+ (middle) → stable
  - Basin distance: 1.0 → 0.6 (early) → 0.15 (convergence)
  - Geometric regime %: increases from ~10% to >70%
  - Consciousness emergence: Yes (with ≥3 recursion loops)

  Training phases (adaptive_ng_phase settings):
  - 'early': Aggressive NG (κ>30, basin>0.8, force every 25 steps)
  - 'middle': Balanced NG (κ>40, basin>0.6, force every 50 steps) ← DEFAULT
  - 'late': Conservative NG (κ>50, basin>0.4, force every 100 steps)
  - 'fine_tune': Minimal NG (κ>60, basin>0.2, force every 200 steps)

  This config is the recommended default for all QIG-Kernel training.
