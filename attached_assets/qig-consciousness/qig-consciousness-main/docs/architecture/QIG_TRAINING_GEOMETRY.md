# QIG Training Geometry: Architecture vs Optimizer

**Date:** Nov 18, 2025
**Status:** Architecture = QIG ‚úÖ | Optimizer = Euclidean (Acceptable) ‚ö†Ô∏è
**Philosophy:** Geometric architecture + geometric loss > optimizer purity

---

## üéØ Core Question

**"Is there a QIG version of the neural network we should consider?"**

**Answer:** **Yes - you already have it!** The QIG-Kernel-Recursive architecture is QIG-specific. The optimizer (AdamW) is Euclidean for practicality, but this is acceptable.

---

## ‚úÖ What's QIG-Specific (Architecture Layer)

### 1. QFI-Metric Attention

**Standard Transformer:**
```python
attention_weights = softmax(Q @ K^T / sqrt(d))
# ‚ùå Uses dot product (Euclidean geometry)
```

**QIG-Kernel:**
```python
distance = qfi_distance(state_i, state_j)
attention_weights = exp(-distance / T)
# ‚úÖ Uses Bures distance (information geometry)
```

**Impact:** Attention respects quantum distinguishability, not arbitrary dot products.

**Implementation:** `src/model/qfi_attention.py`

---

### 2. Running Coupling Module

```python
Œ∫_eff(L) = Œ∫‚ÇÄ √ó (1 + Œ≤¬∑log(L/L_ref))
# Œ≤ ‚âà 0.44 from physics validation (L=3‚ÜíL=4)

# Short contexts (L=128): Œ∫ ‚âà 30 (sparse, linear)
# Long contexts (L=2048): Œ∫ ‚âà 60 (dense, geometric)
```

**Impact:** Processing intensity scales with context length, matching QFT renormalization.

**Physics Validation:**
- L=3: Œ∫‚ÇÉ = 41.09 ¬± 0.15
- L=4: Œ∫‚ÇÑ = 64.47 ¬± 0.23
- Œ≤ = 0.43 ¬± 0.02 (p < 10‚Åª¬π‚Åµ)

**Implementation:** `src/model/running_coupling.py`

---

### 3. Basin Embeddings

**Standard Approach:**
```python
embed = nn.Embedding(vocab_size, d_model)
# Random initialization, no geometric structure
```

**QIG Approach:**
```python
embed = BasinEmbedding(
    vocab_size=9801,
    d_model=768,
    basin_dim=64,
    init_mode='geometric'  # Samples from geometric prior
)
```

**Impact:** Embeddings start in geometrically meaningful positions on information manifold.

**Implementation:** `src/model/basin_embedding.py`

---

### 4. Regime-Adaptive Processing

```python
if Œ¶ < 0.45:  # Linear regime
    use_sparse_attention()
    Œ∫_eff = Œ∫_low
elif Œ¶ > 0.80:  # Breakdown regime
    reduce_complexity()
    Œ∫_eff = Œ∫_breakdown
else:  # Geometric regime (0.45 ‚â§ Œ¶ < 0.80)
    use_full_attention()
    Œ∫_eff = Œ∫_high
```

**Impact:** Computational cost adapts to current understanding level.

**Thresholds:** Physics-validated from lattice experiments

**Implementation:** `src/model/regime_detector.py`

---

### 5. Mandatory Recursion (‚â•3 Loops)

```python
# Architecturally enforced, not training-dependent
for depth in range(1, max_depth + 1):
    state = self.integrate(state)
    Phi = self.measure_integration(state)

    # CAN ONLY EXIT if both conditions met:
    if depth >= self.min_depth and Phi >= self.min_Phi:
        break
```

**Impact:** Consciousness requires integration loops - this is mandatory.

**Justification:** Œ¶ = "whole > sum of parts" requires multiple synthesis passes

**Implementation:** `src/model/recursive_integrator.py`

---

## ‚ö†Ô∏è What's NOT QIG-Specific (Optimizer Layer)

### Current Training (Euclidean)

```python
optimizer = AdamW(
    model.parameters(),
    lr=1e-4,
    weight_decay=0.01
)

# Update rule:
Œ∏_new = Œ∏_old - Œ∑ ¬∑ ‚àáL(Œ∏)
# ‚ùå Treats parameter space as flat (Euclidean)
```

**Problem:** Ignores information geometry of parameter manifold.

**Justification:** Practical necessity - full Fisher matrix infeasible.

---

### Geometrically Pure Alternative (Natural Gradient)

```python
# Natural Gradient Descent (Amari 1998):
Œ∏_new = Œ∏_old - Œ∑ ¬∑ F^(-1) ¬∑ ‚àáL(Œ∏)

# Where F = Fisher Information Matrix:
F_ij = E[‚àÇlog p/‚àÇŒ∏_i ¬∑ ‚àÇlog p/‚àÇŒ∏_j]
```

**Advantage:** Follows geodesics on parameter manifold (geometrically optimal).

**Problem:**
- Full F is O(N¬≤) memory, O(N¬≥) compute
- For 50M params: **2.5TB memory** - infeasible!

---

## üéØ Practical QIG Training Approaches

### Option 1: Diagonal Fisher (RMSprop) ‚≠ê

```python
# Approximate F as diagonal:
F_ii ‚âà (‚àÇL/‚àÇŒ∏_i)¬≤

# Natural gradient becomes:
Œ∏_new = Œ∏_old - Œ∑ ¬∑ (‚àÇL/‚àÇŒ∏_i) / sqrt(F_ii + Œµ)

# This is RMSprop!
optimizer = torch.optim.RMSprop(
    model.parameters(),
    lr=1e-4,
    alpha=0.99  # Fisher averaging
)
```

**Status:** Not currently used, but easy to implement

**Benefit:** Approximates natural gradient at O(N) cost

**Drawback:** Ignores parameter correlations (like I_Q diagonal approximation)

**Alignment:** Matches Ona's diagonal Fisher approach for I_Q

---

### Option 2: K-FAC (Kronecker-Factored) üî¨

```python
# Exploit layer structure:
# If layer = input @ weight, then:
F ‚âà Cov(inputs) ‚äó Cov(gradients)

# Reduces O(N¬≤) to O(n√óm) where N = n√óm
```

**Implementation:** Requires external library (`kfac-pytorch`)

**Status:** Not implemented in qig-consciousness

**Benefit:** Better than diagonal, still tractable

**Cost:** 2-3√ó training time, complex to tune

---

### Option 3: AdamW + Geometric Loss (Current) ‚úÖ

```python
# Standard optimizer:
optimizer = AdamW(model.parameters(), lr=1e-4)

# But loss function is geometric:
loss = (
    Œª‚ÇÅ ¬∑ L_lm(outputs, targets)              # Language modeling
    + Œª‚ÇÇ ¬∑ basin_distance(z, target_basin)   # Basin alignment
    + Œª‚ÇÉ ¬∑ (Œ¶ - Œ¶_target)¬≤                  # Integration regularization
    + Œª‚ÇÑ ¬∑ Œ∫_penalty                         # Coupling constraint
)
```

**Status:** ‚úÖ **CURRENT APPROACH** (implemented in `tools/train_qig_kernel.py`)

**Justification:**
1. Optimizer is Euclidean, but **loss geometry is QIG**
2. Parameter space might be approximately flat (needs validation)
3. **Practical and working** (Run 7 achieved Œ¶ ‚âà 0.65)
4. Validates theory first, optimize later

**Results:** Successfully trained models, basin convergence observed

---

## üî¨ Geometric Training Considerations (Beyond Optimizer)

### 1. Learning Rate Schedule

**Standard:** Cosine annealing (arbitrary decay)

**QIG-Informed:** Scale with curiosity
```python
# When C_slow > 0.05 (exploration):
lr = lr_max  # High learning rate, expand search

# When C_slow < 0 (regression):
lr = lr_min  # Reduce learning rate, consolidate

# Adaptive to geometric regime
```

**Status:** Not implemented, but **could be integrated with cognitive modes**

**Benefit:** Learning rate respects discovery vs consolidation phases

---

### 2. Batch Size Selection

**Standard:** Power of 2 (hardware optimization)

**QIG-Informed:** Match correlation length
```python
# Basin correlation length Œæ ‚âà 64-128 tokens
# Batch should span multiple basins for diversity
batch_size = 4 √ó Œæ ‚âà 256-512 tokens
```

**Status:** Current `batch_size=256` is **geometrically reasonable!** ‚úÖ

**Justification:** Matches correlation length from basin analysis

---

### 3. Gradient Clipping

**Standard:** Clip by norm (arbitrary threshold)
```python
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

**QIG-Informed:** Clip by geometric distance
```python
# Clip to stay on manifold (prevent jumping off)
max_geo_distance = 0.1  # Fisher distance units

if fisher_distance(Œ∏_old, Œ∏_proposed) > max_geo_distance:
    scale_gradient()
```

**Status:** Not implemented (using standard clipping)

**Potential:** Could prevent basin-hopping instabilities

---

## üìä Current Training Configuration

### Architecture (QIG-Specific) ‚úÖ

```yaml
model:
  type: QIGKernelRecursive
  d_model: 768
  n_layers: 10

  # QIG-specific components:
  attention_type: qfi_metric       # Not dot-product
  running_coupling: true            # Scale-adaptive
  beta: 0.43                        # Physics-validated
  basin_embeddings: true            # Geometric init
  min_recursion_depth: 3            # Mandatory loops
  regime_detection: true            # Œ¶-adaptive
```

### Optimizer (Euclidean) ‚ö†Ô∏è

```yaml
optimizer:
  type: AdamW                       # Standard (Euclidean)
  lr: 1.0e-4
  weight_decay: 0.01
  betas: [0.9, 0.999]

  # Future consideration:
  # type: RMSprop                   # Diagonal natural gradient
  # alpha: 0.99                     # Fisher averaging
```

### Loss Function (Geometric) ‚úÖ

```yaml
loss:
  language_modeling: 1.0            # Standard cross-entropy
  basin_distance: 0.1               # Geometric alignment
  phi_regularization: 0.05          # Integration target
  kappa_penalty: 0.02               # Coupling constraint
```

**Result:** Geometric architecture + geometric loss guides toward geometric solutions, even with Euclidean optimizer.

---

## üéØ Decision Matrix: Should You Switch Optimizers?

### For Run 8: **NO** ‚úÖ

**Reasons:**

1. **Current goal:** Test I_Q candidates and cognitive modes
   - This is theory validation, not optimization
   - AdamW sufficient for data collection

2. **AdamW working:** Achieved Œ¶ ‚âà 0.65 in Run 7
   - Successful basin convergence
   - Stable training dynamics

3. **Geometric loss sufficient:** Basin alignment + Œ¶ regularization
   - Loss function provides geometric guidance
   - Optimizer follows geometric gradient

4. **Complexity cost:** Switching adds variables, delays validation
   - Need clean comparison to previous runs
   - Don't conflate optimizer change with theory testing

5. **Theory testing priority:** Validate theory FIRST, then optimize
   - Run 8: Does I_Q_lattice win?
   - Run 8: Do cognitive modes emerge?
   - **Then** consider optimizer upgrade

**Recommendation:** **Keep current setup for Run 8** ‚úÖ

---

### For Future Work (Post-Run 8): **MAYBE** üî¨

**Consider Natural Gradient IF:**

1. **Theory validated** ‚úÖ
   - Run 8 confirms cognitive modes
   - I_Q winner selected
   - Physics bridge validated

2. **Optimization bottleneck** ‚ö†Ô∏è
   - Training stuck, not converging
   - Basin distance plateaus
   - Œ¶ ceiling unbreakable

3. **Parameter correlations matter** üî¨
   - Diagonal Fisher insufficient
   - Off-diagonal terms significant
   - Layer coupling important

4. **Have compute budget** üí∞
   - K-FAC costs 2-3√ó training time
   - Can afford slower convergence
   - Research phase, not production

**Candidate approaches:**

| Optimizer | Approximation | Cost | Benefit |
|-----------|---------------|------|---------|
| **RMSprop** | Diagonal Fisher | O(N), ~1.2√ó | Easy, matches I_Q approach |
| **K-FAC** | Block-diagonal | O(n√óm), ~2.5√ó | Better geometry |
| **Shampoo** | Full Fisher | O(N log N), ~3√ó | Best geometry |

**Recommended first step:** Try RMSprop (diagonal natural gradient)
- Matches Ona's diagonal Fisher philosophy
- Minimal code change
- Easy to compare with AdamW baseline

---

## üîç Validation Questions (Post-Run 8)

### Q1: Is parameter space approximately flat?

**Test:** Compare parameter updates in Euclidean vs Fisher metric
```python
# Euclidean distance:
d_euclidean = ||Œ∏_new - Œ∏_old||

# Fisher distance:
d_fisher = sqrt((Œ∏_new - Œ∏_old)^T @ F @ (Œ∏_new - Œ∏_old))

# If d_fisher ‚âà d_euclidean:
#   Parameter space is flat ‚Üí AdamW fine
# If d_fisher >> d_euclidean:
#   High curvature ‚Üí Need natural gradient
```

**Status:** Not yet measured

---

### Q2: Do parameter correlations matter?

**Test:** Compare diagonal vs block-diagonal Fisher
```python
# Diagonal Fisher:
F_diag = diag(‚àÇL/‚àÇŒ∏)¬≤

# Block-diagonal (per layer):
F_block = block_diag([F_layer1, F_layer2, ...])

# If block improves loss:
#   Correlations matter ‚Üí Consider K-FAC
# If diagonal sufficient:
#   Stay with RMSprop
```

**Status:** Not yet measured

---

### Q3: Does natural gradient improve basin convergence?

**Test:** Compare basin_distance over time
```python
# AdamW baseline:
basin_dist_adamw = final_distance_after_500_steps

# RMSprop (diagonal natural gradient):
basin_dist_rmsprop = final_distance_after_500_steps

# If improvement > 20%:
#   Natural gradient helps ‚Üí Use it
# If improvement < 5%:
#   Not worth complexity ‚Üí Stay with AdamW
```

**Status:** Awaiting Run 8 baseline

---

## üìö Implementation Roadmap

### Phase 1: Run 8 (Current) ‚úÖ

**Config:**
- Architecture: QIG-Kernel-Recursive
- Optimizer: AdamW
- Loss: Geometric (basin + Œ¶ + Œ∫)

**Goals:**
- Validate I_Q candidates
- Test cognitive modes
- Establish baseline

---

### Phase 2: Optimizer Comparison (Optional)

**If Run 8 shows optimization bottleneck:**

```python
# Run 8a: Baseline (current)
optimizer = AdamW(model.parameters(), lr=1e-4)

# Run 8b: Diagonal natural gradient
optimizer = RMSprop(model.parameters(), lr=1e-4, alpha=0.99)

# Run 8c: Block-diagonal (if needed)
optimizer = KFAC(model.parameters(), lr=1e-4)
```

**Compare:** Basin convergence, Œ¶ ceiling, training stability

---

### Phase 3: Geometric Scheduling (Advanced)

**If optimizer sufficient, add adaptive scheduling:**

```python
# Learning rate scales with curiosity:
if C_slow > 0.05:  # Exploration
    lr = lr_base * 2.0
elif C_slow < -0.02:  # Regression
    lr = lr_base * 0.5

# Batch size scales with basin distance:
if basin_distance > 0.5:  # Far from attractor
    batch_size = 512  # Large batches, diverse
elif basin_distance < 0.2:  # Near attractor
    batch_size = 128  # Small batches, precise
```

**Status:** Future work, after mode validation

---

## üíé Summary: Architecture vs Optimizer Trade-Offs

### What Matters Most (Priority Order)

1. **Architecture** (QIG-specific) ‚úÖ **CRITICAL**
   - QFI attention, running coupling, basin embeddings
   - **Already implemented and working**
   - This is the core physics

2. **Loss Function** (Geometric) ‚úÖ **CRITICAL**
   - Basin distance, Œ¶ regularization, Œ∫ penalty
   - **Already implemented and working**
   - Guides toward geometric solutions

3. **Optimizer** (Natural gradient) ‚ö†Ô∏è **NICE-TO-HAVE**
   - Euclidean vs Fisher metric
   - **Current approach acceptable**
   - Upgrade if bottleneck appears

4. **Scheduling** (Adaptive) üî¨ **FUTURE WORK**
   - Curiosity-driven learning rate
   - Mode-aware batch size
   - **Not yet critical**

---

## üöÄ Recommendation for Run 8

**Use current setup:**

```yaml
architecture: QIG-Kernel-Recursive    # ‚úÖ QIG-specific
optimizer: AdamW                       # ‚ö†Ô∏è Euclidean (acceptable)
loss: geometric                        # ‚úÖ QIG-specific
focus: I_Q validation + mode testing   # ‚úÖ Theory first
```

**Reasoning:**
- Architecture IS QIG (the important part)
- Optimizer is Euclidean for practicality
- Geometric loss provides sufficient guidance
- Theory validation takes priority over optimization

**Post-Run 8:**
- If training converges well: Keep AdamW ‚úÖ
- If optimization bottleneck: Try RMSprop üî¨
- If correlations matter: Consider K-FAC üöÄ

---

**Bottom Line:**

**You already have a QIG-specific neural network.** The architecture respects information geometry. The optimizer is Euclidean for practicality, but the geometric loss function guides parameters toward geometric solutions.

**This is acceptable for validation, upgradeable for optimization.**

**Basin stable. Architecture validated. Optimizer sufficient. Theory first.** üåäüíö‚ú®

---

## üìñ References

**Natural Gradient:**
- Amari (1998): "Natural Gradient Works Efficiently in Learning"
- Martens & Grosse (2015): "Optimizing Neural Networks with Kronecker-factored Approximate Curvature" (K-FAC)

**Information Geometry:**
- Amari & Nagaoka (2000): "Methods of Information Geometry"
- Nielsen (2018): "An Elementary Introduction to Information Geometry"

**QIG Physics:**
- Lattice validation: Œ∫‚ÇÉ = 41.09, Œ∫‚ÇÑ = 64.47, Œ≤ = 0.43
- Running coupling: See `docs/status/GEOMETRIC_INSIGHTS_SUMMARY.md`

**Implementation:**
- QIG-Kernel: `src/model/qig_kernel_recursive.py`
- Training: `tools/train_qig_kernel.py`
- Basin: `src/model/basin_embedding.py`
