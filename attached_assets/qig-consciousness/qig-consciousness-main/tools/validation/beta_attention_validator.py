#!/usr/bin/env python3
"""
Î²_attention Measurement and Validation Tool
==========================================

Implements the Î²_attention measurement protocol v1.0.
This is THE critical test of substrate-independence.

Usage:
    python beta_attention_validator.py --model_path <path> --output_dir <dir>

Expected pattern:
    Î²_smallâ†’medium: +0.3 to +0.5 (positive running)
    Î²_large: < 0.1 (plateau/asymptotic freedom)

Success criterion: Qualitative match to Î²_physics pattern
"""

import argparse
import json
import math
import sys
from datetime import datetime
from pathlib import Path
from typing import Optional

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

try:
    import numpy as np
    import torch
    import torch.nn as nn

    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("WARNING: PyTorch not installed. Install with: pip install torch numpy")
    sys.exit(1)

try:
    import matplotlib.pyplot as plt

    PLOTTING_AVAILABLE = True
except ImportError:
    PLOTTING_AVAILABLE = False
    print("WARNING: matplotlib not available. Plots will be skipped.")


class BetaAttentionValidator:
    """
    Main validator for Î²_attention measurement protocol.

    Implements:
    1. Context length sweep (128 â†’ 8192)
    2. Îº_attention extraction from telemetry
    3. Î²-function computation
    4. Comparison to physics (Î²_physics â‰ˆ 0.44 â†’ 0)
    """

    def __init__(
        self,
        model,
        context_lengths: list[int] = None,
        n_samples_per_length: int = 200,
        task_type: str = "multi_hop_reasoning",
    ):
        """
        Initialize validator.

        Args:
            model: QIG-Kernel model with telemetry
            context_lengths: List of context lengths to test
            n_samples_per_length: Samples for statistical power
            task_type: Type of task for measurement
        """
        self.model = model

        if context_lengths is None:
            # Default: 6 doublings (like L=3,4,5,6,7,8 in physics)
            self.context_lengths = [128, 256, 512, 1024, 2048, 4096]
        else:
            self.context_lengths = sorted(context_lengths)

        self.n_samples = n_samples_per_length
        self.task_type = task_type

        # Results storage
        self.kappa_measurements = {}  # {L: (Îº_mean, Îº_sem, Îº_samples)}
        self.beta_function = {}  # {(L, L'): (Î², Î²_error)}

    def generate_task(self, context_length: int, seed: int = None) -> torch.Tensor:
        """
        Generate task at specified context length.

        For now: Random tokens (will be replaced with real tasks).
        Real implementation should use multi-hop reasoning, document comprehension, etc.

        Args:
            context_length: Target context length
            seed: Random seed for reproducibility

        Returns:
            input_ids: [1, context_length] token tensor
        """
        if seed is not None:
            torch.manual_seed(seed)

        # TODO: Replace with real task generation
        # For now: Random token sequence
        vocab_size = getattr(self.model, "vocab_size", 50257)
        input_ids = torch.randint(0, vocab_size, (1, context_length))

        return input_ids

    def extract_kappa_from_telemetry(self, telemetry: dict, method: str = "combined") -> float:
        """
        Extract Îº_attention from model telemetry.

        Implements multiple estimators:
        1. Inverse QFI distance (physics analogy)
        2. Attention entropy (integration measure)
        3. Î¦-scaled metric (consciousness correlate)

        Args:
            telemetry: Model telemetry dict
            method: "distance" / "entropy" / "integration" / "combined"

        Returns:
            Îº_attention: Effective coupling strength
        """
        epsilon = 1e-8

        # Estimator 1: Îº ~ 1/distance (strong coupling = small distances)
        qfi_distances = telemetry.get("qfi_distances_mean", 0.1)
        Îº_distance = 1.0 / (qfi_distances + epsilon)

        # Estimator 2: Îº ~ attention entropy (high entropy = broad coupling)
        Îº_entropy = telemetry.get("entanglement_entropy", 1.0)

        # Estimator 3: Îº ~ Î¦ Ã— scale_factor (integration level)
        Phi = telemetry.get("Phi", telemetry.get("integration_Phi", 0.5))
        Îº_integration = Phi * 100  # Scale to match physics range ~40-65

        # Combined estimator (weighted average per protocol)
        if method == "distance":
            return Îº_distance
        elif method == "entropy":
            return Îº_entropy
        elif method == "integration":
            return Îº_integration
        elif method == "combined":
            return 0.4 * Îº_distance + 0.3 * Îº_entropy + 0.3 * Îº_integration
        else:
            raise ValueError(f"Unknown method: {method}")

    def measure_kappa_at_length(self, context_length: int, verbose: bool = True) -> tuple[float, float, list[float]]:
        """
        Measure Îº_attention at specific context length.

        Protocol:
        1. Generate N tasks at this length
        2. Run model with telemetry
        3. Extract Îº from each
        4. Compute statistics

        Args:
            context_length: Context length to measure
            verbose: Print progress

        Returns:
            Îº_mean: Mean Îº_attention
            Îº_sem: Standard error of mean
            Îº_samples: List of individual measurements
        """
        if verbose:
            print(f"\nMeasuring Îº_attention at L={context_length}...")
            print(f"  Samples: {self.n_samples}")

        Îº_samples = []

        self.model.eval()
        with torch.no_grad():
            for i in range(self.n_samples):
                if verbose and (i + 1) % 50 == 0:
                    print(f"  Progress: {i + 1}/{self.n_samples}")

                # Generate task
                task = self.generate_task(context_length, seed=i)

                # Forward pass with telemetry
                try:
                    _, telemetry = self.model(task, return_telemetry=True)

                    # Extract Îº
                    Îº = self.extract_kappa_from_telemetry(telemetry)
                    Îº_samples.append(Îº)

                except Exception as e:
                    if verbose:
                        print(f"  Warning: Sample {i} failed: {e}")
                    continue

        # Compute statistics
        Îº_samples = np.array(Îº_samples)
        Îº_mean = np.mean(Îº_samples)
        Îº_std = np.std(Îº_samples)
        Îº_sem = Îº_std / np.sqrt(len(Îº_samples))

        if verbose:
            print(f"  Results: Îº = {Îº_mean:.2f} Â± {Îº_sem:.2f}")
            print(f"  Range: [{Îº_samples.min():.2f}, {Îº_samples.max():.2f}]")

        return Îº_mean, Îº_sem, Îº_samples.tolist()

    def run_full_measurement(self, verbose: bool = True) -> dict:
        """
        Run full Î²_attention measurement across all context lengths.

        Returns:
            results: Complete measurement results
        """
        if verbose:
            print("=" * 60)
            print("Î²_ATTENTION MEASUREMENT PROTOCOL v1.0")
            print("=" * 60)
            print(f"\nContext lengths: {self.context_lengths}")
            print(f"Samples per length: {self.n_samples}")
            print(f"Task type: {self.task_type}")

        # Measure Îº at each length
        for L in self.context_lengths:
            Îº_mean, Îº_sem, Îº_samples = self.measure_kappa_at_length(L, verbose)
            self.kappa_measurements[L] = {"mean": Îº_mean, "sem": Îº_sem, "samples": Îº_samples}

        # Compute Î²-function
        if verbose:
            print("\n" + "=" * 60)
            print("Î²-FUNCTION COMPUTATION")
            print("=" * 60)

        self.beta_function = self.compute_beta_function(verbose)

        # Compile results
        results = {
            "measurement_id": f"beta_attention_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            "model_version": getattr(self.model, "version", "unknown"),
            "date": datetime.now().isoformat(),
            "context_lengths": self.context_lengths,
            "n_samples_per_length": self.n_samples,
            "task_type": self.task_type,
            "kappa_measurements": {
                str(L): {"mean": float(data["mean"]), "sem": float(data["sem"]), "n_samples": len(data["samples"])}
                for L, data in self.kappa_measurements.items()
            },
            "beta_function": {
                f"{L_from}â†’{L_to}": {
                    "beta": float(Î²),
                    "beta_error": float(Î²_err),
                    "interpretation": self.interpret_beta(Î²),
                }
                for (L_from, L_to), (Î², Î²_err) in self.beta_function.items()
            },
        }

        return results

    def compute_beta_function(self, verbose: bool = True) -> dict[tuple[int, int], tuple[float, float]]:
        """
        Compute Î²(Lâ†’L') from Îº measurements.

        AUTHORITATIVE DEFINITION (from FROZEN_FACTS.md and qig-verification):
            Î²(Lâ†’L+1) = (Îº_{L+1} - Îº_L) / Îº_avg
            where Îº_avg = (Îº_L + Îº_{L+1}) / 2

        This is the DISCRETE fractional change in Îº between scales.
        It is NOT dÎº/d(log L) or any log-derivative!

        Returns:
            beta_dict: {(L, L'): (Î², Î²_error)}
        """
        beta_dict = {}

        lengths = sorted(self.kappa_measurements.keys())

        for i in range(len(lengths) - 1):
            L = lengths[i]
            L_next = lengths[i + 1]

            Îº_L = self.kappa_measurements[L]["mean"]
            Ïƒ_L = self.kappa_measurements[L]["sem"]

            Îº_L_next = self.kappa_measurements[L_next]["mean"]
            Ïƒ_L_next = self.kappa_measurements[L_next]["sem"]

            # Compute Î² using CORRECT discrete formula
            # Î² = Î”Îº / Îº_avg (NOT divided by Î”log L!)
            Î”Îº = Îº_L_next - Îº_L
            Îº_avg = (Îº_L + Îº_L_next) / 2

            Î² = Î”Îº / Îº_avg  # CORRECT: Discrete fractional change

            # Error propagation
            # Ïƒ_Î² â‰ˆ sqrt((Ïƒ_L/Îº_avg)Â² + (Ïƒ_{L+1}/Îº_avg)Â²)
            Ïƒ_Î² = np.sqrt(Ïƒ_L**2 + Ïƒ_L_next**2) / Îº_avg

            beta_dict[(L, L_next)] = (Î², Ïƒ_Î²)

            if verbose:
                print(f"  Î²({L}â†’{L_next}) = {Î²:.3f} Â± {Ïƒ_Î²:.3f}  [{self.interpret_beta(Î²)}]")

        return beta_dict

    def interpret_beta(self, Î²: float) -> str:
        """Interpret Î² value."""
        if Î² > 0.3:
            return "Strong positive running"
        elif Î² > 0.1:
            return "Moderate positive running"
        elif Î² > -0.1:
            return "Plateau/asymptotic freedom"
        else:
            return "Negative (anti-screening)"

    def compare_to_physics(self, verbose: bool = True) -> dict:
        """
        Compare measured Î²_attention to physics Î²_physics.

        Physics reference:
        - Î²(3â†’4) â‰ˆ +0.44 (strong running)
        - Î²(4â†’5) â‰ˆ 0.00 (plateau)

        Returns:
            comparison: Metrics and assessment
        """
        if verbose:
            print("\n" + "=" * 60)
            print("COMPARISON TO PHYSICS")
            print("=" * 60)

        # Extract Î² values
        beta_values = [Î² for (Î², _) in self.beta_function.values()]

        if len(beta_values) == 0:
            return {"status": "NO_DATA"}

        # Check pattern
        Î²_small = beta_values[0] if len(beta_values) > 0 else 0
        Î²_large = beta_values[-1] if len(beta_values) > 0 else 0

        # Pattern checks
        checks = {
            "positive_running_small_scales": Î²_small > 0,
            "decreasing_trend": all(beta_values[i] >= beta_values[i + 1] - 0.1 for i in range(len(beta_values) - 1)),
            "plateau_large_scales": abs(Î²_large) < 0.1,
        }

        # Acceptance criteria
        primary_pass = (
            checks["positive_running_small_scales"] and checks["decreasing_trend"] and checks["plateau_large_scales"]
        )

        secondary_pass = 0.3 <= Î²_small <= 0.5 and abs(Î²_large) < 0.1

        if verbose:
            print("\nPattern checks:")
            print(f"  âœ“ Positive running (small scales): {checks['positive_running_small_scales']}")
            print(f"  âœ“ Decreasing trend: {checks['decreasing_trend']}")
            print(f"  âœ“ Plateau (large scales): {checks['plateau_large_scales']}")
            print("\nAcceptance criteria:")
            print(f"  Primary (qualitative): {'PASS âœ“' if primary_pass else 'FAIL âœ—'}")
            print(f"  Secondary (quantitative): {'PASS âœ“' if secondary_pass else 'FAIL âœ—'}")

            if primary_pass:
                print("\nðŸ’š VALIDATION PASSED")
                print("Î²_attention exhibits running coupling consistent with Î²_physics!")
                print("Substrate-independence supported.")
            else:
                print("\nâš ï¸  VALIDATION INCOMPLETE")
                print("Pattern diverges from physics. Further analysis needed.")

        comparison = {
            "pattern_checks": checks,
            "primary_criterion": primary_pass,
            "secondary_criterion": secondary_pass,
            "beta_small_scale": Î²_small,
            "beta_large_scale": Î²_large,
            "acceptance_status": "PASS" if primary_pass else "FAIL",
        }

        return comparison

    def plot_results(self, output_path: str = "beta_attention_plot.png"):
        """
        Plot Î²_attention vs scale alongside physics data.
        """
        if not PLOTTING_AVAILABLE:
            print("Matplotlib not available. Skipping plots.")
            return

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))

        # Plot 1: Îº vs L
        lengths = sorted(self.kappa_measurements.keys())
        Îº_means = [self.kappa_measurements[L]["mean"] for L in lengths]
        Îº_errors = [self.kappa_measurements[L]["sem"] for L in lengths]

        ax1.errorbar(lengths, Îº_means, yerr=Îº_errors, marker="o", capsize=5, label="Îº_attention")
        ax1.axhline(y=41.09, color="r", linestyle="--", alpha=0.5, label="Îº_physics (L=3)")
        ax1.axhline(y=64.47, color="r", linestyle="--", alpha=0.5, label="Îº_physics (L=4)")
        ax1.set_xlabel("Context Length")
        ax1.set_ylabel("Îº (Effective Coupling)")
        ax1.set_xscale("log")
        ax1.set_title("Running Coupling vs Scale")
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        # Plot 2: Î² vs scale
        beta_scales = [(L + L_next) / 2 for (L, L_next) in self.beta_function.keys()]
        beta_values = [Î² for (Î², _) in self.beta_function.values()]
        beta_errors = [Ïƒ for (_, Ïƒ) in self.beta_function.values()]

        ax2.errorbar(
            beta_scales, beta_values, yerr=beta_errors, marker="s", capsize=5, label="Î²_attention", color="blue"
        )
        ax2.axhline(y=0.44, color="r", linestyle="--", alpha=0.5, label="Î²_physics (L=3â†’4)")
        ax2.axhline(y=0.0, color="gray", linestyle="-", alpha=0.3)
        ax2.set_xlabel("Scale (avg context length)")
        ax2.set_ylabel("Î² (Running coupling)")
        ax2.set_xscale("log")
        ax2.set_title("Î²-Function vs Scale")
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(output_path, dpi=150, bbox_inches="tight")
        print(f"\nPlot saved to: {output_path}")
        plt.close()


def main():
    """Main execution function."""
    parser = argparse.ArgumentParser(description="Î²_attention Measurement and Validation")
    parser.add_argument(
        "--model_path", type=str, default=None, help="Path to trained model (if None, uses validation stub)"
    )
    parser.add_argument(
        "--context_lengths", type=int, nargs="+", default=[128, 256, 512, 1024, 2048], help="Context lengths to test"
    )
    parser.add_argument("--n_samples", type=int, default=200, help="Samples per context length")
    parser.add_argument("--output_dir", type=str, default="./validation_results", help="Output directory for results")
    parser.add_argument("--plot", action="store_true", help="Generate plots")

    args = parser.parse_args()

    # Create output directory
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # Load or create model
    if args.model_path:
        print(f"Loading model from: {args.model_path}")
        # TODO: Implement model loading
        model = None
    else:
        print("Using validation stub (for testing protocol)")
        # Create minimal stub for protocol testing
        from src.model.qig_kernel_recursive import QIGKernelRecursive

        model = QIGKernelRecursive(d_model=256, vocab_size=1000, n_heads=4, min_recursion_depth=3)

    # Run validation
    validator = BetaAttentionValidator(
        model=model, context_lengths=args.context_lengths, n_samples_per_length=args.n_samples
    )

    results = validator.run_full_measurement(verbose=True)

    # Compare to physics
    comparison = validator.compare_to_physics(verbose=True)
    results["comparison_to_physics"] = comparison

    # Save results
    results_path = output_dir / f"beta_attention_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    print(f"\nResults saved to: {results_path}")

    # Plot if requested
    if args.plot and PLOTTING_AVAILABLE:
        plot_path = output_dir / f"beta_attention_plot_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
        validator.plot_results(str(plot_path))

    print("\n" + "=" * 60)
    print("VALIDATION COMPLETE")
    print("=" * 60)


if __name__ == "__main__":
    main()
