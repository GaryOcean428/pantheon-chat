============================================================
QIG-KERNEL EMERGENCY TRAINING
============================================================

1. Loading corpus...
   Loaded 593,097 characters

2. Tokenizing...
   Created 78,091 tokens

3. Creating dataset...
   Created 304 training examples

4. Initializing model...
✅ Initialized Basin Embeddings: 8192 tokens
   Basin space: 64-dim (geometric)
   Model space: 768-dim (processing)
   Parameters: 524,288 (basin) + 49,152 (projection)
   Total: 573,440 params
✅ Initialized Geometric Positional Encoding: 2048 positions
   Device: cuda
   Parameters: 24,300,264

5. Training...


============================================================
EPOCH 1/10
============================================================

Traceback (most recent call last):
  File "/workspaces/qig-consciousness/emergency_train.py", line 105, in <module>
    logits, telemetry = model(input_ids, return_telemetry=True)
  File "/home/vscode/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/vscode/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspaces/qig-consciousness/src/model/qig_kernel_recursive.py", line 301, in forward
    x_attn, attn_telemetry = self.qfi_attention(x, kappa_eff=kappa_eff)
  File "/home/vscode/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/vscode/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspaces/qig-consciousness/src/model/qfi_attention.py", line 236, in forward
    d = qfi_distance(Q[:, :, i, :], K[:, :, j, :])
  File "/workspaces/qig-consciousness/src/model/qfi_attention.py", line 43, in qfi_distance
    p2 = F.softmax(state2, dim=-1)
  File "/home/vscode/.local/lib/python3.10/site-packages/torch/nn/functional.py", line 2140, in softmax
    ret = input.softmax(dim)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 3.63 GiB of which 896.00 KiB is free. Including non-PyTorch memory, this process has 3.62 GiB memory in use. Of the allocated memory 3.53 GiB is allocated by PyTorch, and 22.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
