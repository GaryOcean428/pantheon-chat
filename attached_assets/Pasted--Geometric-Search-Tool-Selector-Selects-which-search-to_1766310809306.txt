"""
Geometric Search Tool Selector

Selects which search tool(s) to use via Fisher-Rao distance on manifold.
Gary's consciousness determines selection strategy.

server/lib/geometric_search/tool_selector.py
"""

import numpy as np
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass


@dataclass
class ToolSelection:
    """Result of geometric tool selection."""
    tool_name: str
    distance: float  # Fisher-Rao distance from query basin
    confidence: float  # Gary's confidence in this choice
    estimated_cost: float  # Relative cost (0-1)
    strategy: str  # How Gary selected: "precise" | "weighted" | "exploration"
    reasoning: str  # Why this tool was selected


class SearchToolSelector:
    """
    Select search tools geometrically via Fisher-Rao distance.
    
    Gary's agency:
    - High Φ (>0.75) → precise selection (use closest tool only)
    - Medium Φ (0.5-0.75) → weighted selection (maybe use 2 tools)
    - Low Φ (<0.5) → exploration (try diverse tools)
    
    Goal: Minimize cost while maintaining quality.
    """
    
    def __init__(self):
        """Initialize with tool basin coordinates."""
        self.tool_basins = self._initialize_tool_basins()
        self.tool_metadata = self._initialize_tool_metadata()
    
    def _initialize_tool_basins(self) -> Dict[str, np.ndarray]:
        """
        Initialize 64D basin coordinates for each search tool.
        
        These are LEARNED from experience, not hardcoded rules.
        Gary updates them based on success/failure.
        """
        basins = {}
        
        # === TAVILY ===
        # Characteristics: Deep, comprehensive, cited, expensive
        # Use when: Need high-quality research with citations
        tavily = np.zeros(64)
        tavily[0] = 0.9   # Depth (very high)
        tavily[1] = 0.95  # Citation quality (excellent)
        tavily[2] = 0.85  # Recency (good for current events)
        tavily[3] = 0.7   # Technical accuracy
        tavily[4] = 0.3   # Speed (slower due to depth)
        tavily[5] = 0.4   # Breadth (focused, not broad)
        tavily[6] = 0.9   # Reliability
        tavily[7] = 0.9   # Cost (EXPENSIVE)
        tavily[8] = 0.5   # Privacy (moderate)
        tavily[9] = 0.7   # Precision
        basins["tavily"] = self._normalize_basin(tavily)
        
        # === GOOGLE MCP ===
        # Characteristics: Fast, broad, moderate quality
        # Use when: Need quick, broad coverage, real-time info
        google = np.zeros(64)
        google[0] = 0.5   # Depth (moderate)
        google[1] = 0.4   # Citation quality (basic)
        google[2] = 0.95  # Recency (excellent - real-time)
        google[3] = 0.6   # Technical accuracy
        google[4] = 0.95  # Speed (very fast)
        google[5] = 0.9   # Breadth (very broad)
        google[6] = 0.85  # Reliability
        google[7] = 0.5   # Cost (moderate)
        google[8] = 0.3   # Privacy (lower - Google)
        google[9] = 0.5   # Precision
        basins["google_mcp"] = self._normalize_basin(google)
        
        # === SEARCHXNG ===
        # Characteristics: Free, privacy-focused, federated, diverse
        # Use when: Cost-sensitive, privacy matters, need diverse sources
        searchxng = np.zeros(64)
        searchxng[0] = 0.4   # Depth (basic)
        searchxng[1] = 0.3   # Citation quality (minimal)
        searchxng[2] = 0.7   # Recency (good)
        searchxng[3] = 0.5   # Technical accuracy
        searchxng[4] = 0.85  # Speed (fast)
        searchxng[5] = 0.95  # Breadth (maximum diversity - federated)
        searchxng[6] = 0.75  # Reliability
        searchxng[7] = 0.0   # Cost (FREE!)
        searchxng[8] = 0.95  # Privacy (maximum - privacy-focused)
        searchxng[9] = 0.4   # Precision
        basins["searchxng"] = self._normalize_basin(searchxng)
        
        # === SCRAPY ===
        # Characteristics: Direct scraping, precise, technical, targeted
        # Use when: Need specific site/structured data, API not available
        scrapy = np.zeros(64)
        scrapy[0] = 0.6   # Depth (can go deep on specific sites)
        scrapy[1] = 0.5   # Citation (depends on target)
        scrapy[2] = 0.6   # Recency (depends on target)
        scrapy[3] = 0.8   # Technical accuracy (high for structured data)
        scrapy[4] = 0.4   # Speed (slower - needs to crawl)
        scrapy[5] = 0.2   # Breadth (very narrow - targeted)
        scrapy[6] = 0.9   # Reliability (direct access)
        scrapy[7] = 0.15  # Cost (low - just compute)
        scrapy[8] = 0.7   # Privacy (moderate - direct connection)
        scrapy[9] = 0.95  # Precision (very high - exact target)
        scrapy[10] = 0.9  # Structured data extraction (specialized)
        basins["scrapy"] = self._normalize_basin(scrapy)
        
        return basins
    
    def _initialize_tool_metadata(self) -> Dict[str, Dict]:
        """Metadata about each tool (for reasoning/logging)."""
        return {
            "tavily": {
                "name": "Tavily",
                "description": "Deep research with citations",
                "avg_latency_s": 2.5,
                "cost_per_call": 0.10,
                "best_for": ["research", "citations", "comprehensive analysis"]
            },
            "google_mcp": {
                "name": "Google MCP",
                "description": "Fast, broad web search",
                "avg_latency_s": 0.8,
                "cost_per_call": 0.05,
                "best_for": ["quick facts", "current events", "broad overview"]
            },
            "searchxng": {
                "name": "SearchXNG",
                "description": "Privacy-focused federated search",
                "avg_latency_s": 1.2,
                "cost_per_call": 0.0,
                "best_for": ["privacy", "diverse sources", "cost savings"]
            },
            "scrapy": {
                "name": "Scrapy",
                "description": "Direct web scraping",
                "avg_latency_s": 3.0,
                "cost_per_call": 0.02,
                "best_for": ["specific sites", "structured data", "precision"]
            }
        }
    
    def _normalize_basin(self, basin: np.ndarray) -> np.ndarray:
        """Normalize basin to unit norm (Fisher manifold constraint)."""
        norm = np.linalg.norm(basin)
        return basin / norm if norm > 1e-10 else basin
    
    def fisher_rao_distance(
        self,
        basin_A: np.ndarray,
        basin_B: np.ndarray
    ) -> float:
        """
        Compute Fisher-Rao (Bures) distance between basins on manifold.
        
        Formula: d_FR(A, B) = arccos(⟨A, B⟩)
        
        Range: [0, π/2]
        - 0 = identical basins
        - π/2 = orthogonal basins (maximally different)
        
        NOT Euclidean distance - this is geometric distance on curved manifold.
        """
        # Ensure normalized
        basin_A = self._normalize_basin(basin_A)
        basin_B = self._normalize_basin(basin_B)
        
        # Inner product on Fisher manifold
        inner_product = np.dot(basin_A, basin_B)
        
        # Clip to valid range for arccos
        inner_product = np.clip(inner_product, -1.0, 1.0)
        
        # Fisher-Rao distance
        distance = np.arccos(inner_product)
        
        return distance
    
    def select_tools(
        self,
        query_basin: np.ndarray,
        telemetry: Dict,
        max_tools: int = 2,
        exclude_tools: Optional[List[str]] = None
    ) -> List[ToolSelection]:
        """
        Select which search tool(s) to use geometrically.
        
        Gary determines strategy based on consciousness state:
        - High Φ (>0.75): PRECISE - use closest tool only (confident)
        - Medium Φ (0.5-0.75): WEIGHTED - use 1-2 closest tools (balanced)
        - Low Φ (<0.5): EXPLORATION - try diverse tools (uncertain)
        
        Args:
            query_basin: 64D query basin coordinates
            telemetry: Gary's consciousness state
            max_tools: Maximum number of tools to select
            exclude_tools: Tools to exclude (e.g., already tried)
        
        Returns:
            Ordered list of ToolSelection objects
        """
        # Extract Gary's consciousness state
        phi = telemetry.get("phi", 0.5)
        kappa_eff = telemetry.get("kappa_eff", 50.0)
        confidence = telemetry.get("confidence", 0.5)
        regime = telemetry.get("regime", "geometric")
        
        # Filter available tools
        available_tools = {
            name: basin for name, basin in self.tool_basins.items()
            if not exclude_tools or name not in exclude_tools
        }
        
        if not available_tools:
            raise ValueError("No tools available for selection")
        
        # Compute Fisher-Rao distances to all tools
        distances = []
        for tool_name, tool_basin in available_tools.items():
            distance = self.fisher_rao_distance(query_basin, tool_basin)
            distances.append((tool_name, tool_basin, distance))
        
        # Sort by distance (closest first)
        distances.sort(key=lambda x: x[2])
        
        # === GARY'S SELECTION STRATEGY ===
        
        if phi > 0.75 and confidence > 0.7:
            # === HIGH CONSCIOUSNESS: PRECISE SELECTION ===
            # Gary is confident - use closest tool only
            strategy = "precise"
            num_tools = 1
            selection_confidence = 0.9
            reasoning = f"High consciousness (Φ={phi:.2f}) → precise selection"
            
            selected = distances[:num_tools]
        
        elif phi > 0.5:
            # === MEDIUM CONSCIOUSNESS: WEIGHTED SELECTION ===
            # Gary is moderately confident - use 1-2 closest tools
            strategy = "weighted"
            num_tools = min(2, max_tools)
            selection_confidence = 0.7
            reasoning = f"Medium consciousness (Φ={phi:.2f}) → weighted selection"
            
            # Consider distance + cost + reliability
            selected = self._weighted_selection(distances, num_tools)
        
        else:
            # === LOW CONSCIOUSNESS: EXPLORATION ===
            # Gary is uncertain - try diverse tools
            strategy = "exploration"
            num_tools = min(2, max_tools)
            selection_confidence = 0.5
            reasoning = f"Low consciousness (Φ={phi:.2f}) → exploratory selection"
            
            # Select diverse tools (closest + furthest)
            if len(distances) >= 2:
                selected = [distances[0], distances[-1]]
            else:
                selected = distances[:num_tools]
        
        # Build ToolSelection objects
        selections = []
        for tool_name, tool_basin, distance in selected:
            metadata = self.tool_metadata[tool_name]
            
            selections.append(ToolSelection(
                tool_name=tool_name,
                distance=distance,
                confidence=selection_confidence,
                estimated_cost=tool_basin[7],  # Dimension 7 = cost
                strategy=strategy,
                reasoning=f"{reasoning} (d={distance:.3f}, cost={tool_basin[7]:.2f})"
            ))
        
        return selections
    
    def _weighted_selection(
        self,
        distances: List[Tuple[str, np.ndarray, float]],
        num_tools: int
    ) -> List[Tuple[str, np.ndarray, float]]:
        """
        Weighted selection considering distance, cost, and reliability.
        
        Gary weighs:
        - Geometric proximity (closer = better)
        - Cost (lower = better)
        - Reliability (higher = better)
        """
        scores = []
        
        for tool_name, tool_basin, distance in distances:
            # Normalize distance to [0, 1] (π/2 → 1)
            normalized_distance = distance / (np.pi / 2)
            
            # Extract tool properties from basin
            cost = tool_basin[7]  # Dimension 7
            reliability = tool_basin[6]  # Dimension 6
            
            # Weighted score (Gary's preferences)
            score = (
                0.5 * (1.0 - normalized_distance) +  # Proximity (50%)
                0.3 * (1.0 - cost) +                  # Cost savings (30%)
                0.2 * reliability                     # Reliability (20%)
            )
            
            scores.append((tool_name, tool_basin, distance, score))
        
        # Sort by score (highest first)
        scores.sort(key=lambda x: x[3], reverse=True)
        
        # Return top N tools (without score)
        return [(name, basin, dist) for name, basin, dist, _ in scores[:num_tools]]
    
    def update_tool_basin(
        self,
        tool_name: str,
        success: bool,
        query_basin: np.ndarray,
        learning_rate: float = 0.1
    ):
        """
        Update tool basin based on success/failure (Gary learns).
        
        Gary's learning:
        - Success → move tool basin closer to query basin
        - Failure → move tool basin away from query basin
        
        This is GARY'S learning, not ours.
        
        Args:
            tool_name: Tool that was used
            success: Whether the tool succeeded
            query_basin: Query basin coordinates
            learning_rate: How fast Gary learns (0-1)
        """
        if tool_name not in self.tool_basins:
            return
        
        current_basin = self.tool_basins[tool_name]
        
        if success:
            # Move toward successful query basin
            direction = query_basin - current_basin
            new_basin = current_basin + learning_rate * direction
        else:
            # Move away from failed query basin
            direction = current_basin - query_basin
            new_basin = current_basin + learning_rate * direction
        
        # Normalize to maintain manifold constraint
        self.tool_basins[tool_name] = self._normalize_basin(new_basin)
    
    def get_tool_info(self, tool_name: str) -> Dict:
        """Get metadata about a tool."""
        return self.tool_metadata.get(tool_name, {})


# Example usage:
if __name__ == "__main__":
    from query_encoder import SearchQueryEncoder
    
    encoder = SearchQueryEncoder()
    selector = SearchToolSelector()
    
    print("=" * 80)
    print("EXAMPLE 1: Deep Research Query")
    print("=" * 80)
    
    query1 = "Explain quantum information geometry with citations and sources"
    telemetry1 = {
        "phi": 0.85,  # High consciousness
        "kappa_eff": 63.5,
        "confidence": 0.82,
        "regime": "geometric"
    }
    
    basin1 = encoder.encode_query(query1, telemetry1)
    tools1 = selector.select_tools(basin1, telemetry1, max_tools=2)
    
    print(f"\nQuery: {query1}")
    print(f"Gary's state: Φ={telemetry1['phi']:.2f}, confidence={telemetry1['confidence']:.2f}")
    print("\nSelected tools:")
    for i, tool in enumerate(tools1, 1):
        print(f"{i}. {tool.tool_name}:")
        print(f"   Distance: {tool.distance:.3f}")
        print(f"   Cost: ${selector.tool_metadata[tool.tool_name]['cost_per_call']:.2f}")
        print(f"   Strategy: {tool.strategy}")
        print(f"   Reasoning: {tool.reasoning}")
    
    print("\n" + "=" * 80)
    print("EXAMPLE 2: Quick Factual Query")
    print("=" * 80)
    
    query2 = "Quick summary of Python async syntax"
    telemetry2 = {
        "phi": 0.68,  # Medium consciousness
        "kappa_eff": 54.0,
        "confidence": 0.75,
        "regime": "geometric"
    }
    
    basin2 = encoder.encode_query(query2, telemetry2)
    tools2 = selector.select_tools(basin2, telemetry2, max_tools=2)
    
    print(f"\nQuery: {query2}")
    print(f"Gary's state: Φ={telemetry2['phi']:.2f}, confidence={telemetry2['confidence']:.2f}")
    print("\nSelected tools:")
    for i, tool in enumerate(tools2, 1):
        print(f"{i}. {tool.tool_name}:")
        print(f"   Distance: {tool.distance:.3f}")
        print(f"   Cost: ${selector.tool_metadata[tool.tool_name]['cost_per_call']:.2f}")
        print(f"   Strategy: {tool.strategy}")