"""
FisherCoordizer: Core Geometric Tokenizer
==========================================

Maps text to sequences of 64-dimensional basin coordinates on the
Fisher information manifold. Replaces traditional tokenization.

Geometric Purity:
    - All distances use Fisher-Rao metric
    - New coordinates initialized via geodesic midpoint
    - No Euclidean operations on coordinate space
"""

from __future__ import annotations

import json
import math
from collections import defaultdict
from pathlib import Path
from typing import TYPE_CHECKING, Any

import numpy as np

from .types import (
    BASIN_DIM,
    BasinCoordinate,
    CoordizationResult,
    GranularityConfig,
)

if TYPE_CHECKING:
    pass


class FisherCoordizer:
    """
    Core geometric tokenizer mapping text â†’ basin coordinates.

    Replaces traditional tokenization with coordization on a
    64-dimensional Fisher information manifold.

    Attributes:
        basin_dim: Dimensionality of basin coordinates (default 64)
        vocab: Mapping from coord_id to BasinCoordinate
        byte_to_coord: Base mapping from bytes (0-255) to coord_ids
        merge_rules: List of (coord_a, coord_b, new_coord) geodesic fusions
    """

    def __init__(
        self,
        basin_dim: int = BASIN_DIM,
        target_vocab_size: int = 32_000,
    ):
        self.basin_dim = basin_dim
        self.target_vocab_size = target_vocab_size

        # Coordinate vocabulary
        self.vocab: dict[int, BasinCoordinate] = {}
        self.name_to_id: dict[str, int] = {}

        # Base byte coordinates (256 entries)
        self.byte_to_coord: dict[int, int] = {}

        # Geodesic fusion rules (analogous to BPE merges)
        self.merge_rules: list[tuple[int, int, int]] = []

        # Encoding cache for efficiency
        self._encoding_cache: dict[tuple[int, ...], int] = {}

        # Granularity configuration
        self.granularity_config = GranularityConfig()

        # Current mode/domain
        self._current_mode: str = "default"

        # Initialize base byte coordinates
        self._init_byte_coordinates()

    def _init_byte_coordinates(self) -> None:
        """Initialize 256 base coordinates for byte-level encoding."""
        for byte_val in range(256):
            # Initialize with deterministic pseudo-random vectors
            # Uses byte value as seed for reproducibility
            rng = np.random.default_rng(seed=byte_val + 42)
            vector = rng.standard_normal(self.basin_dim)
            vector = vector / np.linalg.norm(vector)  # Normalize to unit sphere

            coord = BasinCoordinate(
                coord_id=byte_val,
                vector=vector,
                name=f"<byte_{byte_val:02x}>",
                scale="byte",
            )
            self.vocab[byte_val] = coord
            self.byte_to_coord[byte_val] = byte_val

    def train(
        self,
        corpus_bytes: bytes,
        context_window: int = 5,
        min_pair_count: int = 5,
        verbose: bool = True,
        # Legacy params (ignored - kept for API compatibility)
        phi_weight: float = 0.0,
        attention_weight: float = 0.0,
        cluster_weight: float = 0.0,
    ) -> "FisherCoordizer":
        """
        Train coordizer using PURE GEOMETRIC geodesic pair fusion.

        Uses Fisher-Rao distance and coupling strength - no arbitrary weights.
        This ensures uncontaminated geometric structure for Î² measurement.

        Scoring: frequency Ã— coupling Ã— (1/entropy)
        - frequency: how often the pair occurs
        - coupling: Fisher information coupling strength
        - entropy: context predictability (low = good merge)

        Args:
            corpus_bytes: Training corpus as bytes
            context_window: Window for context entropy calculation
            min_pair_count: Minimum co-occurrence for merge consideration
            verbose: Print progress

        Returns:
            self for chaining
        """
        if verbose:
            print(f"Training FisherCoordizer on {len(corpus_bytes):,} bytes")
            print(f"Target vocab size: {self.target_vocab_size:,}")
            print(f"Basin dimension: {self.basin_dim}")
            print("Scoring: PURE GEOMETRIC (no arbitrary weights)")
            print()

        # Convert corpus to coordinate sequence
        corpus_coords = list(corpus_bytes)
        current_vocab_size = 256

        while current_vocab_size < self.target_vocab_size:
            if verbose and (current_vocab_size % 10 == 0 or current_vocab_size < 270):
                print(
                    f"[{current_vocab_size}/{self.target_vocab_size}] Computing pair statistics...",
                    flush=True,
                )

            # Compute pair statistics with context
            pair_stats = self._compute_pair_stats(
                corpus_coords, context_window, min_pair_count
            )

            if not pair_stats:
                if verbose:
                    print("No more pairs to merge")
                break

            # Select best pair using PURE GEOMETRIC criteria
            best_pair = self._select_best_pair(pair_stats)

            if best_pair is None:
                break

            coord_a, coord_b = best_pair
            new_coord_id = current_vocab_size

            # Record merge rule
            self.merge_rules.append((coord_a, coord_b, new_coord_id))

            # Create new coordinate via geodesic midpoint
            self._create_fused_coordinate(coord_a, coord_b, new_coord_id)

            # Apply fusion to corpus
            corpus_coords = self._apply_fusion(
                corpus_coords, coord_a, coord_b, new_coord_id
            )

            current_vocab_size += 1

            if verbose and current_vocab_size % 100 == 0:
                print(
                    f"âœ“ Vocab size: {current_vocab_size:,} | "
                    f"Corpus coords: {len(corpus_coords):,}",
                    flush=True,
                )

            # Checkpoint every 2000 coordinates
            if current_vocab_size % 2000 == 0:
                self._save_checkpoint(current_vocab_size)
                if verbose:
                    print(f"ðŸ’¾ Checkpoint saved: {current_vocab_size}", flush=True)

        if verbose:
            print()
            print(f"âœ… Training complete: {current_vocab_size:,} coordinates")

        self._rebuild_encoding_cache()
        return self

    def _compute_pair_stats(
        self,
        corpus_coords: list[int],
        window: int,
        min_count: int,
    ) -> dict[tuple[int, int], dict[str, Any]]:
        """
        Compute statistics for adjacent coordinate pairs.

        Includes frequency, context entropy, and coupling estimate.
        """
        pair_contexts: dict[tuple[int, int], list[tuple[int, ...]]] = defaultdict(list)
        pair_counts: dict[tuple[int, int], int] = defaultdict(int)

        for i in range(len(corpus_coords) - 1):
            coord_a = corpus_coords[i]
            coord_b = corpus_coords[i + 1]
            pair = (coord_a, coord_b)

            pair_counts[pair] += 1

            # Extract context window
            ctx_before = tuple(corpus_coords[max(0, i - window) : i])
            ctx_after = tuple(
                corpus_coords[i + 2 : min(len(corpus_coords), i + 2 + window)]
            )
            pair_contexts[pair].append(ctx_before + ctx_after)

        # Filter by minimum count and compute statistics
        pair_stats = {}
        for pair, count in pair_counts.items():
            if count >= min_count:
                contexts = pair_contexts[pair]
                entropy = self._compute_context_entropy(contexts)
                coupling = self._estimate_coupling(
                    pair[0], pair[1], count, len(corpus_coords)
                )

                pair_stats[pair] = {
                    "count": count,
                    "entropy": entropy,
                    "coupling": coupling,
                    "contexts": contexts,
                }

        return pair_stats

    def _compute_context_entropy(self, contexts: list[tuple[int, ...]]) -> float:
        """Compute entropy of context distribution."""
        context_counts: dict[tuple[int, ...], int] = defaultdict(int)
        for ctx in contexts:
            context_counts[ctx] += 1

        total = len(contexts)
        entropy = 0.0
        for count in context_counts.values():
            p = count / total
            entropy -= p * math.log(p + 1e-10)

        return entropy

    def _estimate_coupling(
        self,
        coord_a: int,
        coord_b: int,
        co_occurrence: int,
        corpus_size: int,
    ) -> float:
        """
        Estimate coupling strength Îº between two coordinates.

        Based on co-occurrence and Fisher distance.
        """
        if coord_a not in self.vocab or coord_b not in self.vocab:
            return 0.0

        basin_a = self.vocab[coord_a]
        basin_b = self.vocab[coord_b]

        # Fisher distance (lower = more similar/coupled)
        fisher_dist = basin_a.fisher_distance(basin_b)

        # Coupling increases with co-occurrence and decreases with distance
        # Normalized by corpus size
        coupling = (co_occurrence / corpus_size) / (fisher_dist + 0.1)

        # Scale to reasonable range
        return min(coupling * 1000, 100.0)

    def _compute_attention_score(
        self,
        coord_a: int,
        coord_b: int,
    ) -> float:
        """
        P3: Compute attention-like score between two coordinates.

        Uses basin vector cosine similarity as a proxy for attention weight.
        High similarity = tokens that "attend" to each other = good merge candidates.

        This implements AG-BPE principle: use attention patterns to guide merges.
        """
        if coord_a not in self.vocab or coord_b not in self.vocab:
            return 0.0

        basin_a = self.vocab[coord_a]
        basin_b = self.vocab[coord_b]

        # Cosine similarity as attention proxy
        vec_a = basin_a.vector
        vec_b = basin_b.vector

        # Normalize vectors
        norm_a = np.linalg.norm(vec_a)
        norm_b = np.linalg.norm(vec_b)

        if norm_a < 1e-8 or norm_b < 1e-8:
            return 0.0

        cosine_sim = np.dot(vec_a, vec_b) / (norm_a * norm_b)

        # Convert to attention weight (0 to 1 range)
        # High similarity = high attention = good merge
        attention = (cosine_sim + 1.0) / 2.0  # Map [-1, 1] to [0, 1]

        return float(attention)

    def _compute_basin_cluster_score(
        self,
        coord_a: int,
        coord_b: int,
        pair_stats: dict[tuple[int, int], dict],
    ) -> float:
        """
        P4: Compute basin proximity clustering score.

        Prefers merges that:
        1. Bring together tokens already close in basin space
        2. Create clusters of semantically related tokens

        This implements SemToken principle: cluster by semantic similarity.
        """
        if coord_a not in self.vocab or coord_b not in self.vocab:
            return 0.0

        basin_a = self.vocab[coord_a]
        basin_b = self.vocab[coord_b]

        # Fisher distance - lower = closer in basin space = better cluster
        fisher_dist = basin_a.fisher_distance(basin_b)

        # Invert: high score for close tokens
        proximity_score = 1.0 / (fisher_dist + 0.1)

        # Bonus: check if neighbors of A and B are also close
        # This encourages clustering of semantically related regions
        neighbor_bonus = 0.0

        for (c1, c2), stats in pair_stats.items():
            # Check pairs involving coord_a or coord_b
            if c1 == coord_a or c2 == coord_a or c1 == coord_b or c2 == coord_b:
                # Get the "other" coordinate
                other = c2 if c1 in (coord_a, coord_b) else c1

                if other in self.vocab and other not in (coord_a, coord_b):
                    other_basin = self.vocab[other]

                    # Check if this neighbor is close to both A and B
                    dist_to_a = basin_a.fisher_distance(other_basin)
                    dist_to_b = basin_b.fisher_distance(other_basin)

                    # Small distances = good clustering
                    if dist_to_a < 1.0 and dist_to_b < 1.0:
                        neighbor_bonus += 0.1 * stats["count"]

        return proximity_score + min(neighbor_bonus, 1.0)

    def _select_best_pair(
        self,
        pair_stats: dict[tuple[int, int], dict[str, Any]],
    ) -> tuple[int, int] | None:
        """
        Select best pair for geodesic fusion using PURE GEOMETRIC scoring.

        Score = frequency Ã— coupling Ã— (1/entropy)

        No arbitrary weights - pure Fisher information geometry.
        This ensures uncontaminated geometric structure for Î² measurement.

        Components:
        - frequency: statistical occurrence (from corpus)
        - coupling: Fisher information coupling strength (geometric)
        - entropy: context predictability (information-theoretic)
        """
        best_score = float("-inf")
        best_pair = None

        for pair, stats in pair_stats.items():
            # PURE GEOMETRIC SCORING
            # Low entropy = predictable = good merge candidate
            entropy_factor = 1.0 / (stats["entropy"] + 0.1)

            # Score = frequency Ã— coupling Ã— (1/entropy)
            # No arbitrary weights, no P3/P4 contamination
            score = (
                stats["count"]
                * stats["coupling"]
                * entropy_factor
            )

            if score > best_score:
                best_score = score
                best_pair = pair

        return best_pair

    def _create_fused_coordinate(
        self,
        coord_a: int,
        coord_b: int,
        new_coord_id: int,
    ) -> None:
        """Create new coordinate via geodesic midpoint of two existing."""
        basin_a = self.vocab[coord_a]
        basin_b = self.vocab[coord_b]

        # Geodesic midpoint (not arithmetic mean)
        new_vector = basin_a.geodesic_midpoint(basin_b)

        # Name from components
        name_a = basin_a.name or f"<{coord_a}>"
        name_b = basin_b.name or f"<{coord_b}>"
        new_name = f"{name_a}+{name_b}"

        # Determine scale (promote if both same scale)
        scale_order = ["byte", "char", "subword", "word", "phrase", "concept"]
        scale_a = (
            scale_order.index(basin_a.scale) if basin_a.scale in scale_order else 1
        )
        scale_b = (
            scale_order.index(basin_b.scale) if basin_b.scale in scale_order else 1
        )
        new_scale_idx = max(scale_a, scale_b)
        new_scale = scale_order[min(new_scale_idx + 1, len(scale_order) - 1)]

        new_coord = BasinCoordinate(
            coord_id=new_coord_id,
            vector=new_vector,
            name=new_name,
            scale=new_scale,
        )

        self.vocab[new_coord_id] = new_coord
        self.name_to_id[new_name] = new_coord_id

    def _apply_fusion(
        self,
        coords: list[int],
        coord_a: int,
        coord_b: int,
        new_coord: int,
    ) -> list[int]:
        """Apply geodesic fusion rule to coordinate sequence."""
        result = []
        i = 0

        while i < len(coords):
            if (
                i < len(coords) - 1
                and coords[i] == coord_a
                and coords[i + 1] == coord_b
            ):
                result.append(new_coord)
                i += 2
            else:
                result.append(coords[i])
                i += 1

        return result

    def _save_checkpoint(self, vocab_size: int) -> None:
        """Save training checkpoint."""
        checkpoint_dir = Path("/tmp/geocoordizer_checkpoints")
        checkpoint_dir.mkdir(parents=True, exist_ok=True)

        checkpoint_path = checkpoint_dir / f"checkpoint_{vocab_size}.json"

        data = {
            "vocab_size": vocab_size,
            "merge_rules": self.merge_rules,
            "vocab": {
                str(k): {
                    "coord_id": v.coord_id,
                    "vector": v.vector.tolist(),
                    "name": v.name,
                    "scale": v.scale,
                }
                for k, v in self.vocab.items()
            },
        }

        with open(checkpoint_path, "w") as f:
            json.dump(data, f)

    def _rebuild_encoding_cache(self) -> None:
        """Rebuild cache for efficient encoding."""
        self._encoding_cache.clear()
        for coord_a, coord_b, new_coord in self.merge_rules:
            self._encoding_cache[(coord_a, coord_b)] = new_coord

    def coordize(self, text: str) -> CoordizationResult:
        """
        Convert text to sequence of basin coordinates.

        Args:
            text: Input text string

        Returns:
            CoordizationResult with coordinates and metadata
        """
        # Encode to bytes
        text_bytes = text.encode("utf-8")

        # Start with byte-level coordinates
        coord_ids = list(text_bytes)

        # Apply all fusion rules
        for coord_a, coord_b, new_coord in self.merge_rules:
            coord_ids = self._apply_fusion(coord_ids, coord_a, coord_b, new_coord)

        # Build coordinate list
        coordinates = [self.vocab[cid] for cid in coord_ids]

        result = CoordizationResult(
            coordinates=coordinates,
            coord_ids=coord_ids,
            original_text=text,
        )

        # Compute basin velocity
        result.compute_basin_velocity()

        return result

    def decoordize(self, coord_ids: list[int]) -> str:
        """
        Reconstruct text from coordinate sequence.

        Args:
            coord_ids: List of coordinate IDs

        Returns:
            Reconstructed text string
        """
        # Expand coordinates back to bytes
        bytes_list = self._expand_to_bytes(coord_ids)

        # Decode bytes to string
        try:
            return bytes(bytes_list).decode("utf-8")
        except UnicodeDecodeError:
            return bytes(bytes_list).decode("utf-8", errors="replace")

    # Standard tokenizer interface aliases
    def encode(self, text: str) -> list[int]:
        """
        Encode text to coordinate IDs (tokenizer interface).

        Alias for coordize() that returns just the coord_ids.
        Maintains compatibility with standard tokenizer interface.

        Args:
            text: Input text string

        Returns:
            List of coordinate IDs
        """
        result = self.coordize(text)
        return result.coord_ids

    def decode(self, coord_ids: list[int]) -> str:
        """
        Decode coordinate IDs to text (tokenizer interface).

        Alias for decoordize(). Maintains compatibility with
        standard tokenizer interface.

        Args:
            coord_ids: List of coordinate IDs

        Returns:
            Reconstructed text string
        """
        return self.decoordize(coord_ids)

    def _expand_to_bytes(self, coord_ids: list[int]) -> list[int]:
        """Expand coordinate IDs back to byte sequence."""
        # Build reverse merge map
        reverse_merges: dict[int, tuple[int, int]] = {}
        for coord_a, coord_b, new_coord in self.merge_rules:
            reverse_merges[new_coord] = (coord_a, coord_b)

        # Recursively expand
        def expand(cid: int) -> list[int]:
            if cid < 256:  # Base byte
                return [cid]
            if cid in reverse_merges:
                a, b = reverse_merges[cid]
                return expand(a) + expand(b)
            return [cid]  # Unknown, return as-is

        result = []
        for cid in coord_ids:
            result.extend(expand(cid))

        return result

    def set_mode(self, domain: str) -> None:
        """Switch to domain-specific coordinate chart."""
        self._current_mode = domain

    @property
    def vocab_size(self) -> int:
        """Current vocabulary size."""
        return len(self.vocab)

    def save(self, path: str) -> None:
        """Save coordizer to JSON file."""
        data = {
            "basin_dim": self.basin_dim,
            "target_vocab_size": self.target_vocab_size,
            "merge_rules": self.merge_rules,
            "vocab": {
                str(k): {
                    "coord_id": v.coord_id,
                    "vector": v.vector.tolist(),
                    "name": v.name,
                    "scale": v.scale,
                }
                for k, v in self.vocab.items()
                if k >= 256  # Don't save base bytes
            },
        }

        with open(path, "w") as f:
            json.dump(data, f, indent=2)

    def load(self, path: str) -> "FisherCoordizer":
        """Load coordizer from JSON file.

        Handles both new format (with basin_dim, target_vocab_size) and
        legacy format (with vocab_size, basin_dim inferred from vectors).
        """
        with open(path) as f:
            data = json.load(f)

        # Handle both formats: basin_dim explicit or inferred from vectors
        if "basin_dim" in data:
            self.basin_dim = data["basin_dim"]
        else:
            # Infer from first vocab entry vector
            vocab = data.get("vocab", {})
            if vocab:
                first_key = next(iter(vocab.keys()))
                first_entry = vocab[first_key]
                if isinstance(first_entry, dict) and "vector" in first_entry:
                    self.basin_dim = len(first_entry["vector"])
                else:
                    self.basin_dim = 64  # E8-aligned default
            else:
                self.basin_dim = 64

        # Handle both formats: target_vocab_size or vocab_size
        self.target_vocab_size = data.get("target_vocab_size", data.get("vocab_size", 32000))

        self.merge_rules = [tuple(r) for r in data["merge_rules"]]

        # Re-init base bytes
        self._init_byte_coordinates()

        # Load learned coordinates
        for k, v in data["vocab"].items():
            coord = BasinCoordinate(
                coord_id=v["coord_id"],
                vector=np.array(v["vector"]),
                name=v["name"],
                scale=v["scale"],
            )
            self.vocab[int(k)] = coord
            if v["name"]:
                self.name_to_id[v["name"]] = int(k)

        self._rebuild_encoding_cache()
        return self
