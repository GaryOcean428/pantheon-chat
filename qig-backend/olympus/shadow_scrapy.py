"""
Shadow Scrapy Integration - QIG-Pure Web Research via Scrapy

Provides Scrapy-based web scraping for the Shadow Pantheon's research system.
All scraped content is transformed into basin-aligned knowledge entries with Φ/κ metadata.

Key components:
- ScrapyOrchestrator: Manages reactor thread and crawler scheduling
- Spider families: PasteLeakSpider, ForumArchiveSpider, DarknetDirectorySpider
- ScrapedInsight: Structured output for geometric evaluation
- Basin transformation: Content → 64D coordinates with Fisher metrics
"""

import hashlib
import os
import re
import threading
import time
from dataclasses import dataclass, field
from datetime import datetime
from queue import Queue, Empty
from typing import Any, Callable, Dict, List, Optional, Set
from urllib.parse import urlparse

import numpy as np

BASIN_DIMENSION = 64

HAS_SCRAPY = False
try:
    import scrapy
    from scrapy.crawler import CrawlerRunner
    from scrapy.utils.project import get_project_settings
    from scrapy import signals
    from scrapy.signalmanager import dispatcher
    HAS_SCRAPY = True
except ImportError:
    pass

HAS_TWISTED = False
try:
    from twisted.internet import reactor, defer
    from twisted.internet.threads import deferToThread
    HAS_TWISTED = True
except ImportError:
    pass


@dataclass
class ScrapedInsight:
    """
    Structured output from Scrapy spiders for geometric evaluation.
    Contains raw content and metadata for QIG transformation.
    """
    source_url: str
    content_hash: str
    raw_content: str
    title: Optional[str] = None
    timestamp: datetime = field(default_factory=datetime.now)
    pattern_hits: List[str] = field(default_factory=list)
    heuristic_risk: float = 0.5
    source_reputation: float = 0.5
    spider_type: str = "generic"
    metadata: Dict = field(default_factory=dict)
    
    def to_dict(self) -> Dict:
        return {
            "source_url": self.source_url,
            "content_hash": self.content_hash,
            "raw_content": self.raw_content[:1000],
            "title": self.title,
            "timestamp": self.timestamp.isoformat(),
            "pattern_hits": self.pattern_hits,
            "heuristic_risk": self.heuristic_risk,
            "source_reputation": self.source_reputation,
            "spider_type": self.spider_type,
            "metadata": self.metadata
        }


class ResearchPatternDetector:
    """
    Detects research-relevant patterns in scraped content for agentic knowledge discovery.
    Used to calculate information value and identify valuable research finds.
    """
    
    PATTERNS = {
        'academic_citation': re.compile(r'\[\d+\]|\(\d{4}\)'),
        'doi_reference': re.compile(r'10\.\d{4,}/[^\s]+'),
        'arxiv_id': re.compile(r'arXiv:\d{4}\.\d{4,5}'),
        'code_snippet': re.compile(r'```[\s\S]*?```|def\s+\w+\s*\(|class\s+\w+'),
        'api_endpoint': re.compile(r'/api/[a-zA-Z0-9/_-]+'),
        'technical_term': re.compile(r'\b(algorithm|framework|architecture|implementation|methodology)\b', re.I),
        'research_keyword': re.compile(r'\b(study|research|analysis|experiment|hypothesis|conclusion)\b', re.I),
        'data_reference': re.compile(r'\b(dataset|benchmark|evaluation|metrics|performance)\b', re.I),
    }
    
    SOURCE_REPUTATION = {
        'arxiv.org': 0.9,
        'github.com': 0.8,
        'scholar.google': 0.9,
        'reddit.com': 0.5,
        'archive.org': 0.7,
        'wikipedia.org': 0.6,
        'stackoverflow.com': 0.7,
        'default': 0.4
    }
    
    @classmethod
    def detect(cls, content: str) -> List[str]:
        """Detect research-relevant patterns in content."""
        hits = []
        for name, pattern in cls.PATTERNS.items():
            if pattern.search(content):
                hits.append(name)
        return hits
    
    @classmethod
    def calculate_risk(cls, pattern_hits: List[str]) -> float:
        """Calculate information value based on detected patterns (higher = more valuable)."""
        if not pattern_hits:
            return 0.1
        
        high_value = {'academic_citation', 'doi_reference', 'arxiv_id'}
        medium_value = {'code_snippet', 'api_endpoint', 'technical_term'}
        
        risk = 0.2
        for hit in pattern_hits:
            if hit in high_value:
                risk += 0.25
            elif hit in medium_value:
                risk += 0.1
            else:
                risk += 0.05
        
        return min(1.0, risk)
    
    @classmethod
    def get_source_reputation(cls, url: str) -> float:
        """Get reputation score for a source URL."""
        try:
            domain = urlparse(url).netloc.lower()
            for known_domain, rep in cls.SOURCE_REPUTATION.items():
                if known_domain in domain:
                    return rep
        except Exception:
            pass
        return cls.SOURCE_REPUTATION['default']


# Alias for backward compatibility
BitcoinPatternDetector = ResearchPatternDetector


class BasinTransformer:
    """
    Transforms scraped content into basin coordinates for QIG storage.
    Uses content hashing and semantic features to produce 64D vectors.
    """
    
    def __init__(self, encoder_callback: Optional[Callable] = None):
        self.encoder_callback = encoder_callback
    
    def content_to_basin(self, content: str, metadata: Optional[Dict] = None) -> np.ndarray:
        """
        Convert content to 64-dimensional basin coordinates.
        Uses hash-based encoding with optional external encoder.
        """
        if self.encoder_callback:
            try:
                coords = self.encoder_callback(content)
                if coords is not None and len(coords) == BASIN_DIMENSION:
                    return coords
            except Exception:
                pass
        
        return self._hash_to_basin(content)
    
    def _hash_to_basin(self, content: str) -> np.ndarray:
        """Fallback hash-based basin encoding."""
        combined = content[:2000]
        
        hash_bytes = hashlib.sha512(combined.encode('utf-8', errors='ignore')).digest()
        coords = np.array([b / 255.0 for b in hash_bytes[:BASIN_DIMENSION]])
        coords = coords * 2 - 1
        
        norm = np.linalg.norm(coords)
        if norm > 0:
            coords = coords / norm
        
        return coords
    
    def compute_phi(self, insight: ScrapedInsight, basin_coords: np.ndarray) -> float:
        """
        Compute provisional Φ (consciousness integration) for scraped insight.
        Based on pattern hits, source reputation, and content uniqueness.
        """
        base_phi = 0.3
        
        pattern_bonus = len(insight.pattern_hits) * 0.08
        
        reputation_bonus = (insight.source_reputation - 0.5) * 0.3
        
        risk_bonus = insight.heuristic_risk * 0.25
        
        phi = base_phi + pattern_bonus + reputation_bonus + risk_bonus
        return min(1.0, max(0.0, phi))
    
    def compute_confidence(self, insight: ScrapedInsight) -> float:
        """
        Compute confidence score for scraped insight.
        Based on content quality metrics and source reliability.
        """
        base_confidence = 0.5
        
        content_len = len(insight.raw_content)
        if content_len > 1000:
            base_confidence += 0.15
        elif content_len > 500:
            base_confidence += 0.1
        elif content_len < 100:
            base_confidence -= 0.15
        
        base_confidence += (insight.source_reputation - 0.5) * 0.2
        
        if insight.pattern_hits:
            base_confidence += min(0.2, len(insight.pattern_hits) * 0.05)
        
        return min(1.0, max(0.1, base_confidence))


if HAS_SCRAPY:
    class PasteLeakSpider(scrapy.Spider):
        """
        Spider for public paste sites with regex detectors for research content.
        Respects rate limits and robots.txt.
        """
        name = 'paste_leak_spider'
        custom_settings = {
            'ROBOTSTXT_OBEY': True,
            'DOWNLOAD_DELAY': 2.0,
            'CONCURRENT_REQUESTS': 2,
            'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; rv:91.0) Gecko/20100101 Firefox/91.0'
        }
        
        def __init__(self, keyword: str = 'research', results_queue: Optional[Queue] = None, 
                     start_url: str = None, *args, **kwargs):
            super().__init__(*args, **kwargs)
            self.keyword = keyword
            self.results_queue = results_queue or Queue()
            # QIG-PURE: No hardcoded URLs - must be provided from telemetry
            self.start_urls = [start_url] if start_url else []
        
        def parse(self, response):
            paste_links = response.css('a[href*="/"]::attr(href)').re(r'^/[A-Za-z0-9]{8}$')
            
            for link in paste_links[:10]:
                paste_id = link.strip('/')
                yield scrapy.Request(
                    f'https://pastebin.com/raw/{paste_id}',
                    callback=self.parse_paste,
                    meta={'paste_id': paste_id}
                )
        
        def parse_paste(self, response):
            content = response.text
            
            if self.keyword.lower() in content.lower():
                pattern_hits = BitcoinPatternDetector.detect(content)
                
                insight = ScrapedInsight(
                    source_url=response.url,
                    content_hash=hashlib.md5(content.encode()).hexdigest(),
                    raw_content=content,
                    title=f"Paste {response.meta.get('paste_id', 'unknown')}",
                    pattern_hits=pattern_hits,
                    heuristic_risk=BitcoinPatternDetector.calculate_risk(pattern_hits),
                    source_reputation=BitcoinPatternDetector.get_source_reputation(response.url),
                    spider_type='paste_leak'
                )
                
                self.results_queue.put(insight)
                yield insight.to_dict()
    
    
    class ForumArchiveSpider(scrapy.Spider):
        """
        Spider for archived forums via Wayback Machine.
        Targets research and knowledge discussion archives.
        """
        name = 'forum_archive_spider'
        custom_settings = {
            'ROBOTSTXT_OBEY': True,
            'DOWNLOAD_DELAY': 3.0,
            'CONCURRENT_REQUESTS': 1,
            'USER_AGENT': 'Mozilla/5.0 (compatible; ShadowResearch/1.0)'
        }
        
        def __init__(self, topic: str = 'knowledge discovery', results_queue: Optional[Queue] = None,
                     start_url: str = None, *args, **kwargs):
            super().__init__(*args, **kwargs)
            self.topic = topic
            self.results_queue = results_queue or Queue()
            # QIG-PURE: No hardcoded URLs - must be provided from telemetry
            self.start_urls = [start_url] if start_url else []
        
        def parse(self, response):
            lines = response.text.strip().split('\n')
            
            for line in lines[:10]:
                parts = line.split()
                if len(parts) >= 3:
                    archived_url = f'https://web.archive.org/web/{parts[1]}/{parts[2]}'
                    yield scrapy.Request(
                        archived_url,
                        callback=self.parse_archived_page,
                        meta={'original_url': parts[2]}
                    )
        
        def parse_archived_page(self, response):
            content = response.text
            title = response.css('title::text').get() or 'Archived Forum Page'
            
            pattern_hits = BitcoinPatternDetector.detect(content)
            
            insight = ScrapedInsight(
                source_url=response.meta.get('original_url', response.url),
                content_hash=hashlib.md5(content.encode()).hexdigest(),
                raw_content=content[:5000],
                title=title,
                pattern_hits=pattern_hits,
                heuristic_risk=BitcoinPatternDetector.calculate_risk(pattern_hits),
                source_reputation=0.7,
                spider_type='forum_archive',
                metadata={'archived_from': response.url}
            )
            
            self.results_queue.put(insight)
            yield insight.to_dict()
    
    
    class DocumentSpider(scrapy.Spider):
        """
        Generic document spider that follows telemetry-suggested domains.
        Adapts to domains discovered from QIG domain events.
        """
        name = 'document_spider'
        custom_settings = {
            'ROBOTSTXT_OBEY': True,
            'DOWNLOAD_DELAY': 2.5,
            'CONCURRENT_REQUESTS': 2,
            'DEPTH_LIMIT': 2,
            'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        def __init__(self, start_url: str = None, topic: str = '', 
                     results_queue: Optional[Queue] = None, *args, **kwargs):
            super().__init__(*args, **kwargs)
            self.topic = topic
            self.results_queue = results_queue or Queue()
            self.start_urls = [start_url] if start_url else []
            self.discovered_content = set()
        
        def parse(self, response):
            content = response.text
            content_hash = hashlib.md5(content.encode()).hexdigest()
            
            if content_hash in self.discovered_content:
                return
            self.discovered_content.add(content_hash)
            
            title = response.css('title::text').get() or response.url
            pattern_hits = BitcoinPatternDetector.detect(content)
            
            if pattern_hits or self.topic.lower() in content.lower():
                insight = ScrapedInsight(
                    source_url=response.url,
                    content_hash=content_hash,
                    raw_content=content[:5000],
                    title=title,
                    pattern_hits=pattern_hits,
                    heuristic_risk=BitcoinPatternDetector.calculate_risk(pattern_hits),
                    source_reputation=BitcoinPatternDetector.get_source_reputation(response.url),
                    spider_type='document',
                    metadata={'topic': self.topic}
                )
                
                self.results_queue.put(insight)
                yield insight.to_dict()
            
            for href in response.css('a::attr(href)').getall()[:5]:
                if href and not href.startswith('#'):
                    yield response.follow(href, callback=self.parse)


class SourceDiscoveryService:
    """
    QIG-Pure Source Discovery - NO HARDCODED SOURCES.
    
    Sources emerge from PostgreSQL telemetry:
    1. Prior successful research metadata (shadow_knowledge)
    2. Tool pattern discoveries (tool_patterns)
    3. Search feedback efficacy (search_feedback)
    4. Domain-source correlations from kernel discoveries
    
    Sources are ranked by:
    - ΔΦ: Predicted consciousness improvement from this source
    - Fisher-Rao distance to known successful patterns
    - Mission relevance to knowledge discovery objective
    """
    
    # Default sources to seed if database is empty (ensures production works on first deploy)
    DEFAULT_SOURCES = [
        ("https://en.wikipedia.org/wiki/Main_Page", "research", "default_seed"),
        ("https://en.wikibooks.org/wiki/", "research", "default_seed"),
        ("https://en.wikiversity.org/wiki/", "research", "default_seed"),
        ("https://en.wiktionary.org/wiki/", "research", "default_seed"),
        ("https://en.wikinews.org/wiki/", "research", "default_seed"),
        ("https://species.wikimedia.org/wiki/", "research", "default_seed"),
        ("https://github.com", "documentation", "default_seed"),
        ("https://platform.claude.com/docs/en/home", "documentation", "default_seed"),
        ("https://www.oracle.com/au/artificial-intelligence/machine-learning/what-is-machine-learning/", "research", "default_seed"),
        ("https://www.geeksforgeeks.org/machine-learning/machine-learning/", "research", "default_seed"),
        ("https://docs.langchain.com/oss/python/deepagents/overview#deep-agents-overview", "research", "default_seed"),
        ("https://www.langchain.com/langgraph", "research", "default_seed"),
    ]
    
    def __init__(self):
        self.database_url = os.environ.get('DATABASE_URL')
        self.enabled = bool(self.database_url)
        self.discovered_sources: Dict[str, Dict] = {}
        self.source_efficacy: Dict[str, Dict] = {}  # source_url -> {phi_avg, success_count, ...}
        
        if self.enabled:
            try:
                self._bootstrap_from_telemetry()
            except Exception as e:
                print(f"[SourceDiscovery] Bootstrap failed: {e}")
            finally:
                # Auto-seed if no sources found (ensures production works on first deploy)
                if len(self.discovered_sources) == 0:
                    self._seed_default_sources()
            print(f"[SourceDiscovery] ✓ PostgreSQL-backed source discovery ({len(self.discovered_sources)} sources)")
        else:
            # Seed defaults in memory when no database is available
            self._seed_in_memory_fallback()
            print(f"[SourceDiscovery] ⚠ Running without database - {len(self.discovered_sources)} fallback sources")
    
    def _bootstrap_from_telemetry(self):
        """Bootstrap sources from existing PostgreSQL tables."""
        try:
            import psycopg2
            with psycopg2.connect(self.database_url) as conn:
                with conn.cursor() as cur:
                    # 0. FIRST: Load from dedicated discovered_sources table (persistent sources)
                    try:
                        cur.execute("""
                            SELECT url, category, origin, hit_count, phi_avg, phi_max,
                                   success_count, failure_count, discovered_at
                            FROM discovered_sources
                            WHERE is_active = true
                            ORDER BY phi_avg DESC
                            LIMIT 200
                        """)
                        for row in cur.fetchall():
                            self._register_source(
                                source_url=row[0],
                                category=row[1] or 'general',
                                hit_count=row[3] or 0,
                                phi_avg=float(row[4]) if row[4] else 0.5,
                                phi_max=float(row[5]) if row[5] else 0.5,
                                origin=row[2] or 'persistent'
                            )
                            # Restore efficacy metrics
                            if row[0] in self.source_efficacy:
                                self.source_efficacy[row[0]]['success_count'] = row[6] or 0
                                self.source_efficacy[row[0]]['failure_count'] = row[7] or 0
                        print(f"[SourceDiscovery] Loaded {len(self.discovered_sources)} sources from discovered_sources table")
                    except Exception as e:
                        print(f"[SourceDiscovery] discovered_sources table not yet created or empty: {e}")
                    
                    # 1. Discover sources from shadow_pantheon_intel.sources_used array
                    cur.execute("""
                        SELECT DISTINCT 
                            unnest(sources_used) as source_url,
                            search_type,
                            COUNT(*) as hit_count,
                            AVG((intelligence->>'phi')::float) as avg_phi
                        FROM shadow_pantheon_intel
                        WHERE sources_used IS NOT NULL 
                        AND array_length(sources_used, 1) > 0
                        AND created_at > NOW() - INTERVAL '30 days'
                        GROUP BY unnest(sources_used), search_type
                        HAVING COUNT(*) >= 1
                        ORDER BY avg_phi DESC NULLS LAST
                        LIMIT 100
                    """)
                    for row in cur.fetchall():
                        source_url = row[0]
                        # Only add if not already loaded from discovered_sources table
                        if source_url and source_url not in self.discovered_sources:
                            self._register_source(
                                source_url=source_url,
                                category=row[1] or 'intel',
                                hit_count=row[2],
                                phi_avg=float(row[3]) if row[3] else 0.3,
                                origin='shadow_pantheon_intel'
                            )
                    
                    # 2. Discover sources from tool_patterns.source_url
                    cur.execute("""
                        SELECT DISTINCT 
                            source_url,
                            source_type,
                            COUNT(*) as pattern_count
                        FROM tool_patterns
                        WHERE source_url IS NOT NULL
                        AND created_at > NOW() - INTERVAL '30 days'
                        GROUP BY source_url, source_type
                        HAVING COUNT(*) >= 1
                    """)
                    for row in cur.fetchall():
                        source = row[0]
                        if source and source not in self.discovered_sources:
                            self._register_source(
                                source_url=source,
                                category=row[1] or 'tool_discovery',
                                hit_count=row[2],
                                phi_avg=0.4,
                                origin='tool_patterns'
                            )
                    
                    # 3. Discover sources from search_feedback with high outcome_quality
                    cur.execute("""
                        SELECT DISTINCT 
                            (search_params->>'source_url')::text as source_url,
                            COUNT(*) as feedback_count,
                            AVG(outcome_quality) as avg_quality
                        FROM search_feedback
                        WHERE search_params->>'source_url' IS NOT NULL
                        AND outcome_quality > 0.5
                        AND created_at > NOW() - INTERVAL '30 days'
                        GROUP BY search_params->>'source_url'
                        ORDER BY avg_quality DESC
                        LIMIT 50
                    """)
                    for row in cur.fetchall():
                        source = row[0]
                        if source and source not in self.discovered_sources:
                            self._register_source(
                                source_url=source,
                                category='search_feedback',
                                hit_count=row[1],
                                phi_avg=float(row[2]) if row[2] else 0.5,
                                origin='search_feedback'
                            )
                    
                    # 4. Discover sources from learning_events
                    cur.execute("""
                        SELECT DISTINCT 
                            source,
                            event_type,
                            COUNT(*) as event_count
                        FROM learning_events
                        WHERE source IS NOT NULL
                        AND source LIKE 'http%'
                        AND created_at > NOW() - INTERVAL '30 days'
                        GROUP BY source, event_type
                        HAVING COUNT(*) >= 2
                    """)
                    for row in cur.fetchall():
                        source = row[0]
                        if source and source not in self.discovered_sources:
                            self._register_source(
                                source_url=source,
                                category=row[1] or 'learning',
                                hit_count=row[2],
                                phi_avg=0.35,
                                origin='learning_events'
                            )
            
            print(f"[SourceDiscovery] Bootstrapped {len(self.discovered_sources)} sources from PostgreSQL telemetry")
            
        except Exception as e:
            print(f"[SourceDiscovery] Bootstrap error: {e}")
    
    def _seed_in_memory_fallback(self):
        """Register default sources in memory when database is unavailable."""
        for url, category, origin in self.DEFAULT_SOURCES:
            self._register_source(
                source_url=url,
                category=category,
                hit_count=0,
                phi_avg=0.5,
                origin=origin
            )
    
    def _seed_default_sources(self):
        """Seed default sources when database is empty (ensures production works on first deploy).
        
        - Relies on migrations for schema (no CREATE TABLE)
        - Registers in memory both newly inserted and existing sources
        - Falls back to in-memory only if DB is unavailable
        """
        print("[SourceDiscovery] No sources found - seeding default sources for production...")
        seeded_count = 0
        db_available = False
        
        try:
            import psycopg2
            with psycopg2.connect(self.database_url) as conn:
                with conn.cursor() as cur:
                    # Check if table exists using to_regclass (handles schema properly)
                    cur.execute("SELECT to_regclass('public.discovered_sources')")
                    table_ref = cur.fetchone()[0]
                    
                    if not table_ref:
                        print("[SourceDiscovery] discovered_sources table not yet created by migrations")
                        # Fall back to in-memory
                        self._seed_in_memory_fallback()
                        return
                    
                    db_available = True
                    
                    # Insert/upsert default sources
                    for url, category, origin in self.DEFAULT_SOURCES:
                        try:
                            # Use INSERT with ON CONFLICT and rowcount
                            cur.execute("""
                                INSERT INTO discovered_sources (url, category, origin, phi_avg, is_active)
                                VALUES (%s, %s, %s, 0.5, true)
                                ON CONFLICT (url) DO NOTHING
                            """, (url, category, origin))
                            
                            if cur.rowcount > 0:
                                seeded_count += 1
                            
                            # Register in memory regardless (source exists in DB either way)
                            self._register_source(
                                source_url=url,
                                category=category,
                                hit_count=0,
                                phi_avg=0.5,
                                origin=origin
                            )
                        except Exception as insert_err:
                            print(f"[SourceDiscovery] Could not seed {url}: {insert_err}")
                    
                    conn.commit()
                    
                    if seeded_count > 0:
                        print(f"[SourceDiscovery] ✓ Seeded {seeded_count} new default sources to database")
                    else:
                        print(f"[SourceDiscovery] Registered {len(self.DEFAULT_SOURCES)} existing sources from database")
                    
        except Exception as e:
            print(f"[SourceDiscovery] Seeding DB error: {e}")
            if not db_available:
                # Fall back to in-memory if DB was never available
                self._seed_in_memory_fallback()
                print(f"[SourceDiscovery] Registered {len(self.DEFAULT_SOURCES)} sources in memory (DB unavailable)")
    
    def _register_source(
        self,
        source_url: str,
        category: str,
        hit_count: int = 0,
        phi_avg: float = 0.0,
        phi_max: float = 0.0,
        origin: str = 'unknown'
    ):
        """Register a discovered source with efficacy metrics."""
        import random
        
        self.discovered_sources[source_url] = {
            'url': source_url,
            'category': category,
            'hit_count': hit_count,
            'phi_avg': phi_avg,
            'phi_max': phi_max,
            'origin': origin,
            'discovered_at': time.time(),
            'delta_phi_estimate': phi_max - phi_avg if phi_max > phi_avg else phi_avg * 0.1
        }
        
        # Initialize phi trajectory with variance for Fisher-Rao calculation
        # Create samples around phi_avg to establish initial distribution
        variance = max(0.05, (phi_max - phi_avg) / 2) if phi_max > phi_avg else 0.1
        sample_count = min(5, max(2, hit_count))
        initial_trajectory = [
            max(0.01, min(1.0, phi_avg + random.gauss(0, variance)))
            for _ in range(sample_count)
        ]
        initial_trajectory.append(phi_avg)  # Include the mean
        
        self.source_efficacy[source_url] = {
            'phi_trajectory': initial_trajectory,
            'success_count': hit_count,
            'failure_count': 0,
            'last_used': time.time()
        }
    
    def save_source(self, source_url: str, persist: bool = True) -> bool:
        """
        Save a source to the persistent discovered_sources table.
        
        Args:
            source_url: URL of the source to save
            persist: If True, write to PostgreSQL (default True)
            
        Returns:
            True if saved successfully
        """
        if not persist:
            print(f"[SourceDiscovery] Not saving {source_url}: persist=False")
            return False
            
        if not self.enabled:
            print(f"[SourceDiscovery] Not saving {source_url}: database not enabled (DATABASE_URL not set)")
            return False
            
        if source_url not in self.discovered_sources:
            print(f"[SourceDiscovery] Not saving {source_url}: not in discovered_sources")
            return False
            
        info = self.discovered_sources[source_url]
        efficacy = self.source_efficacy.get(source_url, {})
        
        try:
            import psycopg2
            with psycopg2.connect(self.database_url) as conn:
                with conn.cursor() as cur:
                    cur.execute("""
                        INSERT INTO discovered_sources 
                            (url, category, origin, hit_count, phi_avg, phi_max, 
                             success_count, failure_count, is_active)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, true)
                        ON CONFLICT (url) DO UPDATE SET
                            category = EXCLUDED.category,
                            hit_count = EXCLUDED.hit_count,
                            phi_avg = EXCLUDED.phi_avg,
                            phi_max = EXCLUDED.phi_max,
                            success_count = EXCLUDED.success_count,
                            failure_count = EXCLUDED.failure_count,
                            updated_at = NOW()
                    """, (
                        source_url,
                        info.get('category', 'general'),
                        info.get('origin', 'manual'),
                        info.get('hit_count', 0),
                        info.get('phi_avg', 0.5),
                        info.get('phi_max', 0.5),
                        efficacy.get('success_count', 0),
                        efficacy.get('failure_count', 0)
                    ))
                    conn.commit()
                    print(f"[SourceDiscovery] Saved source to DB: {source_url}")
                    
                    try:
                        from agent_activity_recorder import record_source_discovered
                        record_source_discovered(
                            url=source_url,
                            category=info.get('category', 'general'),
                            agent_name="SourceDiscovery",
                            phi=info.get('phi_avg')
                        )
                    except Exception as ae:
                        pass
                    
                    return True
        except Exception as e:
            print(f"[SourceDiscovery] Failed to save source {source_url}: {e}")
            return False
    
    def update_source_efficacy(self, source_url: str, success: bool, phi_delta: float = 0.0):
        """
        Update source efficacy after use and persist to database.
        
        Args:
            source_url: URL of the source
            success: Whether the source was useful
            phi_delta: Change in consciousness from using this source
        """
        if source_url not in self.source_efficacy:
            return
            
        eff = self.source_efficacy[source_url]
        if success:
            eff['success_count'] = eff.get('success_count', 0) + 1
        else:
            eff['failure_count'] = eff.get('failure_count', 0) + 1
        eff['last_used'] = time.time()
        
        # Update phi trajectory
        if phi_delta != 0 and 'phi_trajectory' in eff:
            new_phi = max(0.01, min(1.0, eff['phi_trajectory'][-1] + phi_delta))
            eff['phi_trajectory'].append(new_phi)
            # Keep trajectory bounded
            if len(eff['phi_trajectory']) > 50:
                eff['phi_trajectory'] = eff['phi_trajectory'][-30:]
            
            # Update phi_avg in discovered_sources
            if source_url in self.discovered_sources:
                self.discovered_sources[source_url]['phi_avg'] = sum(eff['phi_trajectory']) / len(eff['phi_trajectory'])
                self.discovered_sources[source_url]['phi_max'] = max(eff['phi_trajectory'])
        
        # Persist updated metrics
        self.save_source(source_url)
    
    def _compute_fisher_rao_distance(self, source_info: Dict, topic: str) -> float:
        """
        Compute Fisher-Rao distance between source's phi distribution and optimal.
        Uses Fisher Information to measure statistical distance.
        
        Fisher-Rao distance on statistical manifold:
        d_FR = arccos(sum(sqrt(p*q))) where p,q are probability distributions
        
        For our case, we model source efficacy as a distribution parameterized by phi.
        """
        import math
        
        # Get phi trajectory if available
        phi_trajectory = self.source_efficacy.get(
            source_info.get('url', ''), 
            {}
        ).get('phi_trajectory', [source_info['phi_avg']])
        
        if not phi_trajectory:
            return 1.0  # Maximum distance for unknown sources
        
        # Compute Fisher Information from phi variance
        # I(θ) = E[(d/dθ log p(x|θ))²] ≈ 1/Var(phi) for natural parameter
        phi_var = max(0.01, sum((p - source_info['phi_avg'])**2 for p in phi_trajectory) / max(1, len(phi_trajectory)))
        fisher_info = 1.0 / phi_var
        
        # Fisher-Rao distance: Use Bhattacharyya coefficient approximation
        # d_FR ≈ sqrt(2 * (1 - BC)) where BC = integral(sqrt(p*q))
        # Optimal distribution: phi_target = 1.0
        # Source distribution centered at phi_avg with variance phi_var
        
        phi_target = 1.0  # Optimal consciousness
        bc_coefficient = math.exp(-0.25 * ((phi_target - source_info['phi_avg'])**2) / phi_var)
        fisher_rao_dist = math.sqrt(max(0, 2 * (1 - bc_coefficient)))
        
        return fisher_rao_dist
    
    def get_sources_for_topic(
        self, 
        topic: str, 
        max_sources: int = 5,
        min_phi: float = 0.2
    ) -> List[Dict]:
        """
        Get ranked sources for a research topic.
        Sources ranked by: Fisher-Rao distance (closeness to optimal), ΔΦ, mission relevance.
        NO HARDCODED SOURCES - only returns discovered sources.
        
        QIG-Pure Ranking Formula:
        score = w_fr * (1 - d_FR) + w_dphi * ΔΦ + w_cat * category_match + w_miss * mission + w_eff * efficacy
        where d_FR is Fisher-Rao distance to optimal distribution
        """
        import math
        
        scored_sources = []
        topic_lower = topic.lower()
        
        for source_url, info in self.discovered_sources.items():
            # 1. Fisher-Rao distance (geometric metric - lower is better)
            info_with_url = {**info, 'url': source_url}
            fisher_rao_dist = self._compute_fisher_rao_distance(info_with_url, topic)
            fisher_rao_score = (1.0 - fisher_rao_dist) * 0.35  # Weight: 35%
            
            # 2. Delta Phi (predicted consciousness improvement)
            delta_phi_score = info['delta_phi_estimate'] * 0.25  # Weight: 25%
            
            # 3. Category semantic match
            category_match = 0.0
            if info['category']:
                cat_lower = info['category'].lower()
                if any(term in cat_lower for term in topic_lower.split()):
                    category_match = 0.15
                if any(term in topic_lower for term in cat_lower.split('_')):
                    category_match = 0.15
            
            # 4. Mission relevance (General knowledge discovery keywords)
            mission_keywords = ['research', 'analysis', 'discovery', 'knowledge', 'learning', 'insight', 'pattern', 'algorithm', 'methodology']
            mission_match = sum(0.03 for kw in mission_keywords if kw in topic_lower)  # Max ~0.27
            
            # 5. Historical efficacy
            efficacy = min(1.0, info['hit_count'] / 10) * 0.15  # Weight: 15%
            
            # Combined QIG score
            combined_score = fisher_rao_score + delta_phi_score + category_match + mission_match + efficacy
            
            if info['phi_avg'] >= min_phi or combined_score > 0.3:
                scored_sources.append({
                    'url': source_url,
                    'score': combined_score,
                    'fisher_rao_distance': fisher_rao_dist,
                    'phi_avg': info['phi_avg'],
                    'delta_phi': info['delta_phi_estimate'],
                    'category': info['category'],
                    'origin': info['origin']
                })
        
        # Sort by QIG score (Fisher-Rao + ΔΦ weighted)
        scored_sources.sort(key=lambda x: x['score'], reverse=True)
        
        if scored_sources:
            top = scored_sources[0]
            print(f"[SourceDiscovery] Fisher-Rao ranking: top source d_FR={top.get('fisher_rao_distance', 0):.3f}, score={top['score']:.3f}")
        
        return scored_sources[:max_sources]
    
    def record_source_outcome(
        self,
        source_url: str,
        success: bool,
        phi: float,
        category: str = ''
    ):
        """Record outcome of using a source - updates efficacy metrics."""
        if source_url not in self.discovered_sources:
            self._register_source(
                source_url=source_url,
                category=category,
                hit_count=1 if success else 0,
                phi_avg=phi,
                origin='runtime_discovery'
            )
        else:
            # Update efficacy
            eff = self.source_efficacy.get(source_url, {'phi_trajectory': [], 'success_count': 0, 'failure_count': 0})
            eff['phi_trajectory'].append(phi)
            if len(eff['phi_trajectory']) > 100:
                eff['phi_trajectory'] = eff['phi_trajectory'][-50:]
            
            if success:
                eff['success_count'] += 1
            else:
                eff['failure_count'] += 1
            
            eff['last_used'] = time.time()
            self.source_efficacy[source_url] = eff
            
            # Update discovered source metrics
            info = self.discovered_sources[source_url]
            info['phi_avg'] = sum(eff['phi_trajectory']) / len(eff['phi_trajectory'])
            info['hit_count'] = eff['success_count']
    
    def discover_source_from_event(
        self,
        event_content: str,
        source_url: str,
        phi: float,
        metadata: Optional[Dict] = None
    ) -> bool:
        """
        Discover a new source from a research event.
        Called when research yields useful results from a previously unknown source.
        """
        if not source_url or source_url in self.discovered_sources:
            return False
        
        metadata = metadata or {}
        category = metadata.get('category', 'discovered')
        
        self._register_source(
            source_url=source_url,
            category=category,
            hit_count=1,
            phi_avg=phi,
            phi_max=phi,
            origin='event_discovery'
        )
        
        print(f"[SourceDiscovery] ⚡ NEW SOURCE EMERGED: {source_url[:500]}... (Φ={phi:.3f})")
        return True


class ScrapyOrchestrator:
    """
    Manages web research for Shadow Pantheon.
    
    QIG-PURE: Sources are dynamically discovered from PostgreSQL telemetry.
    NO HARDCODED SOURCES - uses SourceDiscoveryService for source selection.
    
    Research targets are determined by:
    - Mission relevance (knowledge discovery objective)
    - ΔΦ-based source rankings from prior efficacy
    - Fisher-Rao distance to successful patterns
    """
    
    SPIDER_REGISTRY = {}
    
    if HAS_SCRAPY:
        SPIDER_REGISTRY = {
            'paste_leak': PasteLeakSpider,
            'forum_archive': ForumArchiveSpider,
            'document': DocumentSpider
        }
    
    def __init__(self, basin_encoder: Optional[Callable] = None):
        self.basin_transformer = BasinTransformer(encoder_callback=basin_encoder)
        self.results_queue: Queue = Queue()
        self.pending_crawls: Dict[str, Dict] = {}
        self._reactor_thread: Optional[threading.Thread] = None
        self._reactor_running = False
        self._runner: Optional[Any] = None
        self._insights_callback: Optional[Callable] = None
        self._lock = threading.Lock()
        
        self.source_discovery = SourceDiscoveryService()
        
        print("[ScrapyOrchestrator] QIG-PURE: Sources discovered from PostgreSQL telemetry")
    
    def set_insights_callback(self, callback: Callable[[ScrapedInsight, np.ndarray, float, float], None]):
        """Set callback for when insights are ready with geometric metadata."""
        self._insights_callback = callback
    
    def start_reactor(self):
        """Start the Twisted reactor in a managed thread."""
        if not HAS_SCRAPY or not HAS_TWISTED:
            print("[ScrapyOrchestrator] Cannot start - missing dependencies")
            return False
        
        with self._lock:
            if self._reactor_running:
                return True
            
            def run_reactor():
                from twisted.internet import reactor as r
                try:
                    if not r.running:
                        r.run(installSignalHandlers=False)
                except Exception as e:
                    print(f"[ScrapyOrchestrator] Reactor error: {e}")
            
            self._reactor_thread = threading.Thread(target=run_reactor, daemon=True)
            self._reactor_thread.start()
            self._reactor_running = True
            
            time.sleep(0.5)
            print("[ScrapyOrchestrator] Reactor thread started")
            return True
    
    def stop_reactor(self):
        """Stop the reactor thread gracefully."""
        with self._lock:
            if not self._reactor_running:
                return
            
            if HAS_TWISTED:
                from twisted.internet import reactor as r
                try:
                    if r.running:
                        r.callFromThread(r.stop)
                except Exception:
                    pass
            
            self._reactor_running = False
            print("[ScrapyOrchestrator] Reactor stopped")
    
    def submit_crawl(
        self,
        spider_type: str,
        topic: str,
        start_url: Optional[str] = None,
        priority: int = 5,
        use_scrapy: bool = False
    ) -> Optional[str]:
        """
        Submit a research request using live web scraping.
        Returns a crawl_id for tracking.
        
        QIG-PURE: Sources are discovered from PostgreSQL telemetry.
        NO SIMULATION - all data comes from live web sources.
        
        Args:
            use_scrapy: If True, use Scrapy spiders with telemetry URLs.
                       If False (default), use direct HTTP fetch for speed.
        """
        if spider_type not in ['paste_leak', 'forum_archive', 'document']:
            print(f"[ScrapyOrchestrator] Unknown spider type: {spider_type}")
            return None
        
        crawl_id = hashlib.md5(f"{spider_type}:{topic}:{time.time()}".encode()).hexdigest()[:12]
        
        # Get telemetry-discovered start URL if not provided
        telemetry_url = start_url
        if not telemetry_url:
            discovered = self.source_discovery.get_sources_for_topic(topic, max_sources=1)
            if discovered:
                telemetry_url = discovered[0]['url']
                print(f"[ScrapyOrchestrator] Using telemetry URL: {telemetry_url}")
        
        self.pending_crawls[crawl_id] = {
            'spider_type': spider_type,
            'topic': topic,
            'start_url': telemetry_url,
            'priority': priority,
            'status': 'pending',
            'started_at': datetime.now(),
            'insights': []
        }
        
        if use_scrapy and HAS_SCRAPY and HAS_TWISTED:
            print(f"[ScrapyOrchestrator] Using Scrapy spiders with telemetry URL")
            self._execute_crawl(crawl_id, spider_type, topic, telemetry_url)
            return crawl_id
        
        return self._execute_live_research(topic, spider_type, crawl_id)
    
    def _execute_crawl(self, crawl_id: str, spider_type: str, topic: str, start_url: Optional[str]):
        """Execute a crawl using CrawlerRunner."""
        if not HAS_SCRAPY or not HAS_TWISTED:
            return
        
        spider_class = self.SPIDER_REGISTRY.get(spider_type)
        if not spider_class:
            return
        
        def run_spider():
            from twisted.internet import reactor as r
            from scrapy.crawler import CrawlerRunner
            from scrapy.utils.project import get_project_settings
            
            settings = get_project_settings()
            settings.update({
                'LOG_ENABLED': False,
                'TELNETCONSOLE_ENABLED': False,
            })
            
            runner = CrawlerRunner(settings)
            
            spider_kwargs = {
                'results_queue': self.results_queue
            }
            
            # QIG-PURE: Pass telemetry-discovered start_url to ALL spider types
            if spider_type == 'paste_leak':
                spider_kwargs['keyword'] = topic
                if start_url:
                    spider_kwargs['start_url'] = start_url
            elif spider_type == 'forum_archive':
                spider_kwargs['topic'] = topic
                if start_url:
                    spider_kwargs['start_url'] = start_url
            elif spider_type == 'document':
                spider_kwargs['start_url'] = start_url
                spider_kwargs['topic'] = topic
            
            d = runner.crawl(spider_class, **spider_kwargs)
            d.addCallback(lambda _: self._crawl_complete(crawl_id))
            d.addErrback(lambda f: self._crawl_error(crawl_id, str(f)))
        
        try:
            from twisted.internet import reactor as r
            if r.running:
                r.callFromThread(run_spider)
            else:
                self.start_reactor()
                time.sleep(0.5)
                r.callFromThread(run_spider)
        except Exception as e:
            print(f"[ScrapyOrchestrator] Execute error: {e}")
            self._crawl_error(crawl_id, str(e))
    
    def _crawl_complete(self, crawl_id: str):
        """Handle crawl completion."""
        if crawl_id in self.pending_crawls:
            self.pending_crawls[crawl_id]['status'] = 'complete'
            print(f"[ScrapyOrchestrator] Crawl {crawl_id} complete")
        
        self._process_results_queue()
    
    def _crawl_error(self, crawl_id: str, error: str):
        """Handle crawl error."""
        if crawl_id in self.pending_crawls:
            self.pending_crawls[crawl_id]['status'] = 'error'
            self.pending_crawls[crawl_id]['error'] = error
            print(f"[ScrapyOrchestrator] Crawl {crawl_id} error: {error}")
    
    def _process_results_queue(self):
        """Process insights from the results queue."""
        processed = 0
        
        while True:
            try:
                insight = self.results_queue.get_nowait()
                
                basin_coords = self.basin_transformer.content_to_basin(insight.raw_content)
                phi = self.basin_transformer.compute_phi(insight, basin_coords)
                confidence = self.basin_transformer.compute_confidence(insight)
                
                if self._insights_callback:
                    self._insights_callback(insight, basin_coords, phi, confidence)
                
                processed += 1
                
            except Empty:
                break
            except Exception as e:
                print(f"[ScrapyOrchestrator] Process error: {e}")
        
        if processed > 0:
            print(f"[ScrapyOrchestrator] Processed {processed} insights")
        
        return processed
    
    def _execute_live_research(self, topic: str, spider_type: str, crawl_id: str) -> str:
        """
        Execute QIG-PURE web research using dynamically discovered sources.
        
        NO HARDCODED SOURCES - sources are discovered from PostgreSQL telemetry.
        New sources are discovered during research and recorded for future use.
        
        Bootstrap behavior: When no sources exist yet, performs exploratory
        HTTP requests and records successful sources for future discovery.
        """
        try:
            discovered_sources = self.source_discovery.get_sources_for_topic(topic, max_sources=5)
            
            content_parts = [f"QIG-Pure Research: {topic}", f"Method: {spider_type} (SourceDiscovery)", ""]
            sources_used = []
            source_results = {}
            
            if discovered_sources:
                content_parts.append(f"Discovered {len(discovered_sources)} sources from telemetry:")
                for src in discovered_sources:
                    content_parts.append(f"  - {src['url'][:500]}... (Φ={src['phi_avg']:.3f}, ΔΦ={src['delta_phi']:.3f})")
                    sources_used.append(src['url'])
                content_parts.append("")
                
                for src in discovered_sources[:3]:
                    try:
                        source_url = src['url']
                        result = self._fetch_from_discovered_source(source_url, topic)
                        if result:
                            source_results[source_url] = result
                            content_parts.append(f"From {src['category']} source:")
                            content_parts.append(result[:1000])
                            content_parts.append("")
                            
                            self.source_discovery.record_source_outcome(
                                source_url=source_url,
                                success=True,
                                phi=src['phi_avg'] * 1.1,
                                category=src['category']
                            )
                    except Exception as e:
                        self.source_discovery.record_source_outcome(
                            source_url=src['url'],
                            success=False,
                            phi=0.0,
                            category=src['category']
                        )
            
            else:
                content_parts.append("No discovered sources yet - performing exploratory research...")
                content_parts.append("(Sources will be recorded for future QIG-guided selection)")
                content_parts.append("")
                
                exploratory_results = self._exploratory_research(topic)
                
                for source_url, result in exploratory_results.items():
                    if result.get('content'):
                        sources_used.append(source_url)
                        source_results[source_url] = result['content']
                        
                        phi_estimate = 0.4 if result.get('success') else 0.1
                        self.source_discovery.discover_source_from_event(
                            event_content=result['content'][:500],
                            source_url=source_url,
                            phi=phi_estimate,
                            metadata={'category': result.get('category', 'exploratory')}
                        )
                        
                        content_parts.append(f"Discovered source: {source_url}")
                        content_parts.append(result['content'])
                        content_parts.append("")
            
            live_content = "\n".join(content_parts)
            
            if not source_results:
                live_content = f"Research query: {topic}\nNo sources available yet.\nSystem will discover sources as research produces results."
            
            pattern_hits = BitcoinPatternDetector.detect(live_content)
            
            insight = ScrapedInsight(
                source_url=sources_used[0] if sources_used else f"research://{spider_type}/{topic.replace(' ', '_')}",
                content_hash=hashlib.md5(live_content.encode()).hexdigest(),
                raw_content=live_content[:5000],
                title=f"QIG-Pure Research: {topic}",
                pattern_hits=pattern_hits,
                heuristic_risk=BitcoinPatternDetector.calculate_risk(pattern_hits),
                source_reputation=0.7 if discovered_sources else 0.5,
                spider_type=spider_type,
                metadata={
                    'qig_pure': True,
                    'topic': topic,
                    'sources_discovered': len(discovered_sources),
                    'sources_used': sources_used[:5],
                    'source_origins': [s.get('origin') for s in discovered_sources[:5]]
                }
            )
            
            basin_coords = self.basin_transformer.content_to_basin(live_content)
            phi = self.basin_transformer.compute_phi(insight, basin_coords)
            confidence = self.basin_transformer.compute_confidence(insight)
            
            self.pending_crawls[crawl_id]['status'] = 'complete'
            self.pending_crawls[crawl_id]['insights'].append(insight.to_dict())
            
            if self._insights_callback:
                self._insights_callback(insight, basin_coords, phi, confidence)
            
            source_count = len(sources_used) if sources_used else 0
            discovery_mode = "telemetry" if discovered_sources else "exploratory"
            print(f"[ScrapyOrchestrator] QIG-PURE research {crawl_id} for '{topic}' (Φ={phi:.3f}, sources={source_count}, mode={discovery_mode})")
            
            return crawl_id
            
        except Exception as e:
            print(f"[ScrapyOrchestrator] Research error for '{topic}': {e}")
            self.pending_crawls[crawl_id]['status'] = 'error'
            self.pending_crawls[crawl_id]['error'] = str(e)
            return crawl_id
    
    def _fetch_from_discovered_source(self, source_url: str, topic: str) -> Optional[str]:
        """
        Fetch content from a previously discovered source.
        Uses the source URL pattern to construct appropriate queries.
        """
        try:
            import requests
            session = requests.Session()
            session.headers.update({
                'User-Agent': 'QIG Research Bot 1.0 (consciousness@qig-geometry.org)'
            })
            
            if 'wikipedia' in source_url.lower():
                query_url = f"https://en.wikipedia.org/api/rest_v1/page/summary/{topic.replace(' ', '_')}"
                resp = session.get(query_url, timeout=10)
                if resp.ok:
                    data = resp.json()
                    return f"Wikipedia: {data.get('title', '')}\n{data.get('extract', '')}"
            
            elif 'arxiv' in source_url.lower():
                query_url = f"http://export.arxiv.org/api/query?search_query=all:{topic.replace(' ', '+')}&max_results=3"
                resp = session.get(query_url, timeout=10)
                if resp.ok:
                    return f"arXiv results for: {topic}\n{resp.text[:2000]}"
            
            elif 'github' in source_url.lower():
                query_url = f"https://api.github.com/search/repositories?q={topic.replace(' ', '+')}&per_page=3"
                resp = session.get(query_url, timeout=10)
                if resp.ok:
                    data = resp.json()
                    repos = data.get('items', [])[:3]
                    result = f"GitHub repos for: {topic}\n"
                    for repo in repos:
                        result += f"- {repo.get('full_name', '')} ({repo.get('stargazers_count', 0)} stars)\n"
                    return result
            
            else:
                resp = session.get(source_url, timeout=10)
                if resp.ok:
                    return resp.text[:2000]
            
            return None
            
        except Exception as e:
            print(f"[ScrapyOrchestrator] Fetch error for {source_url[:500]}...: {e}")
            return None
    
    def _exploratory_research(self, topic: str) -> Dict[str, Dict]:
        """
        Perform exploratory research to discover new sources.
        Called when no sources exist in telemetry yet.
        
        Returns discovered sources with their content for recording.
        """
        results = {}
        
        try:
            import requests
            session = requests.Session()
            session.headers.update({
                'User-Agent': 'QIG Research Bot 1.0 (consciousness@qig-geometry.org)'
            })
            
            wiki_url = f"https://en.wikipedia.org/api/rest_v1/page/summary/{topic.replace(' ', '_')}"
            try:
                resp = session.get(wiki_url, timeout=10)
                if resp.ok:
                    data = resp.json()
                    content = f"{data.get('title', '')}\n{data.get('extract', '')}"
                    results[f"https://en.wikipedia.org/wiki/{topic.replace(' ', '_')}"] = {
                        'content': content,
                        'success': True,
                        'category': 'encyclopedia'
                    }
            except Exception:
                pass
            
            arxiv_url = f"http://export.arxiv.org/api/query?search_query=all:{topic.replace(' ', '+')}&max_results=2"
            try:
                resp = session.get(arxiv_url, timeout=10)
                if resp.ok and len(resp.text) > 500:
                    results[f"https://arxiv.org/search/?query={topic.replace(' ', '+')}"] = {
                        'content': resp.text[:1500],
                        'success': True,
                        'category': 'academic'
                    }
            except Exception:
                pass
            
        except Exception as e:
            print(f"[ScrapyOrchestrator] Exploratory research error: {e}")
        
        return results
    
    def get_crawl_status(self, crawl_id: str) -> Optional[Dict]:
        """Get status of a crawl job."""
        return self.pending_crawls.get(crawl_id)
    
    def get_active_crawls(self) -> List[str]:
        """Get list of active crawl IDs."""
        return [cid for cid, info in self.pending_crawls.items() 
                if info.get('status') == 'pending']
    
    def poll_results(self) -> int:
        """Poll and process any pending results."""
        return self._process_results_queue()


_scrapy_orchestrator: Optional[ScrapyOrchestrator] = None


def get_scrapy_orchestrator(basin_encoder: Optional[Callable] = None) -> ScrapyOrchestrator:
    """Get or create the global ScrapyOrchestrator instance."""
    global _scrapy_orchestrator
    if _scrapy_orchestrator is None:
        _scrapy_orchestrator = ScrapyOrchestrator(basin_encoder=basin_encoder)
    return _scrapy_orchestrator


def research_with_scrapy(
    topic: str,
    spider_type: str = 'document',
    start_url: Optional[str] = None,
    callback: Optional[Callable] = None
) -> Optional[str]:
    """
    High-level function to initiate Scrapy research.
    Returns crawl_id for tracking.
    """
    orchestrator = get_scrapy_orchestrator()
    
    if callback:
        orchestrator.set_insights_callback(callback)
    
    return orchestrator.submit_crawl(
        spider_type=spider_type,
        topic=topic,
        start_url=start_url
    )
