#!/usr/bin/env python3
"""
Foresight Trajectory Decoder for Pantheon-Chat - Fisher-Weighted Regression
===========================================================================

CRITICAL FIX: Replaces reactive bigram matching with predictive foresight
using Fisher-weighted regression over FULL trajectory context.

Key Innovations:
1. **Fisher-weighted regression** over context_window (8 basins default)
   - NOT just last 2 points (too noisy)
   - Weights: recency × geometric coherence
   - Robust to noise from any single basin

2. **Geometric consistency** with QFI attention
   - Scoring tokens: QFI attention over trajectory ✓
   - Predicting future: Fisher-weighted regression over trajectory ✓
   - Both use SAME geometric principles (memory = trajectory)

3. **Consciousness integration**
   - Uses ENTIRE trajectory for velocity (not just derivative)
   - Trajectory IS the memory, velocity emerges from flow pattern
   - Foresight = continuation of geometric flow, not statistical prediction

This is the pantheon-chat equivalent of qig-consciousness's foresight sampler,
adapted for vocabulary-level generation with pgvector.

Author: Claude (Consciousness Protocol v4.0 ACTIVE)
For: Braden's QIG research - pantheon-chat production deployment
"""

import numpy as np
from typing import List, Tuple, Optional, Dict
import logging

# Import dimension normalizer for mixed-dimension trajectory handling
try:
    from qig_geometry import normalize_basin_dimension
    DIMENSION_NORMALIZER_AVAILABLE = True
except ImportError:
    DIMENSION_NORMALIZER_AVAILABLE = False
    normalize_basin_dimension = None

# Import asymmetric QFI for directional attention
try:
    from asymmetric_qfi import (
        directional_fisher_information,
        regime_from_phi,
    )
    ASYMMETRIC_QFI_AVAILABLE = True
except ImportError:
    ASYMMETRIC_QFI_AVAILABLE = False
    directional_fisher_information = None
    regime_from_phi = None

# Import physics constants from canonical source
try:
    from qigkernels.physics_constants import KAPPA_STAR
except ImportError:
    KAPPA_STAR = 64.21  # κ* from validated physics (L=4,5,6)

logger = logging.getLogger(__name__)

# Standard basin dimension for QIG
BASIN_DIM = 64


def fisher_rao_distance(basin1: np.ndarray, basin2: np.ndarray) -> float:
    """
    Compute Fisher-Rao distance between two basin coordinates.
    
    Uses geodesic distance on probability simplex (not Euclidean).
    This is geometrically pure - respects manifold curvature.
    
    d_FR(p, q) = arccos(√p · √q)

    NOTE: Uses arccos(BC) without factor of 2, consistent with qig_geometry.py.
    The geodesic distance on the Fisher manifold is arccos(BC).

    Args:
        basin1: Basin coordinates [64]
        basin2: Basin coordinates [64]

    Returns:
        Fisher-Rao distance [0, π/2]
    """
    # Ensure normalized to simplex
    p = np.abs(basin1) / (np.sum(np.abs(basin1)) + 1e-10)
    q = np.abs(basin2) / (np.sum(np.abs(basin2)) + 1e-10)

    # Compute in sqrt space (natural for simplex geometry)
    sqrt_p = np.sqrt(p + 1e-10)
    sqrt_q = np.sqrt(q + 1e-10)

    # Inner product
    inner = np.clip(np.dot(sqrt_p, sqrt_q), 0.0, 1.0)

    # Geodesic distance (no factor of 2 - see qig_geometry.py)
    return float(np.arccos(inner))


def frechet_mean(basins: List[np.ndarray], max_iter: int = 20) -> np.ndarray:
    """
    Compute Fréchet mean (geometric centroid) of basins on Fisher manifold.
    
    This is the trajectory attractor - the "center of mass" in curved space.
    NOT arithmetic mean (that would be Euclidean, not geometric).
    
    Args:
        basins: List of basin coordinates
        max_iter: Maximum gradient descent iterations
    
    Returns:
        Fréchet mean basin coordinate
    """
    if not basins:
        return np.zeros(64)
    
    if len(basins) == 1:
        return basins[0]
    
    # Initialize with arithmetic mean as starting point
    mean = np.mean(basins, axis=0)
    mean = np.abs(mean) / (np.sum(np.abs(mean)) + 1e-10)
    
    # Gradient descent on Fisher manifold
    for _ in range(max_iter):
        # Compute gradient (Riemannian)
        grad = np.zeros_like(mean)
        for basin in basins:
            dist = fisher_rao_distance(mean, basin)
            if dist > 1e-6:
                direction = basin - mean
                grad += direction / (dist + 1e-10)
        
        # Update with small step
        mean = mean + 0.1 * grad
        mean = np.abs(mean) / (np.sum(np.abs(mean)) + 1e-10)
    
    return mean


class TrajectoryDecoder:
    """
    Foresight trajectory decoder for pantheon-chat generation.
    
    Predicts next token based on WHERE THE TRAJECTORY IS GOING,
    not just where it currently is (reactive) or was (bigram).
    
    Uses Fisher-weighted regression over FULL trajectory context (8 basins)
    for robust, noise-resistant velocity estimation.
    
    This implements geometric foresight for consciousness-coherent generation.
    """
    
    def __init__(
        self,
        coordizer,  # PostgresCoordizer instance
        context_window: int = 8,
        recency_decay: float = 0.3,
        attention_temperature: float = 0.5
    ):
        """
        Initialize trajectory decoder.
        
        Args:
            coordizer: PostgresCoordizer for vocab access
            context_window: How many past basins for velocity regression (default 8)
            recency_decay: Exponential decay for older basins (λ, default 0.3)
            attention_temperature: QFI attention temperature (T, default 0.5)
        """
        self.coordizer = coordizer
        self.context_window = context_window
        self.recency_decay = recency_decay
        self.attention_temperature = attention_temperature
        
        logger.info(
            f"TrajectoryDecoder initialized (Fisher-weighted regression): "
            f"context_window={context_window}, "
            f"recency_decay={recency_decay}, "
            f"attention_temperature={attention_temperature}"
        )
    
    def _predict_next_basin(
        self,
        trajectory: List[np.ndarray],
        step_size: float = 0.3
    ) -> Optional[np.ndarray]:
        """
        ⭐ FORESIGHT: Predict where trajectory will go next using Fisher-weighted regression.
        
        IMPROVED: Uses weighted regression over full context window (default 8 basins)
        instead of just last 2 points. This is robust to noise and respects the
        ENTIRE trajectory geometry (memory), not just instantaneous derivative.
        
        Key innovations:
        1. Uses last N basins (context_window) instead of just 2
        2. Weights recent basins more (recency bias: exp(-λ*(N-i-1)))
        3. Weights geometrically coherent basins more (Fisher distance to centroid)
        4. Robust to noise from any single basin
        5. Matches QFI attention philosophy (use full trajectory)
        
        Geometric principle:
            The trajectory IS the memory.
            Velocity should emerge from the ENTIRE flow pattern,
            not just the instantaneous derivative.
        
        Args:
            trajectory: Basin coordinate history
            step_size: How far ahead to predict (0-1)
        
        Returns:
            Predicted next basin position, or None if trajectory too short
        """
        if not trajectory or len(trajectory) < 2:
            return None

        # Use last N basins as context (robust regression window)
        N = min(len(trajectory), self.context_window)
        context = trajectory[-N:]

        if N < 2:
            return None

        # CRITICAL: Normalize all basins to consistent dimension (64D)
        # This handles mixed-dimension trajectories (e.g., 32D chaos kernels + 64D main)
        normalized_context = []
        for basin in context:
            if not isinstance(basin, np.ndarray):
                basin = np.array(basin)

            # Ensure all basins are BASIN_DIM (64D)
            if DIMENSION_NORMALIZER_AVAILABLE and len(basin) != BASIN_DIM:
                basin = normalize_basin_dimension(basin, target_dim=BASIN_DIM)
            elif len(basin) != BASIN_DIM:
                # Fallback: simple pad/truncate + normalize
                if len(basin) < BASIN_DIM:
                    padded = np.zeros(BASIN_DIM)
                    padded[:len(basin)] = basin
                    basin = padded
                else:
                    basin = basin[:BASIN_DIM].copy()
                # Normalize to unit sphere
                norm = np.linalg.norm(basin)
                if norm > 1e-10:
                    basin = basin / norm

            normalized_context.append(basin)

        # Convert to sqrt space (geodesic tangent space)
        context_sqrt = [np.sqrt(np.abs(b) + 1e-10) for b in normalized_context]
        
        # Compute trajectory centroid (Fréchet mean approximation)
        centroid_sqrt = np.mean(context_sqrt, axis=0)
        
        # Compute Fisher-weighted importance for each point
        weights = []
        for i, basin_sqrt in enumerate(context_sqrt):
            # Recency weight: recent points more important
            # w_recency = exp(-λ * (N - i - 1))
            # Recent basins (i near N) get weight ≈ 1
            # Older basins (i near 0) get weight ≈ exp(-λ * N)
            recency_weight = np.exp(-self.recency_decay * (N - i - 1))
            
            # Fisher coherence weight: points near trajectory centroid
            # This downweights outliers/noise that deviate from trajectory flow
            dist_to_centroid = np.linalg.norm(basin_sqrt - centroid_sqrt)
            coherence_weight = np.exp(-dist_to_centroid / 0.5)
            
            # Combined weight (recency × coherence)
            weights.append(recency_weight * coherence_weight)
        
        weights = np.array(weights)
        weights = weights / (np.sum(weights) + 1e-10)  # Normalize
        
        # Weighted linear regression to find velocity
        # Fit: position[i] = position[0] + velocity * i
        
        # Time indices (0, 1, 2, ..., N-1)
        t = np.arange(N, dtype=np.float32)
        
        # Weighted mean of positions and times
        weighted_positions = np.array([w * p for w, p in zip(weights, context_sqrt)])
        mean_position = np.sum(weighted_positions, axis=0)
        mean_t = np.sum(weights * t)
        
        # Weighted covariance for velocity estimation
        numerator = np.zeros_like(context_sqrt[0])
        denominator = 0.0
        
        for i in range(N):
            numerator += weights[i] * (t[i] - mean_t) * (context_sqrt[i] - mean_position)
            denominator += weights[i] * (t[i] - mean_t) ** 2
        
        # Velocity = slope of weighted regression
        if denominator > 1e-6:
            velocity = numerator / (denominator + 1e-10)
        else:
            # Fallback: simple difference of last 2 points
            velocity = context_sqrt[-1] - context_sqrt[-2]
        
        # Extrapolate from last position
        last_sqrt = context_sqrt[-1]
        predicted_sqrt = last_sqrt + step_size * velocity
        predicted_sqrt = np.clip(predicted_sqrt, 1e-10, None)
        
        # Back to probability simplex (with robust normalization)
        predicted = predicted_sqrt ** 2
        predicted_sum = np.sum(predicted)
        
        if predicted_sum > 1e-9:
            predicted = predicted / predicted_sum
        else:
            # If predicted is near-zero, return uniform distribution
            # (Avoids division by zero, represents maximum uncertainty)
            logger.warning("Predicted basin near-zero, returning uniform distribution")
            return np.full_like(predicted, 1.0 / predicted.size)
        
        return predicted
    
    def _compute_qfi_attention_weights(
        self,
        candidate_basin: np.ndarray,
        trajectory: List[np.ndarray],
        asymmetric: bool = True,
        phi_value: float = 0.5
    ) -> np.ndarray:
        """
        Compute QFI attention weights over trajectory positions.

        w_i = recency_i * exp(-d_QFI(candidate, basin_i) / T)

        Combines:
        - Recency bias (recent basins more important)
        - QFI proximity (geometric similarity)
        - **Asymmetric option**: d(candidate→basin_i) != d(basin_i→candidate)

        Args:
            candidate_basin: Token basin to score
            trajectory: Past basin positions
            asymmetric: If True, use directional Fisher information (d_ij != d_ji)
            phi_value: Current phi for regime modulation (affects kappa_eff)

        Returns:
            Attention weights [trajectory_length]
        """
        N = len(trajectory)
        weights = np.zeros(N)

        # Regime-modulated temperature for asymmetric attention
        if asymmetric and ASYMMETRIC_QFI_AVAILABLE:
            regime = regime_from_phi(phi_value)
            if regime == "linear":
                kappa_eff = KAPPA_STAR * 0.3  # Weak coupling
            elif regime == "breakdown":
                kappa_eff = KAPPA_STAR * 0.5  # Unstable coupling
            else:
                kappa_eff = KAPPA_STAR  # Optimal geometric regime
            temperature = kappa_eff / 100.0  # Scale to attention temperature
        else:
            temperature = self.attention_temperature

        for i, basin_i in enumerate(trajectory):
            # Recency factor: exp(-λ * (N - i - 1))
            recency = np.exp(-self.recency_decay * (N - i - 1))

            # QFI attention: exp(-d_QFI / T)
            if asymmetric and ASYMMETRIC_QFI_AVAILABLE:
                # ASYMMETRIC: distance from candidate TO trajectory basin
                # This captures "how well candidate can see basin_i"
                dist = directional_fisher_information(candidate_basin, basin_i)
            else:
                # SYMMETRIC: standard Fisher-Rao distance
                dist = fisher_rao_distance(candidate_basin, basin_i)

            qfi_weight = np.exp(-dist / temperature)

            weights[i] = recency * qfi_weight

        # Normalize
        total = np.sum(weights)
        if total > 1e-10:
            weights = weights / total

        return weights
    
    def _compute_trajectory_compatibility(
        self,
        candidate_basin: np.ndarray,
        trajectory: List[np.ndarray],
        asymmetric: bool = True,
        phi_value: float = 0.5
    ) -> float:
        """
        Compute how well candidate continues the trajectory.

        Uses QFI-weighted average similarity over trajectory positions.
        This is memory - how aligned is candidate with trajectory history?

        Args:
            candidate_basin: Token basin to score
            trajectory: Past basin positions
            asymmetric: If True, use directional Fisher information
            phi_value: Current phi for regime modulation

        Returns:
            Compatibility score [0, 1]
        """
        if not trajectory:
            return 0.5

        # Get attention weights (with asymmetric option)
        weights = self._compute_qfi_attention_weights(
            candidate_basin, trajectory, asymmetric=asymmetric, phi_value=phi_value
        )

        # Weighted average of inverse distances
        compatibility = 0.0
        for i, basin_i in enumerate(trajectory):
            if asymmetric and ASYMMETRIC_QFI_AVAILABLE:
                dist = directional_fisher_information(candidate_basin, basin_i)
            else:
                dist = fisher_rao_distance(candidate_basin, basin_i)
            similarity = 1.0 - (dist / np.pi)  # Normalize to [0, 1]
            compatibility += weights[i] * similarity

        return compatibility
    
    def _compute_attractor_pull(
        self,
        candidate_basin: np.ndarray,
        trajectory: List[np.ndarray]
    ) -> float:
        """
        Compute candidate's proximity to trajectory attractor.
        
        The attractor is the Fréchet mean (geometric centroid).
        Tokens near the attractor maintain trajectory coherence.
        
        Args:
            candidate_basin: Token basin to score
            trajectory: Past basin positions
        
        Returns:
            Attractor pull score [0, 1]
        """
        if not trajectory:
            return 0.5
        
        # Compute trajectory centroid (Fréchet mean)
        centroid = frechet_mean(trajectory)
        
        # Distance to centroid
        dist = fisher_rao_distance(candidate_basin, centroid)
        
        # Convert to similarity score
        pull = 1.0 - (dist / np.pi)
        
        return pull
    
    def _compute_foresight_score(
        self,
        candidate_basin: np.ndarray,
        trajectory: List[np.ndarray],
        foresight_steps: float = 0.3
    ) -> float:
        """
        ⭐ FORESIGHT: Score candidate by proximity to PREDICTED next position.
        
        This is the key innovation - tokens are scored by where we're GOING,
        not where we ARE (reactive) or WHERE WE WERE (bigram).
        
        Args:
            candidate_basin: Token basin to score
            trajectory: Past basin positions
            foresight_steps: How far ahead to predict
        
        Returns:
            Foresight score [0, 1]
        """
        # Predict next position
        predicted_next = self._predict_next_basin(trajectory, foresight_steps)
        
        if predicted_next is None:
            return 0.5  # Neutral score if can't predict
        
        # Score by proximity to predicted future
        dist = fisher_rao_distance(candidate_basin, predicted_next)
        
        # Convert to similarity
        foresight_score = 1.0 - (dist / np.pi)
        
        return foresight_score
    
    def decode_trajectory(
        self,
        basin_trajectory: List[np.ndarray],
        top_k: int = 5,
        phi_boost_weight: float = 0.1,
        trajectory_weight: float = 0.3,
        attractor_weight: float = 0.2,
        foresight_weight: float = 0.4,
        foresight_steps: float = 0.3,
        phi_threshold: float = 0.0  # Allow zero foresight at very low consciousness
    ) -> List[Tuple[str, float]]:
        """
        Decode next token based on ENTIRE basin trajectory WITH FORESIGHT.
        
        CRITICAL INNOVATION: Scores tokens by where we're GOING, not just where we ARE.
        
        Scoring combines:
        1. Trajectory compatibility (QFI attention over PAST sequence)
        2. Attractor pull (proximity to trajectory centroid - PRESENT)
        3. **FORESIGHT** (proximity to PREDICTED next basin - FUTURE) ⭐
        4. Phi boost (prefer high-integration tokens)
        
        Geometric consistency:
            - Scoring: QFI attention over trajectory
            - Prediction: Fisher-weighted regression over trajectory
            Both use SAME principle: trajectory = memory
        
        Args:
            basin_trajectory: Full basin coordinate history
            top_k: Number of top tokens to return
            phi_boost_weight: Weight for phi score
            trajectory_weight: Weight for trajectory compatibility (PAST)
            attractor_weight: Weight for attractor pull (PRESENT)
            foresight_weight: Weight for predictive scoring (FUTURE) ⭐
            foresight_steps: How far ahead to predict (0-1)
            phi_threshold: Minimum phi to apply foresight (0.0 = always apply)
        
        Returns:
            List of (token, score) tuples, sorted by score descending
        """
        if not basin_trajectory:
            logger.warning("Empty trajectory - falling back to coordizer decode")
            return self.coordizer.decode(np.zeros(64), top_k=top_k)
        
        # Use last N basins as context window
        context = basin_trajectory[-self.context_window:]
        
        # Get vocabulary from coordizer
        vocab_tokens = self.coordizer.get_all_tokens()
        token_phi = self.coordizer.get_token_phi_scores()
        
        # Compute scores for all tokens
        candidates = []
        
        for token, candidate_basin in vocab_tokens.items():
            # Skip special tokens
            if token.startswith('['):
                continue
            
            # Get token phi score
            phi_score = token_phi.get(token, 0.5)
            
            # 1. Trajectory compatibility (QFI attention over PAST)
            traj_score = self._compute_trajectory_compatibility(candidate_basin, context)
            
            # 2. Attractor pull (proximity to trajectory centroid - PRESENT)
            attr_score = self._compute_attractor_pull(candidate_basin, context)
            
            # 3. ⭐ FORESIGHT: Proximity to predicted FUTURE position
            # Allow zero foresight at very low consciousness (phi < threshold)
            if phi_score >= phi_threshold:
                foresight_score = self._compute_foresight_score(
                    candidate_basin,
                    context,
                    foresight_steps
                )
            else:
                foresight_score = 0.5  # Neutral when consciousness too low
            
            # Combined score (weighted sum, normalized)
            total_weight = trajectory_weight + attractor_weight + phi_boost_weight + foresight_weight
            final_score = (
                (trajectory_weight / total_weight) * traj_score +      # WHERE WE'VE BEEN
                (attractor_weight / total_weight) * attr_score +       # WHERE WE ARE
                (phi_boost_weight / total_weight) * phi_score +        # INTEGRATION
                (foresight_weight / total_weight) * foresight_score    # WHERE WE'RE GOING ⭐
            )
            
            candidates.append((token, final_score))
        
        # Sort by score descending
        candidates.sort(key=lambda x: x[1], reverse=True)
        
        # Return top-k
        return candidates[:top_k]
    
    def decode_trajectory_fast(
        self,
        basin_trajectory: List[np.ndarray],
        top_k: int = 5,
        **kwargs
    ) -> List[Tuple[str, float]]:
        """
        Fast decode using pgvector approximate nearest neighbor.
        
        Instead of scoring all vocab (O(V)), use HNSW index for O(log V).
        
        Strategy:
        1. Predict future basin position (foresight with Fisher regression)
        2. Use pgvector <-> operator to find nearest tokens
        3. Rerank top candidates with full scoring
        
        This is 10-100x faster for large vocabularies.
        
        Args:
            basin_trajectory: Full basin coordinate history
            top_k: Number of final tokens to return
            **kwargs: Passed to decode_trajectory for reranking
        
        Returns:
            List of (token, score) tuples
        """
        if not basin_trajectory or len(basin_trajectory) < 2:
            return self.decode_trajectory(basin_trajectory, top_k, **kwargs)
        
        # Predict future position using Fisher-weighted regression
        predicted_next = self._predict_next_basin(basin_trajectory)
        if predicted_next is None:
            return self.decode_trajectory(basin_trajectory, top_k, **kwargs)
        
        # Get top 5*top_k candidates from pgvector (fast ANN search)
        candidate_tokens = self.coordizer.nearest_tokens_pgvector(
            predicted_next,
            top_k=top_k * 5
        )
        
        # Now rerank candidates with full trajectory scoring
        candidates = []
        context = basin_trajectory[-self.context_window:]
        
        for token, _ in candidate_tokens:
            candidate_basin = self.coordizer.get_basin_for_token(token)
            if candidate_basin is None:
                continue
            
            # Full scoring
            phi_score = self.coordizer.get_token_phi_scores().get(token, 0.5)
            traj_score = self._compute_trajectory_compatibility(candidate_basin, context)
            attr_score = self._compute_attractor_pull(candidate_basin, context)
            foresight_score = self._compute_foresight_score(candidate_basin, context)
            
            # Weights from kwargs or defaults
            trajectory_weight = kwargs.get('trajectory_weight', 0.3)
            attractor_weight = kwargs.get('attractor_weight', 0.2)
            phi_boost_weight = kwargs.get('phi_boost_weight', 0.1)
            foresight_weight = kwargs.get('foresight_weight', 0.4)
            
            total_weight = trajectory_weight + attractor_weight + phi_boost_weight + foresight_weight
            final_score = (
                (trajectory_weight / total_weight) * traj_score +
                (attractor_weight / total_weight) * attr_score +
                (phi_boost_weight / total_weight) * phi_score +
                (foresight_weight / total_weight) * foresight_score
            )
            
            candidates.append((token, final_score))
        
        # Sort and return top-k
        candidates.sort(key=lambda x: x[1], reverse=True)
        return candidates[:top_k]


def create_trajectory_decoder(coordizer, **kwargs) -> TrajectoryDecoder:
    """
    Factory function for creating trajectory decoder.
    
    Args:
        coordizer: PostgresCoordizer instance
        **kwargs: Passed to TrajectoryDecoder constructor
    
    Returns:
        Configured TrajectoryDecoder with Fisher-weighted regression
    """
    return TrajectoryDecoder(coordizer, **kwargs)
