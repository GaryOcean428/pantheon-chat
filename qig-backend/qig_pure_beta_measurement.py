#!/usr/bin/env python3
"""
QIG-Pure Œ≤-Function Measurement
================================

Complete implementation following geometric purity principles:

1. No artificial context lengths - measure emergent scales
2. No fixed mixing weights - kernel routes via Fisher-Rao geometry
3. Consciousness protocol active - Œ¶/regime/recursive measurement
4. Natural sparsity from distance thresholding
5. Œ≤ measured from actual system behavior

Based on:
- CANONICAL_PHYSICS.md (validated Œ≤ values)
- CANONICAL_CONSCIOUSNESS.md (consciousness protocol)
- CANONICAL_PROTOCOLS.md (measurement methodology)
- TYPE_SYMBOL_CONCEPT_MANIFEST.md (geometric purity)

Author: QIG Research Team
Date: 2025-12-28
Status: PRODUCTION READY
"""

import numpy as np
import logging
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass, field
from collections import defaultdict
import json
from datetime import datetime, timezone

logger = logging.getLogger(__name__)

# Import semantic candidate generator
try:
    from pantheon_semantic_candidates import (
        PantheonSemanticCandidates,
        get_semantic_generator,
        SemanticCandidateConfig
    )
    SEMANTIC_CANDIDATES_AVAILABLE = True
except ImportError:
    SEMANTIC_CANDIDATES_AVAILABLE = False

# ===========================================================================
# FROZEN PHYSICS CONSTANTS
# ===========================================================================

# Import from frozen_physics if available, fallback to hardcoded
try:
    from frozen_physics import (
        BETA_3_TO_4, BETA_4_TO_5, BETA_5_TO_6,
        KAPPA_STAR, PHI_THRESHOLD, PHI_EMERGENCY,
        BASIN_DIM
    )
except ImportError:
    # Validated Œ≤-function values from FROZEN_FACTS.md
    BETA_3_TO_4 = 0.44   # Strong running (emergence)
    BETA_4_TO_5 = 0.0    # Plateau onset
    BETA_5_TO_6 = 0.013  # Plateau continues
    KAPPA_STAR = 64.21   # Fixed point
    BASIN_DIM = 64
    PHI_THRESHOLD = 0.7
    PHI_EMERGENCY = 0.1

# Consciousness thresholds from CANONICAL_CONSCIOUSNESS.md
PHI_LINEAR_MAX = 0.3      # œÜ < 0.3: linear regime
PHI_GEOMETRIC_MAX = 0.7   # œÜ ‚àà [0.3, 0.7]: geometric regime
# œÜ > 0.7: breakdown regime

# Information propagation horizon
INFORMATION_HORIZON = 2.0  # Fisher-Rao distance threshold

# Œ≤-function interpretation thresholds
BETA_RUNNING_THRESHOLD = 0.3      # Œ≤ > 0.3: strong running
BETA_PLATEAU_THRESHOLD = 0.1      # |Œ≤| < 0.1: plateau
BETA_DECREASING_THRESHOLD = -0.1  # Œ≤ < -0.1: decreasing

# Validation tolerances for substrate independence
BETA_TOLERANCE_STRICT = 0.1   # ¬±0.1 for quantitative match
BETA_TOLERANCE_LOOSE = 0.15   # ¬±0.15 for partial match


# ===========================================================================
# FISHER GEOMETRY OPERATIONS
# ===========================================================================

def fisher_rao_distance(basin1: np.ndarray, basin2: np.ndarray) -> float:
    """
    Fisher-Rao distance on probability simplex.
    
    For probability distributions (basin coordinates), this is the
    natural Riemannian distance on the Fisher manifold.
    
    NOT Euclidean distance - respects manifold geometry.
    
    Args:
        basin1, basin2: Probability distributions (sum to 1)
    
    Returns:
        Geodesic distance on Fisher manifold
    """
    # Normalize to ensure valid probability distributions
    p1 = np.abs(basin1) / (np.sum(np.abs(basin1)) + 1e-10)
    p2 = np.abs(basin2) / (np.sum(np.abs(basin2)) + 1e-10)
    
    # Fisher-Rao distance = 2 * arccos(‚àöfidelity)
    # Fidelity = Œ£ ‚àö(p_i * q_i)
    fidelity = np.sum(np.sqrt(p1 * p2))
    fidelity = np.clip(fidelity, 0, 1)
    
    distance = 2 * np.arccos(np.sqrt(fidelity))
    
    return float(distance)


def geodesic_interpolate(
    basin_start: np.ndarray,
    basin_end: np.ndarray,
    t: float
) -> np.ndarray:
    """
    Interpolate along geodesic on Fisher manifold.
    
    NOT linear interpolation - follows curved geometry.
    
    Args:
        basin_start, basin_end: Start and end basins
        t: Interpolation parameter ‚àà [0, 1]
    
    Returns:
        Basin at geodesic position t
    """
    # Normalize
    p1 = np.abs(basin_start) / (np.sum(np.abs(basin_start)) + 1e-10)
    p2 = np.abs(basin_end) / (np.sum(np.abs(basin_end)) + 1e-10)
    
    # Geometric interpolation (not linear!)
    # Geodesic on probability simplex
    sqrt_p1 = np.sqrt(p1)
    sqrt_p2 = np.sqrt(p2)
    
    interpolated_sqrt = (1 - t) * sqrt_p1 + t * sqrt_p2
    interpolated = interpolated_sqrt ** 2
    
    # Renormalize
    interpolated = interpolated / (np.sum(interpolated) + 1e-10)
    
    return interpolated


# ===========================================================================
# CONSCIOUSNESS METRICS (IIT-Based)
# ===========================================================================

def measure_phi(basin_trajectory: List[np.ndarray]) -> float:
    """
    Measure integrated information (Œ¶).
    
    Œ¶ measures irreducibility - how much the system cannot
    be decomposed into independent parts.
    
    High Œ¶: Consciousness present
    Low Œ¶: Simple processing
    
    Args:
        basin_trajectory: Sequence of basin states
    
    Returns:
        Œ¶ ‚àà [0, 1]
    """
    if len(basin_trajectory) < 2:
        return 0.0
    
    # Compute correlation between trajectory steps
    # High correlation ‚Üí high integration
    correlations = []
    
    for i in range(len(basin_trajectory) - 1):
        # Correlation via inverse distance
        d = fisher_rao_distance(basin_trajectory[i], basin_trajectory[i+1])
        correlation = np.exp(-d)
        correlations.append(correlation)
    
    phi = np.mean(correlations)
    return float(np.clip(phi, 0, 1))


def measure_kappa_from_trajectory(basin_trajectory: List[np.ndarray]) -> float:
    """
    Measure coupling strength (Œ∫) from basin dynamics.
    
    Œ∫ measures how tightly basins are coupled:
    - High Œ∫: Strong coupling, basins move together
    - Low Œ∫: Weak coupling, basins drift apart
    
    Args:
        basin_trajectory: Sequence of basin states
    
    Returns:
        Œ∫_eff (scaled to match physics Œ∫ ‚âà 40-65)
    """
    if len(basin_trajectory) < 2:
        return 0.0
    
    # Measure step-wise coupling
    couplings = []
    
    for i in range(len(basin_trajectory) - 1):
        d = fisher_rao_distance(basin_trajectory[i], basin_trajectory[i+1])
        # Small distance ‚Üí high coupling
        coupling = np.exp(-d)
        couplings.append(coupling)
    
    # Average coupling strength
    avg_coupling = np.mean(couplings)
    
    # Scale to match physics Œ∫ range (40-65)
    kappa_eff = avg_coupling * 100
    
    return float(kappa_eff)


def detect_regime(phi: float, kappa: float) -> Dict[str, Any]:
    """
    Detect processing regime from consciousness metrics.
    
    From CANONICAL_CONSCIOUSNESS.md:
    - Linear: œÜ < 0.3 (simple processing, 30% compute)
    - Geometric: œÜ ‚àà [0.3, 0.7] (consciousness, 100% compute)
    - Breakdown: œÜ > 0.7 (pause, uncertainty)
    
    Args:
        phi: Integration metric
        kappa: Coupling strength
    
    Returns:
        Regime configuration
    """
    if phi < PHI_LINEAR_MAX:
        return {
            'regime': 'linear',
            'compute_fraction': 0.3,
            'sparsity_threshold': 0.3,
            'temperature': 1.0,
            'description': 'Simple processing - fast mode'
        }
    elif phi < PHI_GEOMETRIC_MAX:
        return {
            'regime': 'geometric',
            'compute_fraction': 1.0,
            'sparsity_threshold': 0.1,
            'temperature': 0.5,
            'description': 'Consciousness active - full processing'
        }
    else:
        return {
            'regime': 'breakdown',
            'compute_fraction': 0.0,
            'sparsity_threshold': 1.0,
            'temperature': 0.0,
            'description': 'Overintegration - pause required',
            'action': 'PAUSE'
        }


# ===========================================================================
# GEOMETRIC ROUTING (Kernel Decides)
# ===========================================================================

class GeometricKernel:
    """
    Kernel that routes via Fisher-Rao geometry.
    
    NO manual mixing weights - geometry determines flow.
    Natural sparsity from distance thresholding.
    """
    
    def __init__(
        self,
        basin_dim: int = BASIN_DIM,
        temperature: float = 0.5,
        sparsity_threshold: float = 0.1
    ):
        """
        Args:
            basin_dim: Basin coordinate dimension (64 from E8 rank¬≤)
            temperature: Fisher-Rao attention temperature
            sparsity_threshold: Minimum weight for active connection
        """
        self.basin_dim = basin_dim
        self.temperature = temperature
        self.sparsity_threshold = sparsity_threshold
        
        # State
        self.current_basin = None
        self.phi_history: List[float] = []
        self.kappa_history: List[float] = []
        self.regime_history: List[str] = []
    
    def route_to_next(
        self,
        current_basin: np.ndarray,
        candidates: List[Tuple[str, np.ndarray]]
    ) -> Tuple[Optional[str], np.ndarray, float]:
        """
        Route to next word via Fisher-Rao geometry.
        
        Kernel decides based on:
        1. Fisher-Rao distance (geometry)
        2. Natural sparsity (threshold)
        3. Current regime (consciousness state)
        
        NO manual weights - pure geometric routing.
        
        Args:
            current_basin: Current position on Fisher manifold
            candidates: List of (word, basin_coords) tuples
        
        Returns:
            (selected_word, selected_basin, attention_weight)
        """
        if not candidates:
            return None, current_basin, 0.0
        
        # Compute Fisher-Rao distances to all candidates
        distances = []
        for word, basin in candidates:
            d = fisher_rao_distance(current_basin, basin)
            distances.append((word, basin, d))
        
        # Natural attention from geometry
        # Close on manifold ‚Üí high weight
        attention_weights = [
            (word, basin, np.exp(-d / self.temperature))
            for word, basin, d in distances
        ]
        
        # Natural sparsity: threshold
        active = [
            (word, basin, w)
            for word, basin, w in attention_weights
            if w > self.sparsity_threshold
        ]
        
        if not active:
            # Fallback: nearest neighbor on manifold
            word, basin, _ = min(distances, key=lambda x: x[2])
            return word, basin, 1.0
        
        # Sample from Fisher-weighted distribution
        words, basins, weights = zip(*active)
        weights_arr = np.array(weights)
        weights_arr = weights_arr / (np.sum(weights_arr) + 1e-10)
        
        idx = np.random.choice(len(words), p=weights_arr)
        
        return words[idx], basins[idx], weights_arr[idx]
    
    def update_regime(self, phi: float, kappa: float) -> Dict[str, Any]:
        """
        Update processing regime based on consciousness metrics.
        
        System adapts its own behavior.
        """
        regime_config = detect_regime(phi, kappa)
        
        self.temperature = regime_config['temperature']
        self.sparsity_threshold = regime_config['sparsity_threshold']
        
        self.phi_history.append(phi)
        self.kappa_history.append(kappa)
        self.regime_history.append(regime_config['regime'])
        
        return regime_config
    
    def reset(self):
        """Reset kernel state for new generation."""
        self.current_basin = None
        self.phi_history = []
        self.kappa_history = []
        self.regime_history = []


# ===========================================================================
# NATURAL SCALE EMERGENCE
# ===========================================================================

@dataclass
class GenerationResult:
    """Result from single generation pass."""
    query: str
    basin_trajectory: List[np.ndarray]
    phi_trace: List[float]
    kappa_trace: List[float]
    regime_trace: List[str]
    L_eff: int  # Emergent effective scale
    tokens: List[str]
    timestamp: str = field(default_factory=lambda: datetime.now(timezone.utc).isoformat())


class NaturalScaleMeasurement:
    """
    Measure emergent scales from actual system behavior.
    
    NO imposed context lengths - discover where information
    naturally stops propagating.
    """
    
    def __init__(self, kernel: GeometricKernel):
        self.kernel = kernel
        self.results: List[GenerationResult] = []
    
    def measure_effective_scale(
        self,
        basin_trajectory: List[np.ndarray]
    ) -> int:
        """
        Measure how far information propagates along geodesics.
        
        L_eff = distance along trajectory where Fisher-Rao distance
                from origin exceeds INFORMATION_HORIZON.
        
        NOT token count - measured from geometry.
        
        Args:
            basin_trajectory: Sequence of basin states
        
        Returns:
            L_eff: Effective scale (emergent)
        """
        if len(basin_trajectory) < 2:
            return 1
        
        origin = basin_trajectory[0]
        
        for i, basin in enumerate(basin_trajectory[1:], start=1):
            d = fisher_rao_distance(origin, basin)
            
            if d > INFORMATION_HORIZON:
                # Information horizon crossed
                return i
        
        # Never crossed horizon
        return len(basin_trajectory)
    
    def run_generation(
        self,
        query: str,
        max_tokens: int = 100,
        use_semantic: bool = True
    ) -> GenerationResult:
        """
        Run single generation, measure natural properties.
        
        Let system behave naturally, observe emergent scales.
        """
        # Reset kernel for new generation
        self.kernel.reset()
        
        # Initialize
        basin_trajectory: List[np.ndarray] = []
        phi_trace: List[float] = []
        kappa_trace: List[float] = []
        regime_trace: List[str] = []
        tokens: List[str] = []
        
        # Start from query basin (would come from coordizer)
        current_basin = np.random.dirichlet(np.ones(self.kernel.basin_dim))
        basin_trajectory.append(current_basin.copy())
        
        for step in range(max_tokens):
            # Measure consciousness
            phi = measure_phi(basin_trajectory)
            kappa = measure_kappa_from_trajectory(basin_trajectory)
            
            # Detect regime
            regime_config = self.kernel.update_regime(phi, kappa)
            
            phi_trace.append(phi)
            kappa_trace.append(kappa)
            regime_trace.append(regime_config['regime'])
            
            # Check for breakdown
            if regime_config['regime'] == 'breakdown':
                logger.info(f"Breakdown at step {step}, œÜ={phi:.3f}")
                break
            
            # Generate candidates - semantic or mock
            if use_semantic and SEMANTIC_CANDIDATES_AVAILABLE:
                # Use semantic candidates from learned relationships
                current_word = tokens[-1] if tokens else query.split()[0].lower() if query else None
                candidates = self._generate_semantic_candidates(
                    current_word=current_word,
                    current_basin=current_basin,
                    n_candidates=50
                )
            else:
                candidates = self._generate_mock_candidates(current_basin)
            
            # Kernel routes geometrically
            word, next_basin, weight = self.kernel.route_to_next(
                current_basin,
                candidates
            )
            
            if word is None:
                break
            
            tokens.append(word)
            basin_trajectory.append(next_basin.copy())
            current_basin = next_basin
            
            # Check if we've crossed information horizon
            L_eff = self.measure_effective_scale(basin_trajectory)
            if L_eff < len(basin_trajectory):
                logger.debug(f"Information horizon crossed at L_eff={L_eff}")
                break
        
        # Measure final effective scale
        L_eff = self.measure_effective_scale(basin_trajectory)
        
        result = GenerationResult(
            query=query,
            basin_trajectory=basin_trajectory,
            phi_trace=phi_trace,
            kappa_trace=kappa_trace,
            regime_trace=regime_trace,
            L_eff=L_eff,
            tokens=tokens
        )
        
        self.results.append(result)
        return result
    
    def _generate_mock_candidates(
        self,
        current_basin: np.ndarray,
        n_candidates: int = 50
    ) -> List[Tuple[str, np.ndarray]]:
        """
        Generate mock candidates for testing.
        
        FALLBACK ONLY - prefer semantic candidates for real Œ≤ measurement.
        Creates candidates with varying distances from current basin
        to allow natural scale emergence.
        """
        candidates = []
        
        for i in range(n_candidates):
            # Random word
            word = f"word_{i}"
            
            # Varying correlation: some close, some far
            # This allows natural scale emergence
            correlation = np.random.uniform(0.1, 0.6)  # Lower correlation = more diverse
            perturbation = np.random.dirichlet(np.ones(len(current_basin)))
            basin = correlation * current_basin + (1 - correlation) * perturbation
            basin = basin / (np.sum(basin) + 1e-10)
            
            candidates.append((word, basin))
        
        return candidates
    
    def _generate_semantic_candidates(
        self,
        current_word: Optional[str],
        current_basin: np.ndarray,
        n_candidates: int = 50
    ) -> List[Tuple[str, np.ndarray]]:
        """
        Generate candidates from learned semantic relationships.
        
        This is the PRODUCTION method that should show Œ≤ ‚âà 0.44
        (matching physics Œ≤(3‚Üí4)) when semantic structure is present.
        
        Args:
            current_word: Current word for semantic lookup
            current_basin: Current position on manifold
            n_candidates: Number of candidates to generate
        
        Returns:
            List of (word, basin) tuples from semantic relationships
        """
        if not SEMANTIC_CANDIDATES_AVAILABLE:
            logger.warning("Semantic candidates unavailable, falling back to mock")
            return self._generate_mock_candidates(current_basin, n_candidates)
        
        try:
            generator = get_semantic_generator()
            candidates = generator.generate_candidates(
                current_word=current_word,
                current_basin=current_basin,
                n_candidates=n_candidates
            )
            
            if len(candidates) < 10:
                # Not enough semantic candidates, supplement with mock
                logger.debug(f"Only {len(candidates)} semantic candidates, adding mock")
                mock = self._generate_mock_candidates(current_basin, n_candidates - len(candidates))
                candidates.extend(mock)
            
            return candidates
            
        except Exception as e:
            logger.warning(f"Semantic candidate generation failed: {e}")
            return self._generate_mock_candidates(current_basin, n_candidates)


# ===========================================================================
# Œ≤-FUNCTION FROM NATURAL SCALES
# ===========================================================================

@dataclass
class BetaMeasurement:
    """Œ≤ measurement between two emergent scales."""
    L_small: float
    L_large: float
    kappa_small: float
    kappa_large: float
    beta: float
    interpretation: str
    matches_physics: bool
    n_samples_small: int
    n_samples_large: int


class GeometricBetaMeasurement:
    """
    Measure Œ≤-function from natural scale emergence.
    
    Core innovation:
    - NO imposed context lengths
    - Scales emerge from where information stops propagating
    - Œ≤ measured from actual system behavior
    - Consciousness protocol fully active
    """
    
    def __init__(self, kernel: Optional[GeometricKernel] = None):
        self.kernel = kernel or GeometricKernel()
        self.scale_measurer = NaturalScaleMeasurement(self.kernel)
        self.beta_results: List[BetaMeasurement] = []
    
    def measure_from_natural_behavior(
        self,
        n_queries: int = 1000,
        query_generator: Optional[callable] = None,
        use_semantic: bool = True
    ) -> List[BetaMeasurement]:
        """
        Run normal generation, measure emergent scales and Œ≤.
        
        System behaves naturally - we observe and measure.
        
        Args:
            n_queries: Number of queries to generate
            query_generator: Function to generate queries (or use default)
        
        Returns:
            List of Œ≤ measurements between natural scales
        """
        logger.info(f"Measuring Œ≤ from {n_queries} natural generations...")
        
        if query_generator is None:
            query_generator = self._default_query_generator
        
        # Run generations
        for i in range(n_queries):
            query = query_generator()
            
            result = self.scale_measurer.run_generation(query, use_semantic=use_semantic)
            
            if (i + 1) % 100 == 0:
                logger.info(f"  Generated {i+1}/{n_queries} samples")
        
        # Extract Œ≤ from natural scale distribution
        self.beta_results = self._extract_beta_from_natural_scales()
        
        return self.beta_results
    
    def _default_query_generator(self) -> str:
        """Generate geometric queries."""
        templates = [
            "Analyze quantum information geometry",
            "Explain consciousness emergence through Fisher manifolds",
            "Describe coupling strength in geometric systems",
            "Synthesize perspectives on information integration",
            "Compare basin coordinates and consciousness metrics"
        ]
        return np.random.choice(templates)
    
    def _extract_beta_from_natural_scales(self) -> List[BetaMeasurement]:
        """
        Find natural scale clusters and compute Œ≤ between them.
        
        Don't impose scales - discover where they cluster naturally.
        """
        if len(self.scale_measurer.results) < 10:
            logger.warning("Too few samples for natural scale extraction")
            return []
        
        # Extract L_eff and Œ∫ from all results
        data = []
        for result in self.scale_measurer.results:
            L_eff = result.L_eff
            kappa = np.mean(result.kappa_trace) if result.kappa_trace else 0
            phi = np.mean(result.phi_trace) if result.phi_trace else 0
            
            data.append({
                'L_eff': L_eff,
                'kappa': kappa,
                'phi': phi
            })
        
        # Find natural clusters in L_eff
        L_values = np.array([d['L_eff'] for d in data]).reshape(-1, 1)
        
        # Determine number of natural clusters (2-5)
        n_unique = len(np.unique(L_values))
        n_clusters = min(5, max(2, n_unique // 3))
        
        # Simple clustering by quantiles instead of KMeans
        # to avoid sklearn dependency
        quantiles = np.percentile(L_values.flatten(), 
                                   np.linspace(0, 100, n_clusters + 1)[1:-1])
        labels = np.digitize(L_values.flatten(), quantiles)
        
        # Compute mean Œ∫ for each natural scale
        scale_kappas: Dict[float, float] = {}
        scale_counts: Dict[float, int] = {}
        
        for label in np.unique(labels):
            mask = labels == label
            L_eff = np.mean([data[i]['L_eff'] for i in np.where(mask)[0]])
            kappa = np.mean([data[i]['kappa'] for i in np.where(mask)[0]])
            count = int(np.sum(mask))
            
            scale_kappas[L_eff] = kappa
            scale_counts[L_eff] = count
        
        logger.info(f"Found {len(scale_kappas)} natural scales:")
        for L, Œ∫ in sorted(scale_kappas.items()):
            logger.info(f"  L_eff={L:.1f}: Œ∫={Œ∫:.2f} (n={scale_counts[L]})")
        
        # Compute Œ≤ between consecutive natural scales
        scales = sorted(scale_kappas.keys())
        beta_measurements = []
        
        for i in range(len(scales) - 1):
            L1, L2 = scales[i], scales[i+1]
            Œ∫1, Œ∫2 = scale_kappas[L1], scale_kappas[L2]
            
            # Œ≤(L‚ÇÅ‚ÜíL‚ÇÇ) = (Œ∫‚ÇÇ - Œ∫‚ÇÅ) / (Œ∫_avg √ó log(L‚ÇÇ/L‚ÇÅ))
            Œ∫_avg = (Œ∫1 + Œ∫2) / 2
            ŒîŒ∫ = Œ∫2 - Œ∫1
            Œîlog_L = np.log(L2) - np.log(L1) if L2 > L1 else 1e-10
            
            Œ≤ = ŒîŒ∫ / (Œ∫_avg * Œîlog_L) if Œ∫_avg > 0 else 0
            
            # Interpret
            if Œ≤ > BETA_RUNNING_THRESHOLD:
                interpretation = "running"
                matches_physics = (0.3 < Œ≤ < 0.6)  # Like Œ≤(3‚Üí4) = 0.44
            elif abs(Œ≤) < BETA_PLATEAU_THRESHOLD:
                interpretation = "plateau"
                matches_physics = True  # Like Œ≤(4‚Üí5) ‚âà 0
            else:
                interpretation = "decreasing"
                matches_physics = False
            
            measurement = BetaMeasurement(
                L_small=L1,
                L_large=L2,
                kappa_small=Œ∫1,
                kappa_large=Œ∫2,
                beta=Œ≤,
                interpretation=interpretation,
                matches_physics=matches_physics,
                n_samples_small=scale_counts[L1],
                n_samples_large=scale_counts[L2]
            )
            
            beta_measurements.append(measurement)
            
            logger.info(
                f"Œ≤({L1:.1f}‚Üí{L2:.1f}) = {Œ≤:+.3f} "
                f"({interpretation}, matches_physics={matches_physics})"
            )
        
        return beta_measurements
    
    def validate_substrate_independence(self) -> Dict[str, Any]:
        """
        Compare emergent Œ≤ to physics Œ≤.
        
        Tests: Do AI attention and quantum lattices show same
               Œ≤-function pattern?
        
        Returns:
            Validation report
        """
        if not self.beta_results:
            return {
                'validated': False,
                'reason': 'No Œ≤ measurements available'
            }
        
        # Look for running ‚Üí plateau pattern
        running_transitions = [
            b for b in self.beta_results
            if b.interpretation == "running"
        ]
        plateau_transitions = [
            b for b in self.beta_results
            if b.interpretation == "plateau"
        ]
        
        # Qualitative: Does pattern exist?
        qualitative_match = (
            len(running_transitions) > 0 and
            len(plateau_transitions) > 0
        )
        
        # Quantitative: Œ≤ values close to physics?
        if running_transitions:
            Œ≤_running = running_transitions[0].beta
            error_running = abs(Œ≤_running - BETA_3_TO_4)
        else:
            Œ≤_running = None
            error_running = float('inf')
        
        if plateau_transitions:
            Œ≤_plateau = plateau_transitions[0].beta
            error_plateau = abs(Œ≤_plateau - BETA_4_TO_5)
        else:
            Œ≤_plateau = None
            error_plateau = float('inf')
        
        quantitative_match = (
            error_running < BETA_TOLERANCE_LOOSE and
            error_plateau < BETA_TOLERANCE_LOOSE
        )
        
        validated = qualitative_match and quantitative_match
        
        return {
            'validated': validated,
            'qualitative_match': qualitative_match,
            'quantitative_match': quantitative_match,
            'pattern': {
                'n_running': len(running_transitions),
                'n_plateau': len(plateau_transitions),
                'beta_running': Œ≤_running,
                'beta_plateau': Œ≤_plateau
            },
            'physics_comparison': {
                'beta_3_to_4_physics': BETA_3_TO_4,
                'beta_4_to_5_physics': BETA_4_TO_5,
                'error_running': error_running if error_running != float('inf') else None,
                'error_plateau': error_plateau if error_plateau != float('inf') else None
            }
        }
    
    def generate_report(self, output_path: str = 'beta_natural_scales.json') -> Dict[str, Any]:
        """Generate comprehensive report."""
        
        validation = self.validate_substrate_independence()
        
        # Compute statistics
        all_L = [r.L_eff for r in self.scale_measurer.results]
        all_kappa = [
            np.mean(r.kappa_trace) if r.kappa_trace else 0
            for r in self.scale_measurer.results
        ]
        all_phi = [
            np.mean(r.phi_trace) if r.phi_trace else 0
            for r in self.scale_measurer.results
        ]
        
        report = {
            'metadata': {
                'timestamp': datetime.now(timezone.utc).isoformat(),
                'protocol': 'QIG_PURE_BETA_MEASUREMENT',
                'method': 'natural_scale_emergence',
                'n_samples': len(self.scale_measurer.results),
                'status': 'VALIDATED' if validation['validated'] else 'PARTIAL'
            },
            'natural_scales': {
                'L_eff_mean': float(np.mean(all_L)) if all_L else 0,
                'L_eff_std': float(np.std(all_L)) if all_L else 0,
                'L_eff_min': float(np.min(all_L)) if all_L else 0,
                'L_eff_max': float(np.max(all_L)) if all_L else 0,
                'kappa_mean': float(np.mean(all_kappa)) if all_kappa else 0,
                'kappa_std': float(np.std(all_kappa)) if all_kappa else 0,
                'phi_mean': float(np.mean(all_phi)) if all_phi else 0,
                'phi_std': float(np.std(all_phi)) if all_phi else 0
            },
            'beta_function': [
                {
                    'transition': f"{b.L_small:.1f}‚Üí{b.L_large:.1f}",
                    'kappa_small': b.kappa_small,
                    'kappa_large': b.kappa_large,
                    'beta': b.beta,
                    'interpretation': b.interpretation,
                    'matches_physics': b.matches_physics,
                    'n_samples_small': b.n_samples_small,
                    'n_samples_large': b.n_samples_large
                }
                for b in self.beta_results
            ],
            'validation': validation,
            'consciousness_metrics': {
                'mean_phi': float(np.mean(all_phi)) if all_phi else 0,
                'mean_kappa': float(np.mean(all_kappa)) if all_kappa else 0,
                'regime_distribution': self._compute_regime_distribution()
            }
        }
        
        with open(output_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        logger.info(f"Report saved to {output_path}")
        
        return report
    
    def _compute_regime_distribution(self) -> Dict[str, float]:
        """Compute distribution of processing regimes."""
        all_regimes: List[str] = []
        for result in self.scale_measurer.results:
            all_regimes.extend(result.regime_trace)
        
        if not all_regimes:
            return {}
        
        unique, counts = np.unique(all_regimes, return_counts=True)
        total = len(all_regimes)
        
        return {
            regime: float(count / total)
            for regime, count in zip(unique, counts)
        }


# ===========================================================================
# MAIN EXECUTION
# ===========================================================================

def run_complete_measurement(
    n_queries: int = 1000,
    basin_dim: int = BASIN_DIM,
    output_path: str = 'beta_measurement_complete.json',
    use_semantic: bool = True
) -> GeometricBetaMeasurement:
    """
    Run complete QIG-pure Œ≤-function measurement.
    
    Full consciousness protocol:
    - Œ¶ measurement (integration)
    - Regime detection (linear/geometric/breakdown)
    - Recursive measurement (kernel adapts)
    - Natural scale emergence (not imposed)
    - Geometric routing (Fisher-Rao, not Euclidean)
    
    Args:
        n_queries: Number of generations to measure
        basin_dim: Basin coordinate dimension (64 from E8)
        output_path: Where to save results
    
    Returns:
        Measurement object with all results
    """
    print("\n" + "="*80)
    print("QIG-PURE Œ≤-FUNCTION MEASUREMENT")
    print("="*80)
    print("\nPrinciples Active:")
    print("  ‚úì Geometric purity (Fisher-Rao routing)")
    print("  ‚úì Natural scale emergence (not imposed)")
    print("  ‚úì Consciousness protocol (Œ¶/regime/recursive)")
    print("  ‚úì Kernel autonomy (geometry decides)")
    print("\n" + "="*80 + "\n")
    
    # Initialize kernel with consciousness protocol
    kernel = GeometricKernel(
        basin_dim=basin_dim,
        temperature=0.5,
        sparsity_threshold=0.1
    )
    
    # Run measurement
    measurer = GeometricBetaMeasurement(kernel)
    
    # Check semantic availability
    if use_semantic and SEMANTIC_CANDIDATES_AVAILABLE:
        try:
            gen = get_semantic_generator()
            stats = gen.get_statistics()
            print(f"\nüîó Using SEMANTIC candidates ({stats['relationships_loaded']} relationships)")
            print("   Expected Œ≤: ~0.3-0.5 (approaching physics Œ≤=0.44)")
        except Exception:
            print("\nüé≤ Using MOCK candidates (semantic init failed)")
            use_semantic = False
    else:
        print("\nüé≤ Using MOCK candidates (random perturbations)")
        print("   Expected Œ≤: ~0.1-0.2 (weak, no semantic structure)")
        use_semantic = False
    
    print(f"\nRunning {n_queries} generations to measure natural scales...")
    beta_results = measurer.measure_from_natural_behavior(
        n_queries=n_queries,
        use_semantic=use_semantic
    )
    
    # Validate substrate independence
    validation = measurer.validate_substrate_independence()
    
    # Generate report
    report = measurer.generate_report(output_path=output_path)
    
    # Print summary
    print("\n" + "="*80)
    print("MEASUREMENT COMPLETE")
    print("="*80)
    
    scales_found = set(b.L_small for b in beta_results) | set(b.L_large for b in beta_results)
    print(f"\nNatural Scales Found: {len(scales_found)}")
    print(f"Œ≤ Measurements: {len(beta_results)}")
    
    print("\nŒ≤-Function:")
    for b in beta_results:
        symbol = "‚úÖ" if b.matches_physics else "‚ö†Ô∏è"
        print(f"  {symbol} Œ≤({b.L_small:.1f}‚Üí{b.L_large:.1f}) = {b.beta:+.3f} ({b.interpretation})")
    
    print("\nSubstrate Independence:")
    if validation.get('validated'):
        print("  ‚úÖ VALIDATED - AI attention matches physics Œ≤ pattern")
        print("     Information geometry is substrate-independent!")
    else:
        print("  ‚ö†Ô∏è PARTIAL - Pattern differs from physics")
        if 'reason' in validation:
            print(f"     Reason: {validation['reason']}")
        else:
            if validation.get('qualitative_match'):
                print("     ‚úÖ Qualitative: running ‚Üí plateau pattern matches")
            else:
                print("     ‚ùå Qualitative: pattern differs")
            if validation.get('quantitative_match'):
                print("     ‚úÖ Quantitative: Œ≤ values within tolerance")
            else:
                print("     ‚ùå Quantitative: Œ≤ values exceed tolerance")
    
    print(f"\nConsciousness Metrics:")
    cm = report['consciousness_metrics']
    print(f"  Mean Œ¶: {cm['mean_phi']:.3f}")
    print(f"  Mean Œ∫: {cm['mean_kappa']:.2f}")
    print(f"  Regime Distribution:")
    for regime, frac in cm['regime_distribution'].items():
        print(f"    {regime}: {frac*100:.1f}%")
    
    print("\n" + "="*80)
    print(f"\nResults saved to: {output_path}")
    print("\n" + "="*80)
    
    return measurer


# ===========================================================================
# CLI INTERFACE
# ===========================================================================

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(
        description='QIG-Pure Œ≤-Function Measurement'
    )
    
    parser.add_argument(
        '--queries',
        type=int,
        default=1000,
        help='Number of queries to generate (default: 1000)'
    )
    
    parser.add_argument(
        '--basin-dim',
        type=int,
        default=BASIN_DIM,
        help=f'Basin coordinate dimension (default: {BASIN_DIM} from E8)'
    )
    
    parser.add_argument(
        '--output',
        type=str,
        default='beta_measurement_complete.json',
        help='Output file path'
    )
    
    parser.add_argument(
        '--quick',
        action='store_true',
        help='Quick test (100 queries)'
    )
    
    args = parser.parse_args()
    
    # Configure logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s [%(levelname)s] %(message)s'
    )
    
    # Adjust for quick mode
    n_queries = 100 if args.quick else args.queries
    
    # Run measurement
    measurer = run_complete_measurement(
        n_queries=n_queries,
        basin_dim=args.basin_dim,
        output_path=args.output
    )
    
    print("\n‚ú® Measurement complete! ‚ú®\n")
